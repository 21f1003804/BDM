{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR1V8tqXAaig",
        "outputId": "f499c917-d3a3-4064-aec1-b841f11ca9f5"
      },
      "id": "iR1V8tqXAaig",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.7.14)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Select kaggle.json from your computer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "ec5AQO3_Abzo",
        "outputId": "0bc43442-1246-4635-bf0f-1f0bea3fcf69"
      },
      "id": "ec5AQO3_Abzo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-886431db-970e-45e4-b765-b66c108ad291\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-886431db-970e-45e4-b765-b66c108ad291\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle  # Create directory if it doesn't exist\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json  # Secure the file"
      ],
      "metadata": {
        "id": "_qigYid4A7nF"
      },
      "id": "_qigYid4A7nF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d ricgomes/global-fashion-retail-stores-dataset -p /content/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itdf-w3SA3tf",
        "outputId": "25df1163-afc5-4526-f6a8-59d61fb65bdd"
      },
      "id": "itdf-w3SA3tf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/ricgomes/global-fashion-retail-stores-dataset\n",
            "License(s): Attribution 4.0 International (CC BY 4.0)\n",
            "Downloading global-fashion-retail-stores-dataset.zip to /content\n",
            " 92% 205M/224M [00:00<00:00, 631MB/s]\n",
            "100% 224M/224M [00:00<00:00, 585MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the dataset (adjust the file name if needed)\n",
        "!unzip /content/global-fashion-retail-stores-dataset.zip -d /content/global-fashion-dataset/\n",
        "\n",
        "# # Example: Read a CSV file from the dataset using pandas\n",
        "# import pandas as pd\n",
        "\n",
        "# # Replace with the actual file path after unzipping (check the Files pane in Colab)\n",
        "# df = pd.read_csv('/content/global-fashion-dataset/global_fashion_brands_stores.csv')  # Example file name – verify yours\n",
        "# print(df.head())  # Preview the first few rows\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSDfWp8qBsxz",
        "outputId": "6a351be8-ae9e-443c-f3e6-01067ef429b9"
      },
      "id": "SSDfWp8qBsxz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/global-fashion-retail-stores-dataset.zip\n",
            "  inflating: /content/global-fashion-dataset/customers.csv  \n",
            "  inflating: /content/global-fashion-dataset/discounts.csv  \n",
            "  inflating: /content/global-fashion-dataset/employees.csv  \n",
            "  inflating: /content/global-fashion-dataset/products.csv  \n",
            "  inflating: /content/global-fashion-dataset/stores.csv  \n",
            "  inflating: /content/global-fashion-dataset/transactions.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Exchange rates you provided\n",
        "exchange_rates = {\n",
        "    'EUR': 1.13,\n",
        "    'GBP': 1.34,\n",
        "    'CNY': 0.14,\n",
        "    'USD': 1.0\n",
        "}\n",
        "\n",
        "city_translation_map = {\n",
        "\n",
        "}\n",
        "\n",
        "country_translation_map = {\n",
        "    '中国': 'China',\n",
        "    'España': 'Spain',\n",
        "    'Portugal': 'Portugal',\n",
        "    'Deutschland': 'Germany',\n",
        "    'France': 'France',\n",
        "    'United Kingdom': 'United Kingdom',\n",
        "    'United States': 'United States'\n",
        "}\n",
        "\n",
        "def load_data_efficiently():\n",
        "    \"\"\"Load data with optimized memory usage\"\"\"\n",
        "    print(\" Loading data efficiently...\")\n",
        "\n",
        "    # Load with pandas first, then convert to polars for better performance\n",
        "    print(\"Loading discounts...\")\n",
        "    discounts = pd.read_csv('/content/global-fashion-dataset/discounts.csv')\n",
        "\n",
        "    print(\"Loading products...\")\n",
        "    products = pd.read_csv(r'/content/global-fashion-dataset/products.csv')\n",
        "\n",
        "    print(\"Loading employees...\")\n",
        "    employees = pd.read_csv(r'/content/global-fashion-dataset/employees.csv')\n",
        "\n",
        "    print(\"Loading stores...\")\n",
        "    stores = pd.read_csv(r'/content/global-fashion-dataset/stores.csv')\n",
        "\n",
        "    print(\"Loading customers...\")\n",
        "    customers = pd.read_csv(r'/content/global-fashion-dataset/customers.csv', low_memory=False)\n",
        "\n",
        "    print(\"Loading transactions (this may take a moment)...\")\n",
        "    transactions = pd.read_csv(r'/content/global-fashion-dataset/transactions.csv', low_memory=False)\n",
        "    print(f\"Intial Transactions loaded: {len(transactions)} rows\")\n",
        "    transactions=transactions.drop_duplicates()\n",
        "    print(f\"Final Transactions loaded: {len(transactions)} rows\")\n",
        "\n",
        "    # Convert to polars for better performance\n",
        "    print(\"Converting to Polars for better performance...\")\n",
        "    discounts_pl = pl.from_pandas(discounts)\n",
        "    products_pl = pl.from_pandas(products)\n",
        "    employees_pl = pl.from_pandas(employees)\n",
        "    stores_pl = pl.from_pandas(stores)\n",
        "    customers_pl = pl.from_pandas(customers)\n",
        "    transactions_pl = pl.from_pandas(transactions)\n",
        "\n",
        "    print(f\" Data loaded successfully!\")\n",
        "    print(f\"Transactions shape: {transactions_pl.shape}\")\n",
        "\n",
        "    return discounts_pl, products_pl, employees_pl, stores_pl, customers_pl, transactions_pl\n",
        "\n",
        "\n",
        "def clean_and_translate_data(master_data, city_translation_map, country_translation_map):\n",
        "    \"\"\"\n",
        "    Clean and translate city and country names into English using Polars DataFrame.\n",
        "    Compatible with older Polars versions.\n",
        "    \"\"\"\n",
        "    master_data = master_data.with_columns([\n",
        "        pl.col(\"City\").replace(city_translation_map).alias(\"City\"),\n",
        "        pl.col(\"Country\").replace(country_translation_map).alias(\"Country\")\n",
        "    ])\n",
        "    return master_data\n",
        "\n",
        "\n",
        "def clean_date_columns(df, date_columns):\n",
        "    \"\"\"Clean and convert date columns to simple YYYY-MM-DD format\"\"\"\n",
        "    print(f\" Converting date columns to YYYY-MM-DD format: {date_columns}\")\n",
        "\n",
        "    for col in date_columns:\n",
        "        if col in df.columns:\n",
        "            try:\n",
        "                # Convert to date (not datetime) - this removes time component automatically\n",
        "                df = df.with_columns(\n",
        "                    pl.col(col).str.strptime(pl.Date, format=\"%Y-%m-%d\", strict=False)\n",
        "                    .fill_null(\n",
        "                        pl.col(col).str.strptime(pl.Date, format=\"%Y-%m-%d %H:%M:%S\", strict=False)\n",
        "                    )\n",
        "                    .fill_null(\n",
        "                        pl.col(col).str.strptime(pl.Date, format=\"%d/%m/%Y\", strict=False)\n",
        "                    )\n",
        "                    .alias(col)\n",
        "                )\n",
        "                print(f\" Converted {col} to YYYY-MM-DD date format\")\n",
        "            except Exception as e:\n",
        "                print(f\" Could not convert {col}: {e}\")\n",
        "                # Keep original if conversion fails\n",
        "                pass\n",
        "\n",
        "    return df\n",
        "\n",
        "def prepare_lookup_tables(discounts_pl, products_pl, employees_pl, stores_pl, customers_pl):\n",
        "    \"\"\"Prepare minimal lookup tables with only essential information\"\"\"\n",
        "    print(\"🔧 Preparing optimized lookup tables...\")\n",
        "\n",
        "    # Clean discount dates - Start and End to YYYY-MM-DD\n",
        "    discounts_clean = clean_date_columns(discounts_pl, ['Start', 'End'])\n",
        "    discounts_lookup = discounts_clean.select([\n",
        "        'Start', 'End', 'Discont', 'Description', 'Category', 'Sub Category'\n",
        "    ])\n",
        "\n",
        "    # Products - keep essential info only\n",
        "    products_lookup = products_pl.select([\n",
        "        'Product ID', 'Category', 'Sub Category', 'Description EN',\n",
        "        'Color', 'Sizes', 'Production Cost'\n",
        "    ])\n",
        "\n",
        "    # Employees - minimal info\n",
        "    employees_lookup = employees_pl.select([\n",
        "        'Employee ID', 'Store ID', 'Name', 'Position'\n",
        "    ])\n",
        "\n",
        "    # Stores - essential location info\n",
        "    stores_pl = clean_and_translate_data(stores_pl, city_translation_map, country_translation_map)\n",
        "    stores_lookup = stores_pl.select([\n",
        "        'Store ID', 'Country', 'City', 'Store Name', 'Number of Employees'\n",
        "    ])\n",
        "\n",
        "    # Customers - clean birth date to YYYY-MM-DD and keep essential info\n",
        "    customers_pl = clean_and_translate_data(customers_pl, city_translation_map, country_translation_map)\n",
        "    customers_clean = clean_date_columns(customers_pl, ['Date Of Birth'])\n",
        "    customers_lookup = customers_clean.select([\n",
        "        'Customer ID', 'Name', 'Email', 'Gender', 'Date Of Birth', 'Job Title'\n",
        "    ])\n",
        "\n",
        "    print(\" Lookup tables prepared\")\n",
        "    return discounts_lookup, products_lookup, employees_lookup, stores_lookup, customers_lookup\n",
        "\n",
        "def process_transactions(transactions_pl, exchange_rates):\n",
        "    \"\"\"Process transactions with USD conversion and date cleaning\"\"\"\n",
        "    print(\" Processing transactions...\")\n",
        "\n",
        "    # Clean transaction dates - convert Date to YYYY-MM-DD format (removes time)\n",
        "\n",
        "    transactions_clean = clean_date_columns(transactions_pl, ['Date'])\n",
        "\n",
        "    # Add exchange rate column\n",
        "    print(\"💱 Adding USD conversion...\")\n",
        "    transactions_clean = transactions_clean.with_columns(\n",
        "        pl.col('Currency').map_elements(\n",
        "            lambda x: exchange_rates.get(x, 1.0),\n",
        "            return_dtype=pl.Float64\n",
        "        ).alias(\"Exchange_Rate_to_USD\")\n",
        "    )\n",
        "\n",
        "    # Convert monetary columns to USD\n",
        "    monetary_columns = ['Unit Price', 'Line Total', 'Invoice Total']\n",
        "\n",
        "    for col in monetary_columns:\n",
        "        if col in transactions_clean.columns:\n",
        "            usd_col_name = f\"{col.replace(' ', '_')}_USD\"\n",
        "            transactions_clean = transactions_clean.with_columns(\n",
        "                (pl.col(col) * pl.col(\"Exchange_Rate_to_USD\")).alias(usd_col_name)\n",
        "            )\n",
        "            print(f\" Created {usd_col_name}\")\n",
        "\n",
        "    return transactions_clean\n",
        "\n",
        "def create_master_dataset():\n",
        "    \"\"\"Create the master dataset with optimized merging\"\"\"\n",
        "    print(\" Creating master dataset...\")\n",
        "\n",
        "    # Load data\n",
        "    discounts_pl, products_pl, employees_pl, stores_pl, customers_pl, transactions_pl = load_data_efficiently()\n",
        "\n",
        "    # Prepare lookup tables\n",
        "    discounts_lookup, products_lookup, employees_lookup, stores_lookup, customers_lookup = prepare_lookup_tables(\n",
        "        discounts_pl, products_pl, employees_pl, stores_pl, customers_pl\n",
        "    )\n",
        "\n",
        "    # Process transactions\n",
        "    transactions_processed = process_transactions(transactions_pl, exchange_rates)\n",
        "\n",
        "    # Merge with lookup tables (left joins to keep all transactions)\n",
        "    print(\"🔗 Merging tables...\")\n",
        "\n",
        "    # Merge with products\n",
        "    master_data = transactions_processed.join(\n",
        "        products_lookup,\n",
        "        on='Product ID',\n",
        "        how='left'\n",
        "    )\n",
        "    print(f\" Merged with products. Shape: {master_data.shape}\")\n",
        "\n",
        "    # Merge with customers\n",
        "    master_data = master_data.join(\n",
        "        customers_lookup,\n",
        "        on='Customer ID',\n",
        "        how='left'\n",
        "    )\n",
        "    print(f\" Merged with customers. Shape: {master_data.shape}\")\n",
        "\n",
        "    # Merge with stores\n",
        "    master_data = master_data.join(\n",
        "        stores_lookup,\n",
        "        on='Store ID',\n",
        "        how='left'\n",
        "    )\n",
        "    print(f\" Merged with stores. Shape: {master_data.shape}\")\n",
        "\n",
        "    # Merge with employees\n",
        "    master_data = master_data.join(\n",
        "        employees_lookup,\n",
        "        on='Employee ID',\n",
        "        how='left'\n",
        "    )\n",
        "    print(f\" Merged with employees. Shape: {master_data.shape}\")\n",
        "\n",
        "    # Create data directory if it doesn't exist\n",
        "    os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "    # Save lookup tables separately for future use\n",
        "    print(\" Saving lookup tables...\")\n",
        "    discounts_lookup.write_parquet(\"data/discounts_lookup.parquet\")\n",
        "    products_lookup.write_parquet(\"data/products_lookup.parquet\")\n",
        "    employees_lookup.write_parquet(\"data/employees_lookup.parquet\")\n",
        "    stores_lookup.write_parquet(\"data/stores_lookup.parquet\")\n",
        "    customers_lookup.write_parquet(\"data/customers_lookup.parquet\")\n",
        "\n",
        "    # Save master dataset\n",
        "    print(\"💾 Saving master dataset...\")\n",
        "    master_data.write_parquet(\"data/master_transactions.parquet\")\n",
        "\n",
        "    # Save a sample CSV for inspection (first 10k rows)\n",
        "    master_data.head(10000).write_csv(\"data/master_transactions_sample.csv\")\n",
        "\n",
        "    print(\" Master dataset created successfully!\")\n",
        "    print(f\"Final shape: {master_data.shape}\")\n",
        "    print(f\"Memory usage optimized by keeping only essential columns from lookup tables\")\n",
        "\n",
        "    return master_data, discounts_lookup\n",
        "\n",
        "def show_dataset_summary(master_data):\n",
        "    \"\"\"Show summary of the final dataset\"\"\"\n",
        "    print(\"\\n MASTER DATASET SUMMARY:\")\n",
        "    print(f\"Shape: {master_data.shape}\")\n",
        "    print(f\"Columns: {len(master_data.columns)}\")\n",
        "\n",
        "    print(\"\\n Column List:\")\n",
        "    for i, col in enumerate(master_data.columns, 1):\n",
        "        print(f\"{i:2d}. {col}\")\n",
        "\n",
        "    print(f\"\\n USD Converted Columns:\")\n",
        "    usd_cols = [col for col in master_data.columns if 'USD' in col]\n",
        "    for col in usd_cols:\n",
        "        print(f\"   • {col}\")\n",
        "\n",
        "    print(f\"\\n Date Columns (YYYY-MM-DD format):\")\n",
        "    date_cols = [col for col in master_data.columns if master_data[col].dtype == pl.Date]\n",
        "    for col in date_cols:\n",
        "        print(f\"   • {col}\")\n",
        "        # Show sample dates\n",
        "        sample_dates = master_data.select(col).drop_nulls().head(3)\n",
        "        print(f\"     Sample: {sample_dates.to_series().to_list()}\")\n",
        "\n",
        "    print(f\"\\n Sample Data:\")\n",
        "    print(master_data.head(3))\n",
        "\n",
        "    return master_data\n",
        "\n",
        "# Execute the pipeline\n",
        "print(\" Starting Optimized Data Pipeline...\")\n",
        "master_data, discounts_lookup = create_master_dataset()\n",
        "final_data = show_dataset_summary(master_data)\n",
        "\n",
        "print(\"\\n Pipeline Complete!\")\n",
        "print(\"Files saved:\")\n",
        "print(\"   • data/master_transactions.parquet (main dataset)\")\n",
        "print(\"   • data/master_transactions_sample.csv (sample for inspection)\")\n",
        "print(\"   • data/discounts_lookup.parquet (for discount analysis)\")\n",
        "print(\"   • data/products_lookup.parquet\")\n",
        "print(\"   • data/employees_lookup.parquet\")\n",
        "print(\"   • data/stores_lookup.parquet\")\n",
        "print(\"   • data/customers_lookup.parquet\")\n",
        "print(\"\\n All dates are now in simple YYYY-MM-DD format!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvitEsLxB_VW",
        "outputId": "2ddce70c-1fbc-496f-9456-1029ed57d668"
      },
      "id": "dvitEsLxB_VW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Starting Optimized Data Pipeline...\n",
            " Creating master dataset...\n",
            " Loading data efficiently...\n",
            "Loading discounts...\n",
            "Loading products...\n",
            "Loading employees...\n",
            "Loading stores...\n",
            "Loading customers...\n",
            "Loading transactions (this may take a moment)...\n",
            "Intial Transactions loaded: 6416827 rows\n",
            "Final Transactions loaded: 6416029 rows\n",
            "Converting to Polars for better performance...\n",
            " Data loaded successfully!\n",
            "Transactions shape: (6416029, 19)\n",
            "🔧 Preparing optimized lookup tables...\n",
            " Converting date columns to YYYY-MM-DD format: ['Start', 'End']\n",
            " Converted Start to YYYY-MM-DD date format\n",
            " Converted End to YYYY-MM-DD date format\n",
            " Converting date columns to YYYY-MM-DD format: ['Date Of Birth']\n",
            " Converted Date Of Birth to YYYY-MM-DD date format\n",
            " Lookup tables prepared\n",
            " Processing transactions...\n",
            " Converting date columns to YYYY-MM-DD format: ['Date']\n",
            " Converted Date to YYYY-MM-DD date format\n",
            "💱 Adding USD conversion...\n",
            " Created Unit_Price_USD\n",
            " Created Line_Total_USD\n",
            " Created Invoice_Total_USD\n",
            "🔗 Merging tables...\n",
            " Merged with products. Shape: (6416029, 29)\n",
            " Merged with customers. Shape: (6416029, 34)\n",
            " Merged with stores. Shape: (6416029, 38)\n",
            " Merged with employees. Shape: (6416029, 41)\n",
            " Saving lookup tables...\n",
            "💾 Saving master dataset...\n",
            " Master dataset created successfully!\n",
            "Final shape: (6416029, 41)\n",
            "Memory usage optimized by keeping only essential columns from lookup tables\n",
            "\n",
            " MASTER DATASET SUMMARY:\n",
            "Shape: (6416029, 41)\n",
            "Columns: 41\n",
            "\n",
            " Column List:\n",
            " 1. Invoice ID\n",
            " 2. Line\n",
            " 3. Customer ID\n",
            " 4. Product ID\n",
            " 5. Size\n",
            " 6. Color\n",
            " 7. Unit Price\n",
            " 8. Quantity\n",
            " 9. Date\n",
            "10. Discount\n",
            "11. Line Total\n",
            "12. Store ID\n",
            "13. Employee ID\n",
            "14. Currency\n",
            "15. Currency Symbol\n",
            "16. SKU\n",
            "17. Transaction Type\n",
            "18. Payment Method\n",
            "19. Invoice Total\n",
            "20. Exchange_Rate_to_USD\n",
            "21. Unit_Price_USD\n",
            "22. Line_Total_USD\n",
            "23. Invoice_Total_USD\n",
            "24. Category\n",
            "25. Sub Category\n",
            "26. Description EN\n",
            "27. Color_right\n",
            "28. Sizes\n",
            "29. Production Cost\n",
            "30. Name\n",
            "31. Email\n",
            "32. Gender\n",
            "33. Date Of Birth\n",
            "34. Job Title\n",
            "35. Country\n",
            "36. City\n",
            "37. Store Name\n",
            "38. Number of Employees\n",
            "39. Store ID_right\n",
            "40. Name_right\n",
            "41. Position\n",
            "\n",
            " USD Converted Columns:\n",
            "   • Exchange_Rate_to_USD\n",
            "   • Unit_Price_USD\n",
            "   • Line_Total_USD\n",
            "   • Invoice_Total_USD\n",
            "\n",
            " Date Columns (YYYY-MM-DD format):\n",
            "   • Date\n",
            "     Sample: [datetime.date(2023, 1, 1), datetime.date(2023, 1, 1), datetime.date(2023, 1, 1)]\n",
            "   • Date Of Birth\n",
            "     Sample: [datetime.date(1983, 12, 25), datetime.date(1983, 12, 25), datetime.date(1983, 12, 25)]\n",
            "\n",
            " Sample Data:\n",
            "shape: (3, 41)\n",
            "┌────────────┬──────┬────────────┬────────────┬───┬────────────┬───────────┬───────────┬───────────┐\n",
            "│ Invoice ID ┆ Line ┆ Customer   ┆ Product ID ┆ … ┆ Number of  ┆ Store     ┆ Name_righ ┆ Position  │\n",
            "│ ---        ┆ ---  ┆ ID         ┆ ---        ┆   ┆ Employees  ┆ ID_right  ┆ t         ┆ ---       │\n",
            "│ str        ┆ i64  ┆ ---        ┆ i64        ┆   ┆ ---        ┆ ---       ┆ ---       ┆ str       │\n",
            "│            ┆      ┆ i64        ┆            ┆   ┆ i64        ┆ i64       ┆ str       ┆           │\n",
            "╞════════════╪══════╪════════════╪════════════╪═══╪════════════╪═══════════╪═══════════╪═══════════╡\n",
            "│ INV-US-001 ┆ 1    ┆ 47162      ┆ 485        ┆ … ┆ 10         ┆ 1         ┆ Melissa   ┆ Sales     │\n",
            "│ -03558761  ┆      ┆            ┆            ┆   ┆            ┆           ┆ Wilson    ┆ Associate │\n",
            "│ INV-US-001 ┆ 2    ┆ 47162      ┆ 2779       ┆ … ┆ 10         ┆ 1         ┆ Melissa   ┆ Sales     │\n",
            "│ -03558761  ┆      ┆            ┆            ┆   ┆            ┆           ┆ Wilson    ┆ Associate │\n",
            "│ INV-US-001 ┆ 3    ┆ 47162      ┆ 64         ┆ … ┆ 10         ┆ 1         ┆ Melissa   ┆ Sales     │\n",
            "│ -03558761  ┆      ┆            ┆            ┆   ┆            ┆           ┆ Wilson    ┆ Associate │\n",
            "└────────────┴──────┴────────────┴────────────┴───┴────────────┴───────────┴───────────┴───────────┘\n",
            "\n",
            " Pipeline Complete!\n",
            "Files saved:\n",
            "   • data/master_transactions.parquet (main dataset)\n",
            "   • data/master_transactions_sample.csv (sample for inspection)\n",
            "   • data/discounts_lookup.parquet (for discount analysis)\n",
            "   • data/products_lookup.parquet\n",
            "   • data/employees_lookup.parquet\n",
            "   • data/stores_lookup.parquet\n",
            "   • data/customers_lookup.parquet\n",
            "\n",
            " All dates are now in simple YYYY-MM-DD format!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b_boM7BYFA4W"
      },
      "id": "b_boM7BYFA4W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Geographic , Trend & Discount Analysis**"
      ],
      "metadata": {
        "id": "mMFVvLwxFFZF"
      },
      "id": "mMFVvLwxFFZF"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.offline as pyo\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "def load_master_data_safely():\n",
        "    \"\"\"Safely load master data and check columns\"\"\"\n",
        "    print(\"Loading master dataset safely...\")\n",
        "\n",
        "    try:\n",
        "        # Try loading with coordinates first\n",
        "        if os.path.exists(\"data/master_transactions_with_coords.parquet\"):\n",
        "            master_data = pl.read_parquet(\"data/master_transactions_with_coords.parquet\")\n",
        "            print(f\" Master data with coordinates loaded: {master_data.shape}\")\n",
        "        else:\n",
        "            # Load original master data\n",
        "            master_data = pl.read_parquet(\"data/master_transactions.parquet\")\n",
        "            print(f\"Master data loaded: {master_data.shape}\")\n",
        "\n",
        "            # Add coordinates if needed\n",
        "            print(\"Adding coordinates...\")\n",
        "            stores_original = pd.read_csv(r'/content/global-fashion-dataset/stores.csv')\n",
        "            stores_coords = pl.from_pandas(stores_original).select([\n",
        "                'Store ID', 'Latitude', 'Longitude'\n",
        "            ])\n",
        "\n",
        "            master_data = master_data.join(stores_coords, on='Store ID', how='left')\n",
        "            master_data.write_parquet(\"data/master_transactions_with_coords.parquet\")\n",
        "            print(\" Coordinates added and saved!\")\n",
        "\n",
        "        # Verify key columns exist\n",
        "        required_columns = ['Line_Total_USD', 'Unit_Price_USD', 'Quantity', 'Country', 'Transaction Type']\n",
        "        missing_columns = [col for col in required_columns if col not in master_data.columns]\n",
        "\n",
        "        if missing_columns:\n",
        "            print(f\" Missing columns: {missing_columns}\")\n",
        "            return None\n",
        "\n",
        "        print(\" All required columns found!\")\n",
        "        return master_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Errror loading master data: {e}\")\n",
        "        return None\n",
        "\n",
        "def prepare_optimized_geographic_data(master_data):\n",
        "    \"\"\"Prepare optimized data for geographic analysis using SIMPLE discount calculation\"\"\"\n",
        "    print(\" Preparing optimized geographic analysis data...\")\n",
        "\n",
        "    # Filter for sales only (exclude returns)\n",
        "    sales_data = master_data.filter(pl.col(\"Transaction Type\") == \"Sale\")\n",
        "    print(f\"Sales data shape: {sales_data.shape}\")\n",
        "\n",
        "    # Verify we have the required columns\n",
        "    print(\" Verifying required columns in sales data...\")\n",
        "    required_cols = ['Line_Total_USD', 'Unit_Price_USD', 'Quantity', 'Country', 'Date']\n",
        "    for col in required_cols:\n",
        "        if col not in sales_data.columns:\n",
        "            print(f\" Missing column: {col}\")\n",
        "            return None\n",
        "        else:\n",
        "            print(f\" Found column: {col}\")\n",
        "\n",
        "    # Add time-based columns\n",
        "    sales_data = sales_data.with_columns([\n",
        "        pl.col(\"Date\").dt.strftime(\"%Y-%m\").alias(\"Year_Month\"),\n",
        "        pl.col(\"Date\").dt.strftime(\"%Y-W%U\").alias(\"Year_Week\"),\n",
        "        pl.col(\"Date\").dt.weekday().alias(\"Day_of_Week_Num\"),\n",
        "        pl.col(\"Date\").dt.strftime(\"%A\").alias(\"Day_of_Week_Name\")\n",
        "    ])\n",
        "\n",
        "    print(\" Creating optimized data aggregations with SIMPLE discount calculation...\")\n",
        "\n",
        "    # 1. Monthly trends - ALL data (cumulative)\n",
        "    print(\"Creating monthly trends...\")\n",
        "    monthly_trends = sales_data.group_by(\"Year_Month\").agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Monthly_Sales_USD\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Monthly_Quantity\"),\n",
        "        pl.col(\"Line_Total_USD\").count().alias(\"Monthly_Transactions\")\n",
        "    ]).sort(\"Year_Month\")\n",
        "\n",
        "    # 2. Weekly trends - ALL data (cumulative)\n",
        "    print(\"Creating weekly trends...\")\n",
        "    weekly_trends = sales_data.group_by(\"Year_Week\").agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Weekly_Sales_USD\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Weekly_Quantity\"),\n",
        "        pl.col(\"Line_Total_USD\").count().alias(\"Weekly_Transactions\")\n",
        "    ]).sort(\"Year_Week\")\n",
        "\n",
        "    # 3. Monthly trends by country - ALL countries\n",
        "    print(\"Creating monthly country trends...\")\n",
        "    monthly_country_trends = sales_data.group_by([\"Country\", \"Year_Month\"]).agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Monthly_Sales_USD\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Monthly_Quantity\")\n",
        "    ]).sort([\"Country\", \"Year_Month\"])\n",
        "\n",
        "    # 4. COMBINED Country-wise analysis using SIMPLE discount calculation\n",
        "    print(\"Creating country combined analysis...\")\n",
        "    country_combined = sales_data.group_by(\"Country\").agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Total_Sales_USD\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Total_Quantity\"),\n",
        "        pl.col(\"Line_Total_USD\").count().alias(\"Total_Transactions\"),\n",
        "        pl.col(\"Store ID\").n_unique().alias(\"Number_of_Stores\"),\n",
        "        pl.col(\"Customer ID\").n_unique().alias(\"Unique_Customers\"),\n",
        "        # SIMPLE discount calculation - Unit_Price_USD * Quantity = Total without discount\n",
        "        (pl.col(\"Unit_Price_USD\") * pl.col(\"Quantity\")).sum().alias(\"Total_Without_Discount_USD\")\n",
        "    ]).with_columns([\n",
        "        # Calculate average realization\n",
        "        (pl.col(\"Total_Sales_USD\") / pl.col(\"Total_Quantity\")).alias(\"Avg_Realization_USD\"),\n",
        "        # Calculate simple average discount percentage\n",
        "        ((pl.col(\"Total_Without_Discount_USD\") - pl.col(\"Total_Sales_USD\")) /\n",
        "         pl.col(\"Total_Without_Discount_USD\") * 100).alias(\"Avg_Discount_Percent\")\n",
        "    ]).sort(\"Total_Sales_USD\", descending=True)\n",
        "\n",
        "\n",
        "    # 5. COMBINED City-wise analysis - FIXED to use store-level data\n",
        "    print(\"Creating city combined analysis...\")\n",
        "    # First get store-level aggregated data with discount calculation\n",
        "    store_level_with_discount = sales_data.filter(\n",
        "        pl.col(\"Store ID\").is_not_null() &\n",
        "        pl.col(\"Store Name\").is_not_null() &\n",
        "        pl.col(\"Country\").is_not_null() &\n",
        "        pl.col(\"City\").is_not_null()\n",
        "    ).group_by([\n",
        "        \"Store ID\"\n",
        "    ]).agg([\n",
        "        pl.col(\"Store Name\").first().alias(\"Store_Name\"),\n",
        "        pl.col(\"Country\").first().alias(\"Country\"),\n",
        "        pl.col(\"City\").first().alias(\"City\"),\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Total_Sales_USD\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Total_Quantity\"),\n",
        "        pl.col(\"Line_Total_USD\").count().alias(\"Total_Transactions\"),\n",
        "        pl.col(\"Customer ID\").n_unique().alias(\"Unique_Customers\"),\n",
        "        (pl.col(\"Unit_Price_USD\") * pl.col(\"Quantity\")).sum().alias(\"Total_Without_Discount_USD\")\n",
        "    ]).with_columns([\n",
        "        (pl.col(\"Total_Sales_USD\") / pl.col(\"Total_Quantity\")).alias(\"Avg_Realization_USD\"),\n",
        "        ((pl.col(\"Total_Without_Discount_USD\") - pl.col(\"Total_Sales_USD\")) /\n",
        "         pl.col(\"Total_Without_Discount_USD\") * 100).alias(\"Avg_Discount_Percent\")\n",
        "    ])\n",
        "\n",
        "    # Then aggregate by city using store-level data\n",
        "    city_combined = store_level_with_discount.group_by([\"Country\", \"City\"]).agg([\n",
        "        pl.col(\"Total_Sales_USD\").sum().alias(\"Total_Sales_USD\"),\n",
        "        pl.col(\"Total_Quantity\").sum().alias(\"Total_Quantity\"),\n",
        "        pl.col(\"Total_Transactions\").sum().alias(\"Total_Transactions\"),\n",
        "        pl.col(\"Store ID\").count().alias(\"Number_of_Stores\"),\n",
        "        pl.col(\"Unique_Customers\").sum().alias(\"Unique_Customers\"),\n",
        "        pl.col(\"Total_Without_Discount_USD\").sum().alias(\"Total_Without_Discount_USD\")\n",
        "    ]).with_columns([\n",
        "        (pl.col(\"Total_Sales_USD\") / pl.col(\"Total_Quantity\")).alias(\"Avg_Realization_USD\"),\n",
        "        ((pl.col(\"Total_Without_Discount_USD\") - pl.col(\"Total_Sales_USD\")) /\n",
        "         pl.col(\"Total_Without_Discount_USD\") * 100).alias(\"Avg_Discount_Percent\")\n",
        "    ]).sort(\"Total_Sales_USD\", descending=True)\n",
        "\n",
        "\n",
        "    # 6. Store-level for heatmap - ALL stores - FIXED\n",
        "    print(\"Creating store heatmap data...\")\n",
        "    store_heatmap = sales_data.filter(\n",
        "        pl.col(\"Store ID\").is_not_null() &\n",
        "        pl.col(\"Store Name\").is_not_null() &\n",
        "        pl.col(\"Country\").is_not_null() &\n",
        "        pl.col(\"City\").is_not_null()\n",
        "    ).group_by([\n",
        "        \"Store ID\"\n",
        "    ]).agg([\n",
        "        pl.col(\"Store Name\").first().alias(\"Store Name\"),\n",
        "        pl.col(\"Country\").first().alias(\"Country\"),\n",
        "        pl.col(\"City\").first().alias(\"City\"),\n",
        "        pl.col(\"Latitude\").first().alias(\"Latitude\"),\n",
        "        pl.col(\"Longitude\").first().alias(\"Longitude\"),\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Total_Sales_USD\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Total_Quantity\"),\n",
        "        pl.col(\"Line_Total_USD\").count().alias(\"Total_Transactions\")\n",
        "    ]).sort(\"Total_Sales_USD\", descending=True)\n",
        "\n",
        "\n",
        "    # 7. Day of week analysis - Sales and Quantity separately\n",
        "    print(\"Creating day of week analysis...\")\n",
        "    day_of_week_sales = sales_data.group_by([\"Day_of_Week_Num\", \"Day_of_Week_Name\"]).agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Total_Sales_USD\")\n",
        "    ]).sort(\"Day_of_Week_Num\")\n",
        "\n",
        "    day_of_week_quantity = sales_data.group_by([\"Day_of_Week_Num\", \"Day_of_Week_Name\"]).agg([\n",
        "        pl.col(\"Quantity\").sum().alias(\"Total_Quantity\")\n",
        "    ]).sort(\"Day_of_Week_Num\")\n",
        "\n",
        "    print(\" Optimized geographic data prepared with SIMPLE discount calculation\")\n",
        "    print(\" Discount calculation:\")\n",
        "    print(\"   • Total Without Discount = Unit_Price_USD × Quantity\")\n",
        "    print(\"   • Total With Discount = Line_Total_USD\")\n",
        "    print(\"   • Discount % = (Total Without Discount - Total With Discount) ÷ Total Without Discount × 100\")\n",
        "\n",
        "    return {\n",
        "        \"sales_data\": sales_data,\n",
        "        \"monthly_trends\": monthly_trends,\n",
        "        \"weekly_trends\": weekly_trends,\n",
        "        \"monthly_country_trends\": monthly_country_trends,\n",
        "        \"country_combined\": country_combined,  # Combined data\n",
        "        \"city_combined\": city_combined,        # Combined data\n",
        "        \"store_heatmap\": store_heatmap,\n",
        "        \"day_of_week_sales\": day_of_week_sales,\n",
        "        \"day_of_week_quantity\": day_of_week_quantity\n",
        "    }\n",
        "\n",
        "\n",
        "def create_optimized_geographic_dashboard(data_dict):\n",
        "    \"\"\"Create optimized dashboard with combined visualizations\"\"\"\n",
        "    print(\" Creating optimized geographic dashboard...\")\n",
        "\n",
        "    # Convert all data to pandas for plotting\n",
        "    monthly_trends_df = data_dict[\"monthly_trends\"].to_pandas()\n",
        "    weekly_trends_df = data_dict[\"weekly_trends\"].to_pandas()\n",
        "    monthly_country_df = data_dict[\"monthly_country_trends\"].to_pandas()\n",
        "    country_df = data_dict[\"country_combined\"].to_pandas()  # Combined data\n",
        "    city_df = data_dict[\"city_combined\"].to_pandas()        # Combined data\n",
        "    print(city_df.shape)\n",
        "    store_df = data_dict[\"store_heatmap\"].to_pandas()\n",
        "    day_sales_df = data_dict[\"day_of_week_sales\"].to_pandas()\n",
        "    day_qty_df = data_dict[\"day_of_week_quantity\"].to_pandas()\n",
        "\n",
        "    # Create HTML content\n",
        "    html_content = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Optimized Geographic Analysis Dashboard</title>\n",
        "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
        "        <style>\n",
        "            body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }\n",
        "            .chart-container { background-color: white; margin: 20px 0; padding: 20px; border-radius: 10px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }\n",
        "            .chart-title { font-size: 24px; font-weight: bold; text-align: center; margin-bottom: 20px; color: #333; }\n",
        "            .dashboard-title { font-size: 36px; font-weight: bold; text-align: center; margin-bottom: 30px; color: #2c3e50; }\n",
        "            .column-info { background-color: #e8f4f8; padding: 15px; margin: 10px 0; border-radius: 8px; font-size: 14px; }\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <div class=\"dashboard-title\"> Optimized Geographic Analysis Dashboard</div>\n",
        "        <div class=\"column-info\">\n",
        "            <strong> Simple Discount Calculation:</strong><br>\n",
        "            • <strong>Total Without Discount</strong>: Unit Price × Quantity<br>\n",
        "            • <strong>Total With Discount</strong>: Line_Total_USD<br>\n",
        "            • <strong>Discount %</strong>: (Total Without Discount - Total With Discount) ÷ Total Without Discount × 100\n",
        "        </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Monthly Cumulative Trends\n",
        "    fig1 = go.Figure()\n",
        "    fig1.add_trace(go.Scatter(\n",
        "        x=monthly_trends_df['Year_Month'],\n",
        "        y=monthly_trends_df['Monthly_Sales_USD'],\n",
        "        mode='lines+markers',\n",
        "        name='Monthly Sales (USD)',\n",
        "        line=dict(color='#1f77b4', width=3),\n",
        "        marker=dict(size=8)\n",
        "    ))\n",
        "    fig1.update_layout(\n",
        "        title=\" Monthly Sales Trends (Cumulative)\",\n",
        "        xaxis_title=\"Month\",\n",
        "        yaxis_title=\"Sales (USD)\",\n",
        "        height=500,\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "    fig1.update_xaxes(tickangle=45)\n",
        "\n",
        "    # 2. Weekly Cumulative Trends\n",
        "    fig2 = go.Figure()\n",
        "    fig2.add_trace(go.Scatter(\n",
        "        x=weekly_trends_df['Year_Week'],\n",
        "        y=weekly_trends_df['Weekly_Sales_USD'],\n",
        "        mode='lines+markers',\n",
        "        name='Weekly Sales (USD)',\n",
        "        line=dict(color='#ff7f0e', width=2),\n",
        "        marker=dict(size=6)\n",
        "    ))\n",
        "    fig2.update_layout(\n",
        "        title=\" Weekly Sales Trends (Cumulative)\",\n",
        "        xaxis_title=\"Week\",\n",
        "        yaxis_title=\"Sales (USD)\",\n",
        "        height=500,\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "    fig2.update_xaxes(tickangle=45)\n",
        "\n",
        "    # 3. Monthly Trends by Country (Line Graph)\n",
        "    fig3 = px.line(\n",
        "        monthly_country_df,\n",
        "        x=\"Year_Month\",\n",
        "        y=\"Monthly_Sales_USD\",\n",
        "        color=\"Country\",\n",
        "        title=\" Monthly Sales Trends by Country\",\n",
        "        markers=True,\n",
        "        height=600\n",
        "    )\n",
        "    fig3.update_xaxes(tickangle=45)\n",
        "    fig3.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # 4. COMBINED Country Analysis - Sales & Quantity (bars) + Avg Realization (line)\n",
        "    fig4 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "    # Add Sales bars\n",
        "    fig4.add_trace(\n",
        "        go.Bar(\n",
        "            x=country_df['Country'],\n",
        "            y=country_df['Total_Sales_USD'],\n",
        "            name='Total Sales (Line_Total_USD)',\n",
        "            marker_color='lightblue',\n",
        "            yaxis='y'\n",
        "        ),\n",
        "        secondary_y=False,\n",
        "    )\n",
        "\n",
        "    # Add Quantity bars\n",
        "    fig4.add_trace(\n",
        "        go.Bar(\n",
        "            x=country_df['Country'],\n",
        "            y=country_df['Total_Quantity'],\n",
        "            name='Total Quantity',\n",
        "            marker_color='lightgreen',\n",
        "            yaxis='y'\n",
        "        ),\n",
        "        secondary_y=False,\n",
        "    )\n",
        "\n",
        "    # Add Average Realization line\n",
        "    fig4.add_trace(\n",
        "        go.Scatter(\n",
        "            x=country_df['Country'],\n",
        "            y=country_df['Avg_Realization_USD'],\n",
        "            mode='lines+markers',\n",
        "            name='Avg Realization (Line_Total_USD/Quantity)',\n",
        "            line=dict(color='red', width=3),\n",
        "            marker=dict(size=8),\n",
        "            yaxis='y2'\n",
        "        ),\n",
        "        secondary_y=True,\n",
        "    )\n",
        "\n",
        "    fig4.update_xaxes(title_text=\"Country\", tickangle=45)\n",
        "    fig4.update_yaxes(title_text=\"Sales (USD) / Quantity\", secondary_y=False)\n",
        "    fig4.update_yaxes(title_text=\"Average Realization (USD)\", secondary_y=True)\n",
        "    fig4.update_layout(\n",
        "        title_text=\"  Country Analysis: Sales, Quantity & Avg Realization\",\n",
        "        height=600,\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "\n",
        "    # 5. COMBINED City Analysis - Sales & Quantity (bars) + Avg Realization (line)\n",
        "    city_df['City_Country'] = city_df['City'] + ', ' + city_df['Country']\n",
        "    fig5 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "    # Add Sales bars\n",
        "    fig5.add_trace(\n",
        "        go.Bar(\n",
        "            x=city_df['City_Country'],\n",
        "            y=city_df['Total_Sales_USD'],\n",
        "            name='Total Sales (Line_Total_USD)',\n",
        "            marker_color='lightcoral',\n",
        "            yaxis='y'\n",
        "        ),\n",
        "        secondary_y=False,\n",
        "    )\n",
        "\n",
        "    # Add Quantity bars\n",
        "    fig5.add_trace(\n",
        "        go.Bar(\n",
        "            x=city_df['City_Country'],\n",
        "            y=city_df['Total_Quantity'],\n",
        "            name='Total Quantity',\n",
        "            marker_color='lightpink',\n",
        "            yaxis='y'\n",
        "        ),\n",
        "        secondary_y=False,\n",
        "    )\n",
        "\n",
        "    # Add Average Realization line\n",
        "    fig5.add_trace(\n",
        "        go.Scatter(\n",
        "            x=city_df['City_Country'],\n",
        "            y=city_df['Avg_Realization_USD'],\n",
        "            mode='lines+markers',\n",
        "            name='Avg Realization (Line_Total_USD/Quantity)',\n",
        "            line=dict(color='orange', width=3),\n",
        "            marker=dict(size=6),\n",
        "            yaxis='y2'\n",
        "        ),\n",
        "        secondary_y=True,\n",
        "    )\n",
        "\n",
        "    fig5.update_xaxes(title_text=\"City, Country\", tickangle=45)\n",
        "    fig5.update_yaxes(title_text=\"Sales (USD) / Quantity\", secondary_y=False)\n",
        "    fig5.update_yaxes(title_text=\"Average Realization (USD)\", secondary_y=True)\n",
        "    fig5.update_layout(\n",
        "        title_text=\"  City Analysis: Sales, Quantity & Avg Realization\",\n",
        "        height=700,\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "\n",
        "    # 6. Store Heatmap (Scatter Map)\n",
        "    store_clean = store_df.dropna(subset=['Latitude', 'Longitude'])\n",
        "    fig6 = px.scatter_mapbox(\n",
        "        store_clean,\n",
        "        lat=\"Latitude\",\n",
        "        lon=\"Longitude\",\n",
        "        size=\"Total_Sales_USD\",\n",
        "        color=\"Total_Sales_USD\",\n",
        "        hover_name=\"Store Name\",\n",
        "        hover_data={\n",
        "            \"Country\": True,\n",
        "            \"City\": True,\n",
        "            \"Total_Sales_USD\": \":,.0f\",\n",
        "            \"Total_Quantity\": \":,\",\n",
        "            \"Total_Transactions\": \":,\"\n",
        "        },\n",
        "        color_continuous_scale=\"Plasma\",\n",
        "        size_max=50,\n",
        "        zoom=1,\n",
        "        title=\" Store Sales Heatmap\",\n",
        "        height=700\n",
        "    )\n",
        "    fig6.update_layout(mapbox_style=\"open-street-map\", template=\"plotly_white\")\n",
        "\n",
        "    # 7. Day of Week - Sales Analysis\n",
        "    fig7 = go.Figure()\n",
        "    fig7.add_trace(go.Bar(\n",
        "        x=day_sales_df['Day_of_Week_Name'],\n",
        "        y=day_sales_df['Total_Sales_USD'],\n",
        "        name='Sales by Day',\n",
        "        marker_color='skyblue'\n",
        "    ))\n",
        "    fig7.update_layout(\n",
        "        title=\" Sales by Day of Week\",\n",
        "        xaxis_title=\"Day of Week\",\n",
        "        yaxis_title=\"Total Sales (USD)\",\n",
        "        height=500,\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "\n",
        "    # 8. Day of Week - Quantity Analysis\n",
        "    fig8 = go.Figure()\n",
        "    fig8.add_trace(go.Bar(\n",
        "        x=day_qty_df['Day_of_Week_Name'],\n",
        "        y=day_qty_df['Total_Quantity'],\n",
        "        name='Quantity by Day',\n",
        "        marker_color='lightseagreen'\n",
        "    ))\n",
        "    fig8.update_layout(\n",
        "        title=\" Quantity by Day of Week\",\n",
        "        xaxis_title=\"Day of Week\",\n",
        "        yaxis_title=\"Total Quantity\",\n",
        "        height=500,\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "\n",
        "    # 9. SIMPLE Discount vs Sales by Country\n",
        "    fig9 = px.scatter(\n",
        "        country_df,\n",
        "        x=\"Avg_Discount_Percent\",\n",
        "        y=\"Total_Sales_USD\",\n",
        "        text=\"Country\",\n",
        "        title=\" Average Discount % vs Total Sales by Country<br><sub>Simple Calculation: (Unit Price × Quantity - Line_Total_USD) ÷ (Unit Price × Quantity) × 100</sub>\",\n",
        "        height=600\n",
        "    )\n",
        "    fig9.update_traces(textposition=\"top center\")\n",
        "    fig9.update_layout(\n",
        "        xaxis_title=\"Average Discount (%)\",\n",
        "        yaxis_title=\"Total Sales (Line_Total_USD)\",\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "\n",
        "    # Convert all figures to HTML\n",
        "    figures = [fig1, fig2, fig3, fig4, fig5, fig6, fig7, fig8, fig9]\n",
        "    titles = [\n",
        "        \"Monthly Sales Trends (Cumulative)\",\n",
        "        \"Weekly Sales Trends (Cumulative)\",\n",
        "        \"Monthly Sales Trends by Country\",\n",
        "        \"Combined Country Analysis: Sales, Quantity & Avg Realization\",\n",
        "        \"Combined City Analysis: Sales, Quantity & Avg Realization\",\n",
        "        \"Store Sales Heatmap\",\n",
        "        \"Sales by Day of Week\",\n",
        "        \"Quantity by Day of Week\",\n",
        "        \"Simple Discount vs Sales by Country\"\n",
        "    ]\n",
        "\n",
        "    for i, (fig, title) in enumerate(zip(figures, titles)):\n",
        "        html_content += f\"\"\"\n",
        "        <div class=\"chart-container\">\n",
        "            <div class=\"chart-title\">{title}</div>\n",
        "            <div id=\"chart{i+1}\"></div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "    html_content += \"\"\"\n",
        "    <script>\n",
        "    \"\"\"\n",
        "\n",
        "    # Add JavaScript for each plot\n",
        "    for i, fig in enumerate(figures):\n",
        "        plot_json = fig.to_json()\n",
        "        html_content += f\"\"\"\n",
        "        var plotData{i+1} = {plot_json};\n",
        "        Plotly.newPlot('chart{i+1}', plotData{i+1}.data, plotData{i+1}.layout, {{responsive: true}});\n",
        "        \"\"\"\n",
        "\n",
        "    html_content += \"\"\"\n",
        "    </script>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    return html_content\n",
        "\n",
        "def save_optimized_data_files(data_dict):\n",
        "    \"\"\"Save optimized combined data files\"\"\"\n",
        "    print(\" Saving optimized combined data files...\")\n",
        "\n",
        "    os.makedirs(\"geographic_analysis/data\", exist_ok=True)\n",
        "\n",
        "    # Save optimized datasets\n",
        "    data_dict[\"monthly_trends\"].write_csv(\"geographic_analysis/data/monthly_trends.csv\")\n",
        "    data_dict[\"weekly_trends\"].write_csv(\"geographic_analysis/data/weekly_trends.csv\")\n",
        "    data_dict[\"monthly_country_trends\"].write_csv(\"geographic_analysis/data/monthly_country_trends.csv\")\n",
        "    data_dict[\"country_combined\"].write_csv(\"geographic_analysis/data/country_combined_analysis.csv\")  # Combined\n",
        "    data_dict[\"city_combined\"].write_csv(\"geographic_analysis/data/city_combined_analysis.csv\")        # Combined\n",
        "    data_dict[\"store_heatmap\"].write_csv(\"geographic_analysis/data/store_heatmap.csv\")\n",
        "    data_dict[\"day_of_week_sales\"].write_csv(\"geographic_analysis/data/day_of_week_sales.csv\")\n",
        "    data_dict[\"day_of_week_quantity\"].write_csv(\"geographic_analysis/data/day_of_week_quantity.csv\")\n",
        "\n",
        "    print(\" Optimized data files saved with SIMPLE discount calculation!\")\n",
        "\n",
        "def run_optimized_geographic_analysis():\n",
        "    \"\"\"Run the optimized geographic analysis with simple discount calculation\"\"\"\n",
        "    print(\" Starting Optimized Geographic Analysis with SIMPLE Discount Calculation...\")\n",
        "\n",
        "    # Load data safely\n",
        "    master_data = load_master_data_safely()\n",
        "    if master_data is None:\n",
        "        print(\" Failed to load master data!\")\n",
        "        return None\n",
        "\n",
        "    # Print column info for verification\n",
        "    print(f\"\\n Master data shape: {master_data.shape}\")\n",
        "    print(\" Key columns verified:\")\n",
        "    key_cols = ['Line_Total_USD', 'Unit Price', 'Quantity', 'Country', 'Transaction Type']\n",
        "    for col in key_cols:\n",
        "        if col in master_data.columns:\n",
        "            print(f\"    {col}\")\n",
        "        else:\n",
        "            print(f\"    {col} - MISSING!\")\n",
        "\n",
        "    # Prepare optimized data\n",
        "    data_dict = prepare_optimized_geographic_data(master_data)\n",
        "    if data_dict is None:\n",
        "        print(\" Failed to prepare data!\")\n",
        "        return None\n",
        "\n",
        "    # Save optimized data files\n",
        "    save_optimized_data_files(data_dict)\n",
        "\n",
        "    # Create optimized dashboard\n",
        "    html_content = create_optimized_geographic_dashboard(data_dict)\n",
        "\n",
        "    # Create directory and save HTML\n",
        "    os.makedirs(\"geographic_analysis\", exist_ok=True)\n",
        "\n",
        "    with open(\"geographic_analysis/optimized_geographic_dashboard.html\", \"w\", encoding='utf-8') as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "    # Print summary\n",
        "    country_df = data_dict[\"country_combined\"].to_pandas()\n",
        "    city_df = data_dict[\"city_combined\"].to_pandas()\n",
        "\n",
        "    print(\"\\n OPTIMIZED GEOGRAPHIC ANALYSIS COMPLETE!\")\n",
        "    print(f\" Total Global Sales (Line_Total_USD): ${country_df['Total_Sales_USD'].sum():,.0f}\")\n",
        "    print(f\" Total Quantity Sold: {country_df['Total_Quantity'].sum():,}\")\n",
        "    print(f\" Countries Analyzed: {len(country_df)}\")\n",
        "    print(f\" Cities Analyzed: {len(city_df)}\")\n",
        "    print(f\" Global Avg Realization: ${country_df['Total_Sales_USD'].sum() / country_df['Total_Quantity'].sum():.2f}\")\n",
        "    print(f\" Global Avg Discount: {((country_df['Total_Without_Discount_USD'].sum() - country_df['Total_Sales_USD'].sum()) / country_df['Total_Without_Discount_USD'].sum() * 100):.2f}%\")\n",
        "\n",
        "    print(\"\\n Dashboard Created with SIMPLE Discount Calculation:\")\n",
        "    print(\"    geographic_analysis/optimized_geographic_dashboard.html\")\n",
        "    print(\"    geographic_analysis/data/ (Optimized CSV files)\")\n",
        "\n",
        "    print(\"\\n  Discount Calculation Applied:\")\n",
        "    print(\"    Total Without Discount = Unit Price USD × Quantity\")\n",
        "    print(\"    Total With Discount = Line_Total_USD\")\n",
        "    print(\"    Discount % = (Total Without Discount - Total With Discount) ÷ Total Without Discount × 100\")\n",
        "    print(\"    Using actual column names from your data!\")\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "# Run the optimized analysis with simple discount calculation\n",
        "results = run_optimized_geographic_analysis()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z03y-ZTUFCt8",
        "outputId": "750959da-cdb8-41c4-bd71-799e190c731c"
      },
      "id": "Z03y-ZTUFCt8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Starting Optimized Geographic Analysis with SIMPLE Discount Calculation...\n",
            "Loading master dataset safely...\n",
            " Master data with coordinates loaded: (6416029, 43)\n",
            " All required columns found!\n",
            "\n",
            " Master data shape: (6416029, 43)\n",
            " Key columns verified:\n",
            "    Line_Total_USD\n",
            "    Unit Price\n",
            "    Quantity\n",
            "    Country\n",
            "    Transaction Type\n",
            " Preparing optimized geographic analysis data...\n",
            "Sales data shape: (6077200, 43)\n",
            " Verifying required columns in sales data...\n",
            " Found column: Line_Total_USD\n",
            " Found column: Unit_Price_USD\n",
            " Found column: Quantity\n",
            " Found column: Country\n",
            " Found column: Date\n",
            " Creating optimized data aggregations with SIMPLE discount calculation...\n",
            "Creating monthly trends...\n",
            "Creating weekly trends...\n",
            "Creating monthly country trends...\n",
            "Creating country combined analysis...\n",
            "Creating city combined analysis...\n",
            "Creating store heatmap data...\n",
            "Creating day of week analysis...\n",
            " Optimized geographic data prepared with SIMPLE discount calculation\n",
            " Discount calculation:\n",
            "   • Total Without Discount = Unit_Price_USD × Quantity\n",
            "   • Total With Discount = Line_Total_USD\n",
            "   • Discount % = (Total Without Discount - Total With Discount) ÷ Total Without Discount × 100\n",
            " Saving optimized combined data files...\n",
            " Optimized data files saved with SIMPLE discount calculation!\n",
            " Creating optimized geographic dashboard...\n",
            "(35, 10)\n",
            "\n",
            " OPTIMIZED GEOGRAPHIC ANALYSIS COMPLETE!\n",
            " Total Global Sales (Line_Total_USD): $305,884,837\n",
            " Total Quantity Sold: 6,686,124\n",
            " Countries Analyzed: 7\n",
            " Cities Analyzed: 35\n",
            " Global Avg Realization: $45.75\n",
            " Global Avg Discount: 12.70%\n",
            "\n",
            " Dashboard Created with SIMPLE Discount Calculation:\n",
            "    geographic_analysis/optimized_geographic_dashboard.html\n",
            "    geographic_analysis/data/ (Optimized CSV files)\n",
            "\n",
            "  Discount Calculation Applied:\n",
            "    Total Without Discount = Unit Price USD × Quantity\n",
            "    Total With Discount = Line_Total_USD\n",
            "    Discount % = (Total Without Discount - Total With Discount) ÷ Total Without Discount × 100\n",
            "    Using actual column names from your data!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.offline as pyo\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "def load_master_data_for_pricing():\n",
        "    \"\"\"Load master data for pricing analysis\"\"\"\n",
        "    print(\"Loading master data for pricing analysis...\")\n",
        "\n",
        "    try:\n",
        "        if os.path.exists(\"data/master_transactions_with_coords.parquet\"):\n",
        "            master_data = pl.read_parquet(\"data/master_transactions_with_coords.parquet\")\n",
        "        else:\n",
        "            master_data = pl.read_parquet(\"data/master_transactions.parquet\")\n",
        "\n",
        "        print(f\"Master data loaded: {master_data.shape}\")\n",
        "        return master_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading master data: {e}\")\n",
        "        return None\n",
        "\n",
        "def prepare_pricing_analysis_data(master_data):\n",
        "    \"\"\"Prepare comprehensive pricing and discount strategy analysis data\"\"\"\n",
        "    print(\"Preparing pricing and discount strategy analysis data...\")\n",
        "\n",
        "    # Filter for sales only\n",
        "    sales_data = master_data.filter(pl.col(\"Transaction Type\") == \"Sale\")\n",
        "    print(f\"Sales data shape: {sales_data.shape}\")\n",
        "\n",
        "    # Add discount calculations\n",
        "    sales_data = sales_data.with_columns([\n",
        "        # Calculate discount amount and percentage\n",
        "        (pl.col(\"Unit_Price_USD\") * pl.col(\"Quantity\") - pl.col(\"Line_Total_USD\")).alias(\"Discount_Amount_USD\"),\n",
        "        ((pl.col(\"Unit_Price_USD\") * pl.col(\"Quantity\") - pl.col(\"Line_Total_USD\")) /\n",
        "         (pl.col(\"Unit_Price_USD\") * pl.col(\"Quantity\")) * 100).alias(\"Discount_Percent\"),\n",
        "\n",
        "        # Add time dimensions\n",
        "        pl.col(\"Date\").dt.strftime(\"%Y-%m\").alias(\"Year_Month\"),\n",
        "        pl.col(\"Date\").dt.quarter().alias(\"Quarter\"),\n",
        "\n",
        "        # Product identifier\n",
        "        (pl.col(\"Product ID\").cast(pl.Utf8) + \"_\" + pl.col(\"Size\") + \"_\" + pl.col(\"Color\")).alias(\"Product_SKU\")\n",
        "    ])\n",
        "\n",
        "    print(\"Creating product-level aggregations...\")\n",
        "\n",
        "    # 1. Product Performance Matrix Data\n",
        "    product_performance = sales_data.group_by([\n",
        "        \"Product ID\", \"Category\", \"Sub Category\", \"Description EN\"\n",
        "    ]).agg([\n",
        "        # Revenue metrics\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Total_Revenue_USD\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Total_Quantity\"),\n",
        "        pl.col(\"Line_Total_USD\").count().alias(\"Total_Transactions\"),\n",
        "\n",
        "        # Price metrics\n",
        "        (pl.col(\"Line_Total_USD\").sum() / pl.col(\"Quantity\").sum()).alias(\"Avg_Unit_Price_USD\"),\n",
        "\n",
        "        # Discount metrics - WEIGHTED AVERAGE\n",
        "        (pl.col(\"Discount_Amount_USD\").sum() / (pl.col(\"Unit_Price_USD\") * pl.col(\"Quantity\")).sum() * 100).alias(\"Avg_Discount_Percent\"),\n",
        "        pl.col(\"Discount_Amount_USD\").sum().alias(\"Total_Discount_USD\"),\n",
        "\n",
        "        # Customer metrics\n",
        "        pl.col(\"Customer ID\").n_unique().alias(\"Unique_Customers\"),\n",
        "        pl.col(\"Store ID\").n_unique().alias(\"Stores_Available\")\n",
        "    ]).with_columns([\n",
        "        # Calculate additional metrics\n",
        "        (pl.col(\"Total_Revenue_USD\") / pl.col(\"Total_Quantity\")).alias(\"Revenue_Per_Unit\"),\n",
        "        (pl.col(\"Total_Discount_USD\") / pl.col(\"Total_Quantity\")).alias(\"Discount_Per_Unit\"),\n",
        "        (pl.col(\"Total_Revenue_USD\") / pl.col(\"Unique_Customers\")).alias(\"Revenue_Per_Customer\")\n",
        "    ])\n",
        "\n",
        "    # Add percentile ranks for matrix positioning\n",
        "    product_performance = product_performance.with_columns([\n",
        "        pl.col(\"Total_Revenue_USD\").rank(method=\"average\").alias(\"Revenue_Rank\"),\n",
        "        pl.col(\"Avg_Discount_Percent\").rank(method=\"average\").alias(\"Discount_Rank\")\n",
        "    ]).with_columns([\n",
        "        # Convert to percentile (0-1 scale)\n",
        "        (pl.col(\"Revenue_Rank\") / pl.col(\"Revenue_Rank\").max()).alias(\"Revenue_Percentile_Rank\"),\n",
        "        (pl.col(\"Discount_Rank\") / pl.col(\"Discount_Rank\").max()).alias(\"Discount_Percentile_Rank\")\n",
        "    ]).sort(\"Total_Revenue_USD\", descending=True)\n",
        "\n",
        "    print(\"Creating price elasticity analysis...\")\n",
        "\n",
        "    # 2. Price Elasticity Analysis (Product + Size + Color level)\n",
        "    price_elasticity = sales_data.group_by([\n",
        "        \"Product ID\", \"Size\", \"Color\", \"Category\", \"Sub Category\"\n",
        "    ]).agg([\n",
        "        (pl.col(\"Line_Total_USD\").sum() / pl.col(\"Quantity\").sum()).alias(\"Avg_Unit_Price_USD\"),  # Final price after discount\n",
        "        pl.col(\"Quantity\").sum().alias(\"Total_Quantity_Sold\"),\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Total_Revenue_USD\"),\n",
        "        (pl.col(\"Discount_Amount_USD\").sum() / (pl.col(\"Unit_Price_USD\") * pl.col(\"Quantity\")).sum() * 100).alias(\"Avg_Discount_Percent\"),\n",
        "        pl.col(\"Line_Total_USD\").count().alias(\"Total_Transactions\")\n",
        "    ]).filter(\n",
        "        # Filter for products with meaningful sales\n",
        "        (pl.col(\"Total_Quantity_Sold\") >= 10) &\n",
        "        (pl.col(\"Total_Transactions\") >= 5)\n",
        "    ).sort(\"Total_Revenue_USD\", descending=True)\n",
        "\n",
        "    print(\"Creating discount effectiveness analysis...\")\n",
        "\n",
        "    # 3. Discount Effectiveness by Category\n",
        "    discount_effectiveness = sales_data.group_by([\n",
        "        \"Category\", \"Sub Category\"\n",
        "    ]).agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Total_Revenue_USD\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Total_Quantity\"),\n",
        "        (pl.col(\"Discount_Amount_USD\").sum() / (pl.col(\"Unit_Price_USD\") * pl.col(\"Quantity\")).sum() * 100).alias(\"Avg_Discount_Percent\"),\n",
        "        (pl.col(\"Line_Total_USD\").sum() / pl.col(\"Quantity\").sum()).alias(\"Avg_Unit_Price_USD\"),\n",
        "        pl.col(\"Customer ID\").n_unique().alias(\"Unique_Customers\")\n",
        "    ]).with_columns([\n",
        "        (pl.col(\"Total_Revenue_USD\") / pl.col(\"Total_Quantity\")).alias(\"Revenue_Per_Unit\"),\n",
        "        (pl.col(\"Total_Revenue_USD\") / pl.col(\"Unique_Customers\")).alias(\"Revenue_Per_Customer\")\n",
        "    ]).sort(\"Total_Revenue_USD\", descending=True)\n",
        "\n",
        "    print(\"Creating discount bucket analysis...\")\n",
        "\n",
        "    # 4. Discount Bucket Analysis\n",
        "    discount_buckets = sales_data.with_columns([\n",
        "        pl.when(pl.col(\"Discount_Percent\") <= 5)\n",
        "        .then(pl.lit(\"0-5%\"))\n",
        "        .when(pl.col(\"Discount_Percent\") <= 15)\n",
        "        .then(pl.lit(\"6-15%\"))\n",
        "        .when(pl.col(\"Discount_Percent\") <= 25)\n",
        "        .then(pl.lit(\"16-25%\"))\n",
        "        .when(pl.col(\"Discount_Percent\") <= 35)\n",
        "        .then(pl.lit(\"26-35%\"))\n",
        "        .when(pl.col(\"Discount_Percent\") <= 50)\n",
        "        .then(pl.lit(\"36-50%\"))\n",
        "        .otherwise(pl.lit(\"50%+\"))\n",
        "        .alias(\"Discount_Bucket\")\n",
        "    ]).group_by(\"Discount_Bucket\").agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Total_Revenue_USD\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Total_Quantity\"),\n",
        "        pl.col(\"Line_Total_USD\").count().alias(\"Total_Transactions\"),\n",
        "        pl.col(\"Customer ID\").n_unique().alias(\"Unique_Customers\")\n",
        "    ]).with_columns([\n",
        "        (pl.col(\"Total_Revenue_USD\") / pl.col(\"Total_Quantity\")).alias(\"Revenue_Per_Unit\")\n",
        "    ])\n",
        "\n",
        "    print(\"Creating temporal pricing trends...\")\n",
        "\n",
        "    # 5. Monthly Pricing Trends\n",
        "    monthly_pricing = sales_data.group_by(\"Year_Month\").agg([\n",
        "        (pl.col(\"Line_Total_USD\").sum() / pl.col(\"Quantity\").sum()).alias(\"Avg_Unit_Price_USD\"),\n",
        "        (pl.col(\"Discount_Amount_USD\").sum() / (pl.col(\"Unit_Price_USD\") * pl.col(\"Quantity\")).sum() * 100).alias(\"Avg_Discount_Percent\"),\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Total_Revenue_USD\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Total_Quantity\")\n",
        "    ]).sort(\"Year_Month\")\n",
        "\n",
        "    print(\"Pricing analysis data prepared successfully!\")\n",
        "\n",
        "    return {\n",
        "        \"sales_data\": sales_data,\n",
        "        \"product_performance\": product_performance,\n",
        "        \"price_elasticity\": price_elasticity,\n",
        "        \"discount_effectiveness\": discount_effectiveness,\n",
        "        \"discount_buckets\": discount_buckets,\n",
        "        \"monthly_pricing\": monthly_pricing\n",
        "    }\n",
        "\n",
        "\n",
        "def create_pricing_strategy_dashboard(data_dict):\n",
        "    \"\"\"Create comprehensive pricing and discount strategy dashboard\"\"\"\n",
        "    print(\"Creating Pricing and Discount Strategy Dashboard...\")\n",
        "\n",
        "    # Convert to pandas for plotting\n",
        "    product_perf_df = data_dict[\"product_performance\"].to_pandas()\n",
        "    price_elasticity_df = data_dict[\"price_elasticity\"].to_pandas()\n",
        "    discount_eff_df = data_dict[\"discount_effectiveness\"].to_pandas()\n",
        "    discount_buckets_df = data_dict[\"discount_buckets\"].to_pandas()\n",
        "    monthly_pricing_df = data_dict[\"monthly_pricing\"].to_pandas()\n",
        "\n",
        "    # Create HTML structure\n",
        "    html_content = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Pricing and Discount Strategy Optimization Dashboard</title>\n",
        "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
        "        <style>\n",
        "            body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }\n",
        "            .chart-container { background-color: white; margin: 20px 0; padding: 20px; border-radius: 10px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }\n",
        "            .chart-title { font-size: 24px; font-weight: bold; text-align: center; margin-bottom: 20px; color: #333; }\n",
        "            .dashboard-title { font-size: 36px; font-weight: bold; text-align: center; margin-bottom: 30px; color: #2c3e50; }\n",
        "            .insights-box { background-color: #e8f4f8; padding: 15px; margin: 10px 0; border-radius: 8px; font-size: 14px; }\n",
        "            .metric-highlight { background-color: #fff3cd; padding: 10px; margin: 5px 0; border-radius: 5px; font-weight: bold; }\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <div class=\"dashboard-title\">Pricing and Discount Strategy Optimization</div>\n",
        "\n",
        "        <div class=\"insights-box\">\n",
        "            <strong>Key Insights:</strong><br>\n",
        "            • <strong>Product Performance Matrix</strong>: Shows relationship between discount levels and revenue performance<br>\n",
        "            • <strong>Price Elasticity</strong>: Analyzes how price changes affect quantity sold<br>\n",
        "            • <strong>Discount Effectiveness</strong>: Measures ROI of discount strategies across categories<br>\n",
        "            • <strong>Optimal Pricing</strong>: Identifies sweet spots for maximum profitability\n",
        "        </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Product Performance Matrix (inspired by your chart)\n",
        "    fig1 = px.scatter(\n",
        "        product_perf_df,\n",
        "        x=\"Revenue_Percentile_Rank\",\n",
        "        y=\"Discount_Percentile_Rank\",\n",
        "        size=\"Total_Revenue_USD\",\n",
        "        color=\"Total_Quantity\",\n",
        "        hover_name=\"Description EN\",\n",
        "        hover_data={\n",
        "            \"Category\": True,\n",
        "            \"Sub Category\": True,\n",
        "            \"Total_Revenue_USD\": \":,.0f\",\n",
        "            \"Total_Quantity\": \":,\",\n",
        "            \"Avg_Discount_Percent\": \":.1f%\",\n",
        "            \"Avg_Unit_Price_USD\": \":.2f\"\n",
        "        },\n",
        "        title=\"Product Performance Matrix: Revenue vs Discount Strategy\",\n",
        "        labels={\n",
        "            \"Revenue_Percentile_Rank\": \"Revenue Percentile Rank\",\n",
        "            \"Discount_Percentile_Rank\": \"Discount Percentile Rank\",\n",
        "            \"Total_Quantity\": \"Total Quantity\"\n",
        "        },\n",
        "        color_continuous_scale=\"Viridis\",\n",
        "        height=700\n",
        "    )\n",
        "\n",
        "    # Add quadrant lines\n",
        "    fig1.add_hline(y=0.5, line_dash=\"dash\", line_color=\"gray\", opacity=0.7)\n",
        "    fig1.add_vline(x=0.5, line_dash=\"dash\", line_color=\"gray\", opacity=0.7)\n",
        "\n",
        "    # Add quadrant labels\n",
        "    fig1.add_annotation(x=0.25, y=0.75, text=\"High Discount<br>Low Revenue\",\n",
        "                       showarrow=False, font=dict(size=14, color=\"red\"))\n",
        "    fig1.add_annotation(x=0.75, y=0.75, text=\"High Discount<br>High Revenue\",\n",
        "                       showarrow=False, font=dict(size=14, color=\"orange\"))\n",
        "    fig1.add_annotation(x=0.25, y=0.25, text=\"Low Discount<br>Low Revenue\",\n",
        "                       showarrow=False, font=dict(size=14, color=\"gray\"))\n",
        "    fig1.add_annotation(x=0.75, y=0.25, text=\"Low Discount<br>High Revenue\",\n",
        "                       showarrow=False, font=dict(size=14, color=\"green\"))\n",
        "\n",
        "    fig1.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # 2. Product Price Elasticity (inspired by your chart)\n",
        "    fig2 = px.scatter(\n",
        "        price_elasticity_df,\n",
        "        x=\"Avg_Unit_Price_USD\",\n",
        "        y=\"Total_Quantity_Sold\",\n",
        "        size=\"Total_Revenue_USD\",\n",
        "        color=\"Avg_Discount_Percent\",\n",
        "        hover_name=\"Product ID\",\n",
        "        hover_data={\n",
        "            \"Category\": True,\n",
        "            \"Sub Category\": True,\n",
        "            \"Size\": True,\n",
        "            \"Color\": True,\n",
        "            \"Total_Revenue_USD\": \":,.0f\",\n",
        "            \"Avg_Discount_Percent\": \":.1f%\"\n",
        "        },\n",
        "        title=\"Product Price Elasticity Analysis\",\n",
        "        labels={\n",
        "            \"Avg_Unit_Price_USD\": \"Average Unit Price (USD)\",\n",
        "            \"Total_Quantity_Sold\": \"Total Quantity Sold\",\n",
        "            \"Avg_Discount_Percent\": \"Avg Discount %\"\n",
        "        },\n",
        "        color_continuous_scale=\"RdYlBu_r\",\n",
        "        height=700,\n",
        "        log_y=True  # Log scale for better visualization\n",
        "    )\n",
        "    fig2.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # 3. Discount Effectiveness by Category\n",
        "    fig3 = px.scatter(\n",
        "        discount_eff_df,\n",
        "        x=\"Avg_Discount_Percent\",\n",
        "        y=\"Revenue_Per_Unit\",\n",
        "        size=\"Total_Revenue_USD\",\n",
        "        color=\"Category\",\n",
        "        hover_name=\"Sub Category\",\n",
        "        hover_data={\n",
        "            \"Total_Revenue_USD\": \":,.0f\",\n",
        "            \"Total_Quantity\": \":,\",\n",
        "            \"Unique_Customers\": \":,\",\n",
        "            \"Avg_Unit_Price_USD\": \":.2f\"\n",
        "        },\n",
        "        title=\"Discount Effectiveness by Category\",\n",
        "        labels={\n",
        "            \"Avg_Discount_Percent\": \"Average Discount Percentage (%)\",\n",
        "            \"Revenue_Per_Unit\": \"Revenue Per Unit (USD)\"\n",
        "        },\n",
        "        height=600\n",
        "    )\n",
        "    fig3.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # 4. Discount Bucket Analysis\n",
        "    discount_buckets_df['Discount_Order'] = discount_buckets_df['Discount_Bucket'].map({\n",
        "        '0-5%': 1, '6-15%': 2, '16-25%': 3, '26-35%': 4, '36-50%': 5, '50%+': 6\n",
        "    })\n",
        "    discount_buckets_df = discount_buckets_df.sort_values('Discount_Order')\n",
        "\n",
        "    fig4 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "    # Add revenue bars\n",
        "    fig4.add_trace(\n",
        "        go.Bar(\n",
        "            x=discount_buckets_df['Discount_Bucket'],\n",
        "            y=discount_buckets_df['Total_Revenue_USD'],\n",
        "            name='Total Revenue (USD)',\n",
        "            marker_color='lightblue',\n",
        "            yaxis='y'\n",
        "        ),\n",
        "        secondary_y=False,\n",
        "    )\n",
        "\n",
        "    # Add revenue per unit line\n",
        "    fig4.add_trace(\n",
        "        go.Scatter(\n",
        "            x=discount_buckets_df['Discount_Bucket'],\n",
        "            y=discount_buckets_df['Revenue_Per_Unit'],\n",
        "            mode='lines+markers',\n",
        "            name='Revenue Per Unit (USD)',\n",
        "            line=dict(color='red', width=3),\n",
        "            marker=dict(size=8),\n",
        "            yaxis='y2'\n",
        "        ),\n",
        "        secondary_y=True,\n",
        "    )\n",
        "\n",
        "    fig4.update_xaxes(title_text=\"Discount Bucket\")\n",
        "    fig4.update_yaxes(title_text=\"Total Revenue (USD)\", secondary_y=False)\n",
        "    fig4.update_yaxes(title_text=\"Revenue Per Unit (USD)\", secondary_y=True)\n",
        "    fig4.update_layout(\n",
        "        title_text=\"Revenue Performance by Discount Bucket\",\n",
        "        height=600,\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "\n",
        "    # 5. Monthly Pricing Trends\n",
        "    fig5 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "    # Add average price line\n",
        "    fig5.add_trace(\n",
        "        go.Scatter(\n",
        "            x=monthly_pricing_df['Year_Month'],\n",
        "            y=monthly_pricing_df['Avg_Unit_Price_USD'],\n",
        "            mode='lines+markers',\n",
        "            name='Avg Unit Price (USD)',\n",
        "            line=dict(color='blue', width=3),\n",
        "            yaxis='y'\n",
        "        ),\n",
        "        secondary_y=False,\n",
        "    )\n",
        "\n",
        "    # Add discount percentage line\n",
        "    fig5.add_trace(\n",
        "        go.Scatter(\n",
        "            x=monthly_pricing_df['Year_Month'],\n",
        "            y=monthly_pricing_df['Avg_Discount_Percent'],\n",
        "            mode='lines+markers',\n",
        "            name='Avg Discount %',\n",
        "            line=dict(color='red', width=3),\n",
        "            yaxis='y2'\n",
        "        ),\n",
        "        secondary_y=True,\n",
        "    )\n",
        "\n",
        "    fig5.update_xaxes(title_text=\"Month\", tickangle=45)\n",
        "    fig5.update_yaxes(title_text=\"Average Unit Price (USD)\", secondary_y=False)\n",
        "    fig5.update_yaxes(title_text=\"Average Discount (%)\", secondary_y=True)\n",
        "    fig5.update_layout(\n",
        "        title_text=\"Monthly Pricing and Discount Trends\",\n",
        "        height=600,\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "\n",
        "    # 6. Category Performance Heatmap\n",
        "    category_pivot = discount_eff_df.pivot_table(\n",
        "        values='Revenue_Per_Unit',\n",
        "        index='Category',\n",
        "        columns='Sub Category',\n",
        "        fill_value=0\n",
        "    )\n",
        "\n",
        "    fig6 = px.imshow(\n",
        "        category_pivot.values,\n",
        "        labels=dict(x=\"Sub Category\", y=\"Category\", color=\"Revenue Per Unit\"),\n",
        "        x=category_pivot.columns,\n",
        "        y=category_pivot.index,\n",
        "        title=\"Revenue Per Unit Heatmap by Category & Sub Category\",\n",
        "        color_continuous_scale=\"RdYlGn\",\n",
        "        height=600\n",
        "    )\n",
        "    fig6.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # 7. Price vs Quantity Correlation by Category\n",
        "    fig7 = px.scatter(\n",
        "        price_elasticity_df,\n",
        "        x=\"Avg_Unit_Price_USD\",\n",
        "        y=\"Total_Quantity_Sold\",\n",
        "        color=\"Category\",\n",
        "        size=\"Total_Revenue_USD\",\n",
        "        hover_name=\"Product ID\",\n",
        "        title=\"Price vs Quantity Correlation by Category\",\n",
        "        labels={\n",
        "            \"Avg_Unit_Price_USD\": \"Average Unit Price (USD)\",\n",
        "            \"Total_Quantity_Sold\": \"Total Quantity Sold\"\n",
        "        },\n",
        "        height=600\n",
        "    )\n",
        "    fig7.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # Convert all figures to HTML\n",
        "    figures = [fig1, fig2, fig3, fig4, fig5, fig6, fig7]\n",
        "    titles = [\n",
        "        \"Product Performance Matrix: Revenue vs Discount Strategy\",\n",
        "        \"Product Price Elasticity Analysis\",\n",
        "        \"Discount Effectiveness by Category\",\n",
        "        \"Revenue Performance by Discount Bucket\",\n",
        "        \"Monthly Pricing and Discount Trends\",\n",
        "        \"Revenue Per Unit Heatmap by Category & Sub Category\",\n",
        "        \"Price vs Quantity Correlation by Category\"\n",
        "    ]\n",
        "\n",
        "    for i, (fig, title) in enumerate(zip(figures, titles)):\n",
        "        html_content += f\"\"\"\n",
        "        <div class=\"chart-container\">\n",
        "            <div class=\"chart-title\">{title}</div>\n",
        "            <div id=\"chart{i+1}\"></div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "    # Add key metrics summary\n",
        "    total_revenue = product_perf_df['Total_Revenue_USD'].sum()\n",
        "    avg_discount = product_perf_df['Avg_Discount_Percent'].mean()\n",
        "    best_performing_category = discount_eff_df.loc[discount_eff_df['Revenue_Per_Unit'].idxmax(), 'Category']\n",
        "    optimal_discount_bucket = discount_buckets_df.loc[discount_buckets_df['Revenue_Per_Unit'].idxmax(), 'Discount_Bucket']\n",
        "\n",
        "    html_content += f\"\"\"\n",
        "        <div class=\"insights-box\">\n",
        "            <strong>Key Performance Metrics:</strong><br>\n",
        "            <div class=\"metric-highlight\">Total Revenue: ${total_revenue:,.0f}</div>\n",
        "            <div class=\"metric-highlight\">Average Discount: {avg_discount:.1f}%</div>\n",
        "            <div class=\"metric-highlight\">Best Performing Category: {best_performing_category}</div>\n",
        "            <div class=\"metric-highlight\">Optimal Discount Range: {optimal_discount_bucket}</div>\n",
        "        </div>\n",
        "    \"\"\"\n",
        "\n",
        "    html_content += \"\"\"\n",
        "    <script>\n",
        "    \"\"\"\n",
        "\n",
        "    # Add JavaScript for each plot\n",
        "    for i, fig in enumerate(figures):\n",
        "        plot_json = fig.to_json()\n",
        "        html_content += f\"\"\"\n",
        "        var plotData{i+1} = {plot_json};\n",
        "        Plotly.newPlot('chart{i+1}', plotData{i+1}.data, plotData{i+1}.layout, {{responsive: true}});\n",
        "        \"\"\"\n",
        "\n",
        "    html_content += \"\"\"\n",
        "    </script>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    return html_content\n",
        "\n",
        "def save_pricing_analysis_data(data_dict):\n",
        "    \"\"\"Save pricing analysis data files\"\"\"\n",
        "    print(\"Saving pricing analysis data files...\")\n",
        "\n",
        "    os.makedirs(\"pricing_analysis/data\", exist_ok=True)\n",
        "\n",
        "    # Save all datasets\n",
        "    data_dict[\"product_performance\"].write_csv(\"pricing_analysis/data/product_performance_matrix.csv\")\n",
        "    data_dict[\"price_elasticity\"].write_csv(\"pricing_analysis/data/price_elasticity_analysis.csv\")\n",
        "    data_dict[\"discount_effectiveness\"].write_csv(\"pricing_analysis/data/discount_effectiveness.csv\")\n",
        "    data_dict[\"discount_buckets\"].write_csv(\"pricing_analysis/data/discount_bucket_analysis.csv\")\n",
        "    data_dict[\"monthly_pricing\"].write_csv(\"pricing_analysis/data/monthly_pricing_trends.csv\")\n",
        "\n",
        "    print(\"Pricing analysis data files saved successfully!\")\n",
        "\n",
        "def run_pricing_strategy_analysis():\n",
        "    \"\"\"Run comprehensive pricing and discount strategy analysis\"\"\"\n",
        "    print(\"Starting Pricing and Discount Strategy Optimization Analysis...\")\n",
        "\n",
        "    # Load data\n",
        "    master_data = load_master_data_for_pricing()\n",
        "    if master_data is None:\n",
        "        print(\"Failed to load master data!\")\n",
        "        return None\n",
        "\n",
        "    # Prepare analysis data\n",
        "    data_dict = prepare_pricing_analysis_data(master_data)\n",
        "    if data_dict is None:\n",
        "        print(\"Failed to prepare pricing data!\")\n",
        "        return None\n",
        "\n",
        "    # Save data files\n",
        "    save_pricing_analysis_data(data_dict)\n",
        "\n",
        "    # Create dashboard\n",
        "    html_content = create_pricing_strategy_dashboard(data_dict)\n",
        "\n",
        "    # Create directory and save HTML\n",
        "    os.makedirs(\"pricing_analysis\", exist_ok=True)\n",
        "\n",
        "    with open(\"pricing_analysis/pricing_strategy_dashboard.html\", \"w\", encoding='utf-8') as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "    # Print summary insights\n",
        "    product_perf_df = data_dict[\"product_performance\"].to_pandas()\n",
        "    discount_buckets_df = data_dict[\"discount_buckets\"].to_pandas()\n",
        "\n",
        "    print(\"\\nPRICING STRATEGY ANALYSIS COMPLETE!\")\n",
        "    print(f\"Total Revenue Analyzed: ${product_perf_df['Total_Revenue_USD'].sum():,.0f}\")\n",
        "    print(f\"Products Analyzed: {len(product_perf_df):,}\")\n",
        "    print(f\"Average Discount Rate: {product_perf_df['Avg_Discount_Percent'].mean():.1f}%\")\n",
        "    print(f\"Highest Revenue Product: {product_perf_df.iloc[0]['Description EN']}\")\n",
        "\n",
        "    # Find optimal discount bucket\n",
        "    optimal_bucket = discount_buckets_df.loc[discount_buckets_df['Revenue_Per_Unit'].idxmax()]\n",
        "    print(f\"Optimal Discount Range: {optimal_bucket['Discount_Bucket']} (Revenue/Unit: ${optimal_bucket['Revenue_Per_Unit']:.2f})\")\n",
        "\n",
        "    print(\"\\nDashboard and Data Files Created:\")\n",
        "    print(\"   pricing_analysis/pricing_strategy_dashboard.html\")\n",
        "    print(\"   pricing_analysis/data/ (CSV files for detailed analysis)\")\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "# Run the pricing strategy analysis\n",
        "results = run_pricing_strategy_analysis()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3tzWuBLFCxK",
        "outputId": "5fd355c0-97ed-4666-94fb-d62563fffbb8"
      },
      "id": "t3tzWuBLFCxK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Pricing and Discount Strategy Optimization Analysis...\n",
            "Loading master data for pricing analysis...\n",
            "Master data loaded: (6416029, 41)\n",
            "Preparing pricing and discount strategy analysis data...\n",
            "Sales data shape: (6077200, 41)\n",
            "Creating product-level aggregations...\n",
            "Creating price elasticity analysis...\n",
            "Creating discount effectiveness analysis...\n",
            "Creating discount bucket analysis...\n",
            "Creating temporal pricing trends...\n",
            "Pricing analysis data prepared successfully!\n",
            "Saving pricing analysis data files...\n",
            "Pricing analysis data files saved successfully!\n",
            "Creating Pricing and Discount Strategy Dashboard...\n",
            "\n",
            "PRICING STRATEGY ANALYSIS COMPLETE!\n",
            "Total Revenue Analyzed: $305,884,837\n",
            "Products Analyzed: 17,940\n",
            "Average Discount Rate: 8.9%\n",
            "Highest Revenue Product: Men'S Blazer Of Light Fabric With Cut In Thin Stripes\n",
            "Optimal Discount Range: 0-5% (Revenue/Unit: $52.59)\n",
            "\n",
            "Dashboard and Data Files Created:\n",
            "   pricing_analysis/pricing_strategy_dashboard.html\n",
            "   pricing_analysis/data/ (CSV files for detailed analysis)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5lZXUYj1FC0b"
      },
      "id": "5lZXUYj1FC0b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Workforce/Store Optimzation"
      ],
      "metadata": {
        "id": "V9K_3n41GI12"
      },
      "id": "V9K_3n41GI12"
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def load_workforce_data():\n",
        "    \"\"\"Load data for workforce performance analysis\"\"\"\n",
        "    print(\"Loading workforce data...\")\n",
        "\n",
        "    try:\n",
        "        # Load master transactions with coordinates if available, otherwise use standard file\n",
        "        if os.path.exists(\"data/master_transactions_with_coords.parquet\"):\n",
        "            master_data = pl.read_parquet(\"data/master_transactions_with_coords.parquet\")\n",
        "        else:\n",
        "            master_data = pl.read_parquet(\"data/master_transactions.parquet\")\n",
        "\n",
        "        print(f\"Master data loaded successfully: {master_data.shape}\")\n",
        "        return master_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading workforce data: {e}\")\n",
        "        return None\n",
        "\n",
        "def prepare_separate_datasets(master_data):\n",
        "    \"\"\"Prepare separate, properly aggregated datasets for each visualization\"\"\"\n",
        "    print(\"Preparing separate datasets for each visualization...\")\n",
        "\n",
        "    # Filter for sales transactions only\n",
        "    sales_data = master_data.filter(pl.col(\"Transaction Type\") == \"Sale\")\n",
        "    print(f\"Sales data shape: {sales_data.shape}\")\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(\"workforce_analysis/data\", exist_ok=True)\n",
        "\n",
        "    # 1. STORE PERFORMANCE DATASET (For Fig1 & Fig2)\n",
        "    print(\"Creating store performance dataset...\")\n",
        "    store_performance = sales_data.filter(\n",
        "        pl.col(\"Store ID\").is_not_null() &\n",
        "        pl.col(\"Store Name\").is_not_null() &\n",
        "        pl.col(\"Country\").is_not_null() &\n",
        "        pl.col(\"City\").is_not_null()\n",
        "    ).group_by([\n",
        "        \"Store ID\"\n",
        "    ]).agg([\n",
        "        pl.col(\"Store Name\").first().alias(\"Store_Name\"),\n",
        "        pl.col(\"Country\").first().alias(\"Country\"),\n",
        "        pl.col(\"City\").first().alias(\"City\"),\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Total_Revenue_USD\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Total_Quantity\"),\n",
        "        pl.col(\"Line_Total_USD\").count().alias(\"Total_Transactions\"),\n",
        "        pl.col(\"Employee ID\").n_unique().alias(\"Actual_Employees\"),\n",
        "        pl.col(\"Customer ID\").n_unique().alias(\"Unique_Customers\"),\n",
        "        pl.col(\"Product ID\").n_unique().alias(\"Unique_Products_Sold\"),\n",
        "    ]).with_columns([\n",
        "        (pl.col(\"Total_Revenue_USD\") / pl.col(\"Total_Transactions\")).alias(\"Avg_Transaction_Value\"),\n",
        "        (pl.col(\"Total_Revenue_USD\") / pl.col(\"Unique_Customers\")).alias(\"Revenue_Per_Customer\"),\n",
        "        (pl.col(\"Total_Quantity\") / pl.col(\"Total_Transactions\")).alias(\"Items_Per_Transaction\")\n",
        "    ]).sort(\"Total_Revenue_USD\", descending=True)\n",
        "\n",
        "    print(f\"Store Performance Dataset: {store_performance.shape}\")\n",
        "    store_performance.write_csv(\"workforce_analysis/data/store_performance.csv\")\n",
        "\n",
        "    # 2. STORE EFFICIENCY DATASET (For Fig3 - Efficiency Matrix)\n",
        "    print(\"Creating store efficiency dataset...\")\n",
        "    store_efficiency = store_performance.with_columns([\n",
        "        pl.col(\"Total_Revenue_USD\").rank(method=\"average\").alias(\"Revenue_Rank_Raw\"),\n",
        "        pl.col(\"Avg_Transaction_Value\").rank(method=\"average\").alias(\"Transaction_Value_Rank_Raw\"),\n",
        "        pl.col(\"Revenue_Per_Customer\").rank(method=\"average\").alias(\"Customer_Value_Rank_Raw\")\n",
        "    ]).with_columns([\n",
        "        (pl.col(\"Revenue_Rank_Raw\") / pl.col(\"Revenue_Rank_Raw\").max()).alias(\"Revenue_Percentile\"),\n",
        "        (pl.col(\"Transaction_Value_Rank_Raw\") / pl.col(\"Transaction_Value_Rank_Raw\").max()).alias(\"Transaction_Value_Percentile\"),\n",
        "        (pl.col(\"Customer_Value_Rank_Raw\") / pl.col(\"Customer_Value_Rank_Raw\").max()).alias(\"Customer_Value_Percentile\")\n",
        "    ])\n",
        "\n",
        "    print(f\"Store Efficiency Dataset: {store_efficiency.shape}\")\n",
        "    store_efficiency.write_csv(\"workforce_analysis/data/store_efficiency.csv\")\n",
        "\n",
        "    # 3. EMPLOYEE PRODUCTIVITY DATASET (For Fig4)\n",
        "    print(\"Creating employee productivity dataset...\")\n",
        "    employee_productivity = store_performance.with_columns([\n",
        "        (pl.col(\"Total_Revenue_USD\") / pl.col(\"Actual_Employees\")).alias(\"Revenue_Per_Employee\"),\n",
        "        (pl.col(\"Total_Transactions\") / pl.col(\"Actual_Employees\")).alias(\"Transactions_Per_Employee\"),\n",
        "        (pl.col(\"Unique_Customers\") / pl.col(\"Actual_Employees\")).alias(\"Customers_Per_Employee\")\n",
        "    ])\n",
        "\n",
        "    print(f\"Employee Productivity Dataset: {employee_productivity.shape}\")\n",
        "    employee_productivity.write_csv(\"workforce_analysis/data/employee_productivity.csv\")\n",
        "\n",
        "\n",
        "    # 4. LOCATION PERFORMANCE DATASET (For Fig5) - FIXED\n",
        "    print(\"Creating location performance dataset...\")\n",
        "    location_performance = store_performance.group_by([\n",
        "        \"Country\", \"City\"\n",
        "    ]).agg([\n",
        "        pl.col(\"Store ID\").count().alias(\"Number_of_Stores\"),\n",
        "        pl.col(\"Total_Revenue_USD\").sum().alias(\"Total_Location_Revenue_USD\"),\n",
        "        pl.col(\"Total_Transactions\").sum().alias(\"Total_Location_Transactions\"),\n",
        "        pl.col(\"Unique_Customers\").sum().alias(\"Total_Location_Customers\")\n",
        "    ]).with_columns([\n",
        "        (pl.col(\"Total_Location_Revenue_USD\") / pl.col(\"Number_of_Stores\")).alias(\"Avg_Revenue_Per_Store\"),\n",
        "        (pl.col(\"Total_Location_Transactions\") / pl.col(\"Number_of_Stores\")).alias(\"Avg_Transactions_Per_Store\"),\n",
        "        (pl.col(\"Total_Location_Customers\") / pl.col(\"Number_of_Stores\")).alias(\"Avg_Customers_Per_Store\")\n",
        "    ]).sort(\"Total_Location_Revenue_USD\", descending=True)\n",
        "    location_performance.write_csv(\"workforce_analysis/data/location_performance.csv\")\n",
        "\n",
        "    # 5. CUSTOMER EFFICIENCY DATASET (For Fig6)\n",
        "    print(\"Creating customer efficiency dataset...\")\n",
        "    customer_efficiency = store_performance.select([\n",
        "        \"Store ID\", \"Store_Name\", \"Country\", \"City\",\n",
        "        \"Unique_Customers\", \"Revenue_Per_Customer\", \"Total_Revenue_USD\"\n",
        "    ])\n",
        "\n",
        "    print(f\"Customer Efficiency Dataset: {customer_efficiency.shape}\")\n",
        "    customer_efficiency.write_csv(\"workforce_analysis/data/customer_efficiency.csv\")\n",
        "\n",
        "    print(\"All datasets created and saved successfully!\")\n",
        "\n",
        "    return {\n",
        "        \"store_performance\": store_performance,\n",
        "        \"store_efficiency\": store_efficiency,\n",
        "        \"employee_productivity\": employee_productivity,\n",
        "        \"location_performance\": location_performance,\n",
        "        \"customer_efficiency\": customer_efficiency\n",
        "    }\n",
        "\n",
        "def create_workforce_dashboard_from_csv():\n",
        "    \"\"\"Create dashboard by loading separate CSV files for each visualization\"\"\"\n",
        "    print(\"Creating dashboard from separate CSV datasets...\")\n",
        "\n",
        "    # Load each dataset separately\n",
        "    store_perf_df = pd.read_csv(\"workforce_analysis/data/store_performance.csv\")\n",
        "    store_eff_df = pd.read_csv(\"workforce_analysis/data/store_efficiency.csv\")\n",
        "    employee_prod_df = pd.read_csv(\"workforce_analysis/data/employee_productivity.csv\")\n",
        "    location_perf_df = pd.read_csv(\"workforce_analysis/data/location_performance.csv\")\n",
        "    customer_eff_df = pd.read_csv(\"workforce_analysis/data/customer_efficiency.csv\")\n",
        "\n",
        "    print(f\"Store Performance: {store_perf_df.shape}\")\n",
        "    print(f\"Store Efficiency: {store_eff_df.shape}\")\n",
        "    print(f\"Employee Productivity: {employee_prod_df.shape}\")\n",
        "    print(f\"Location Performance: {location_perf_df.shape}\")\n",
        "    print(f\"Customer Efficiency: {customer_eff_df.shape}\")\n",
        "\n",
        "    # Create HTML structure\n",
        "    html_content = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Workforce Optimization Dashboard</title>\n",
        "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
        "        <style>\n",
        "            body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }\n",
        "            .chart-container { background-color: white; margin: 20px 0; padding: 20px; border-radius: 10px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }\n",
        "            .chart-title { font-size: 24px; font-weight: bold; text-align: center; margin-bottom: 20px; color: #333; }\n",
        "            .dashboard-title { font-size: 36px; font-weight: bold; text-align: center; margin-bottom: 30px; color: #2c3e50; }\n",
        "            .kpi-container { display: flex; justify-content: space-around; margin: 20px 0; }\n",
        "            .kpi-box { background-color: #ffffff; padding: 20px; border-radius: 10px; text-align: center; box-shadow: 0 2px 5px rgba(0,0,0,0.1); min-width: 150px; }\n",
        "            .kpi-value { font-size: 28px; font-weight: bold; color: #2c3e50; }\n",
        "            .kpi-label { font-size: 14px; color: #7f8c8d; margin-top: 5px; }\n",
        "            .insights-box { background-color: #e8f4f8; padding: 15px; margin: 10px 0; border-radius: 8px; font-size: 14px; }\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <div class=\"dashboard-title\">Workforce Optimization Dashboard</div>\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate KPIs\n",
        "    total_stores = len(store_perf_df)\n",
        "    total_revenue = store_perf_df['Total_Revenue_USD'].sum()\n",
        "    avg_revenue_per_store = store_perf_df['Total_Revenue_USD'].mean()\n",
        "    avg_transaction_value = store_perf_df['Avg_Transaction_Value'].mean()\n",
        "    total_employees_est = employee_prod_df['Actual_Employees'].sum()\n",
        "\n",
        "    html_content += f\"\"\"\n",
        "        <div class=\"kpi-container\">\n",
        "            <div class=\"kpi-box\">\n",
        "                <div class=\"kpi-value\">{total_stores}</div>\n",
        "                <div class=\"kpi-label\">Total Stores</div>\n",
        "            </div>\n",
        "            <div class=\"kpi-box\">\n",
        "                <div class=\"kpi-value\">${total_revenue:,.0f}</div>\n",
        "                <div class=\"kpi-label\">Total Revenue</div>\n",
        "            </div>\n",
        "            <div class=\"kpi-box\">\n",
        "                <div class=\"kpi-value\">${avg_revenue_per_store:,.0f}</div>\n",
        "                <div class=\"kpi-label\">Avg Revenue/Store</div>\n",
        "            </div>\n",
        "            <div class=\"kpi-box\">\n",
        "                <div class=\"kpi-value\">${avg_transaction_value:.2f}</div>\n",
        "                <div class=\"kpi-label\">Avg Transaction Value</div>\n",
        "            </div>\n",
        "            <div class=\"kpi-box\">\n",
        "                <div class=\"kpi-value\">{total_employees_est}</div>\n",
        "                <div class=\"kpi-label\">Total Employees</div>\n",
        "            </div>\n",
        "        </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # Figure 1: Store Performance Matrix\n",
        "    fig1 = px.scatter(\n",
        "        store_perf_df.head(50),\n",
        "        x=\"Total_Revenue_USD\",\n",
        "        y=\"Avg_Transaction_Value\",\n",
        "        size=\"Total_Transactions\",\n",
        "        color=\"Country\",\n",
        "        hover_name=\"Store_Name\",\n",
        "        hover_data={\n",
        "            \"City\": True,\n",
        "            \"Total_Transactions\": \":,\",\n",
        "            \"Unique_Customers\": \":,\",\n",
        "            \"Revenue_Per_Customer\": \":,.2f\"\n",
        "        },\n",
        "        title=\"Store Performance: Revenue vs Transaction Value\",\n",
        "        labels={\n",
        "            \"Total_Revenue_USD\": \"Total Revenue (USD)\",\n",
        "            \"Avg_Transaction_Value\": \"Average Transaction Value (USD)\"\n",
        "        },\n",
        "        height=600\n",
        "    )\n",
        "    fig1.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # Figure 2: Top 20 Stores\n",
        "    fig2 = px.bar(\n",
        "        store_perf_df.head(20),\n",
        "        x=\"Store_Name\",\n",
        "        y=\"Total_Revenue_USD\",\n",
        "        color=\"Country\",\n",
        "        title=\"Top 20 Store Performance by Revenue\",\n",
        "        labels={\"Total_Revenue_USD\": \"Total Revenue (USD)\"},\n",
        "        height=600\n",
        "    )\n",
        "    fig2.update_xaxes(tickangle=45)\n",
        "    fig2.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # Figure 3: Store Efficiency Matrix\n",
        "    fig3 = px.scatter(\n",
        "        store_eff_df,\n",
        "        x=\"Revenue_Percentile\",\n",
        "        y=\"Customer_Value_Percentile\",\n",
        "        size=\"Total_Revenue_USD\",\n",
        "        color=\"Transaction_Value_Percentile\",\n",
        "        hover_name=\"Store_Name\",\n",
        "        hover_data={\n",
        "            \"Country\": True,\n",
        "            \"City\": True,\n",
        "            \"Total_Revenue_USD\": \":,.0f\",\n",
        "            \"Revenue_Per_Customer\": \":,.2f\"\n",
        "        },\n",
        "        title=\"Store Efficiency Matrix\",\n",
        "        labels={\n",
        "            \"Revenue_Percentile\": \"Revenue Performance (Percentile)\",\n",
        "            \"Customer_Value_Percentile\": \"Customer Value Performance (Percentile)\",\n",
        "            \"Transaction_Value_Percentile\": \"Transaction Value\"\n",
        "        },\n",
        "        color_continuous_scale=\"RdYlBu_r\",\n",
        "        height=600,\n",
        "        size_max=20\n",
        "    )\n",
        "\n",
        "    fig3.update_traces(\n",
        "        marker=dict(sizemin=5, sizemode='diameter', line=dict(width=1, color='white'))\n",
        "    )\n",
        "    fig3.add_hline(y=0.5, line_dash=\"dash\", line_color=\"gray\", opacity=0.7)\n",
        "    fig3.add_vline(x=0.5, line_dash=\"dash\", line_color=\"gray\", opacity=0.7)\n",
        "    fig3.add_annotation(x=0.25, y=0.75, text=\"Low Revenue<br>High Customer Value\",\n",
        "                       showarrow=False, font=dict(size=10, color=\"gray\"))\n",
        "    fig3.add_annotation(x=0.75, y=0.75, text=\"High Revenue<br>High Customer Value\",\n",
        "                       showarrow=False, font=dict(size=10, color=\"gray\"))\n",
        "    fig3.add_annotation(x=0.25, y=0.25, text=\"Low Revenue<br>Low Customer Value\",\n",
        "                       showarrow=False, font=dict(size=10, color=\"gray\"))\n",
        "    fig3.add_annotation(x=0.75, y=0.25, text=\"High Revenue<br>Low Customer Value\",\n",
        "                       showarrow=False, font=dict(size=10, color=\"gray\"))\n",
        "    fig3.update_layout(template=\"plotly_white\", showlegend=True,\n",
        "                      coloraxis_colorbar=dict(title=\"Transaction Value Percentile\", titleside=\"right\"))\n",
        "\n",
        "\n",
        "    # Figure 4: Employee Productivity\n",
        "    fig4 = px.scatter(\n",
        "        employee_prod_df.head(30),\n",
        "        x=\"Revenue_Per_Employee\",\n",
        "        y=\"Transactions_Per_Employee\",\n",
        "        size=\"Total_Revenue_USD\",\n",
        "        color=\"Country\",\n",
        "        hover_name=\"Store_Name\",\n",
        "        hover_data={\n",
        "            \"City\": True,\n",
        "            \"Actual_Employees\": True,\n",
        "            \"Customers_Per_Employee\": \":,.0f\"\n",
        "        },\n",
        "        title=\"Employee Productivity Analysis\",\n",
        "        labels={\n",
        "            \"Revenue_Per_Employee\": \"Revenue Per Employee (USD)\",\n",
        "            \"Transactions_Per_Employee\": \"Transactions Per Employee\"\n",
        "        },\n",
        "        height=600\n",
        "    )\n",
        "    fig4.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # Figure 5: Location Performance\n",
        "    fig5 = px.treemap(\n",
        "        location_perf_df,\n",
        "        path=['Country', 'City'],\n",
        "        values='Total_Location_Revenue_USD',\n",
        "        color='Avg_Revenue_Per_Store',\n",
        "        hover_data={\n",
        "            'Number_of_Stores': ':,',\n",
        "            'Total_Location_Revenue_USD': ':,.0f',\n",
        "            'Avg_Revenue_Per_Store': ':,.0f'\n",
        "        },\n",
        "        title='Location Performance by Revenue',\n",
        "        color_continuous_scale='Viridis',\n",
        "        height=600\n",
        "    )\n",
        "    fig5.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # Figure 6: Customer Efficiency\n",
        "    fig6 = px.scatter(\n",
        "        customer_eff_df,\n",
        "        x=\"Unique_Customers\",\n",
        "        y=\"Revenue_Per_Customer\",\n",
        "        size=\"Total_Revenue_USD\",\n",
        "        color=\"Country\",\n",
        "        hover_name=\"Store_Name\",\n",
        "        title=\"Customer Efficiency: Volume vs Value\",\n",
        "        labels={\n",
        "            \"Unique_Customers\": \"Number of Unique Customers\",\n",
        "            \"Revenue_Per_Customer\": \"Revenue Per Customer (USD)\"\n",
        "        },\n",
        "        height=600\n",
        "    )\n",
        "    fig6.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # Add charts to HTML\n",
        "    figures = [fig1, fig2, fig3, fig4, fig5, fig6]\n",
        "    titles = [\n",
        "        \"Store Performance: Revenue vs Transaction Value\",\n",
        "        \"Top 20 Store Performance by Revenue\",\n",
        "        \"Store Efficiency Matrix\",\n",
        "        \"Employee Productivity Analysis\",\n",
        "        \"Location Performance by Revenue\",\n",
        "        \"Customer Efficiency: Volume vs Value\"\n",
        "    ]\n",
        "\n",
        "    for i, (fig, title) in enumerate(zip(figures, titles)):\n",
        "        html_content += f\"\"\"\n",
        "        <div class=\"chart-container\">\n",
        "            <div class=\"chart-title\">{title}</div>\n",
        "            <div id=\"chart{i+1}\"></div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "    # Add insights\n",
        "    best_store = store_perf_df.iloc[0]\n",
        "    best_location = location_perf_df.iloc[0]\n",
        "    avg_productivity = employee_prod_df['Revenue_Per_Employee'].mean()\n",
        "\n",
        "    html_content += f\"\"\"\n",
        "        <div class=\"insights-box\">\n",
        "            <strong>Key Workforce Insights:</strong><br>\n",
        "            • <strong>Top Store:</strong> {best_store['Store_Name']} ({best_store['City']}, {best_store['Country']}) - ${best_store['Total_Revenue_USD']:,.0f}<br>\n",
        "            • <strong>Best Location:</strong> {best_location['City']}, {best_location['Country']} ({best_location['Number_of_Stores']} stores)<br>\n",
        "            • <strong>Avg Employee Productivity:</strong> ${avg_productivity:,.0f} revenue per employee<br>\n",
        "            • <strong>Revenue Range:</strong> ${store_perf_df['Total_Revenue_USD'].min():,.0f} - ${store_perf_df['Total_Revenue_USD'].max():,.0f}\n",
        "        </div>\n",
        "    \"\"\"\n",
        "\n",
        "    html_content += \"<script>\"\n",
        "\n",
        "    for i, fig in enumerate(figures):\n",
        "        plot_json = fig.to_json()\n",
        "        html_content += f\"\"\"\n",
        "        var plotData{i+1} = {plot_json};\n",
        "        Plotly.newPlot('chart{i+1}', plotData{i+1}.data, plotData{i+1}.layout, {{responsive: true}});\n",
        "        \"\"\"\n",
        "\n",
        "    html_content += \"\"\"\n",
        "    </script>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    return html_content\n",
        "\n",
        "def run_workforce_analysis():\n",
        "    \"\"\"Run the complete workforce analysis pipeline\"\"\"\n",
        "    print(\"Starting Workforce Analysis...\")\n",
        "\n",
        "    # Step 1: Load data\n",
        "    master_data = load_workforce_data()\n",
        "    if master_data is None:\n",
        "        return None\n",
        "\n",
        "    # Step 2: Prepare separate datasets and save to CSV\n",
        "    datasets = prepare_separate_datasets(master_data)\n",
        "\n",
        "    # Step 3: Create dashboard from CSV files\n",
        "    html_content = create_workforce_dashboard_from_csv()\n",
        "\n",
        "    # Step 4: Save dashboard\n",
        "    with open(\"workforce_analysis/workforce_dashboard.html\", \"w\", encoding='utf-8') as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "    print(\"\\nWorkforce Analysis Complete!\")\n",
        "    print(\"Files Created:\")\n",
        "    print(\"   Interactive Dashboard: workforce_analysis/workforce_dashboard.html\")\n",
        "    print(\"   Separate Data Files:\")\n",
        "    print(\"     - store_performance.csv\")\n",
        "    print(\"     - store_efficiency.csv\")\n",
        "    print(\"     - employee_productivity.csv\")\n",
        "    print(\"     - location_performance.csv\")\n",
        "    print(\"     - customer_efficiency.csv\")\n",
        "\n",
        "    return datasets\n",
        "\n",
        "# Execute the analysis\n",
        "results = run_workforce_analysis()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3CqrQW8FC28",
        "outputId": "b77faa59-b0a2-4eea-aa70-9a642eeb2d6a"
      },
      "id": "i3CqrQW8FC28",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Workforce Analysis...\n",
            "Loading workforce data...\n",
            "Master data loaded successfully: (6416029, 43)\n",
            "Preparing separate datasets for each visualization...\n",
            "Sales data shape: (6077200, 43)\n",
            "Creating store performance dataset...\n",
            "Store Performance Dataset: (35, 13)\n",
            "Creating store efficiency dataset...\n",
            "Store Efficiency Dataset: (35, 19)\n",
            "Creating employee productivity dataset...\n",
            "Employee Productivity Dataset: (35, 16)\n",
            "Creating location performance dataset...\n",
            "Creating customer efficiency dataset...\n",
            "Customer Efficiency Dataset: (35, 7)\n",
            "All datasets created and saved successfully!\n",
            "Creating dashboard from separate CSV datasets...\n",
            "Store Performance: (35, 13)\n",
            "Store Efficiency: (35, 19)\n",
            "Employee Productivity: (35, 16)\n",
            "Location Performance: (35, 9)\n",
            "Customer Efficiency: (35, 7)\n",
            "\n",
            "Workforce Analysis Complete!\n",
            "Files Created:\n",
            "   Interactive Dashboard: workforce_analysis/workforce_dashboard.html\n",
            "   Separate Data Files:\n",
            "     - store_performance.csv\n",
            "     - store_efficiency.csv\n",
            "     - employee_productivity.csv\n",
            "     - location_performance.csv\n",
            "     - customer_efficiency.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o13RkiwUFC6I"
      },
      "id": "o13RkiwUFC6I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demographic Analysis"
      ],
      "metadata": {
        "id": "r-q1CXK1Gq1R"
      },
      "id": "r-q1CXK1Gq1R"
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def load_and_explore_data():\n",
        "    \"\"\"Safely load and explore the master data\"\"\"\n",
        "    print(\"Loading Master Sales Data for Regional Demographics Analysis...\")\n",
        "\n",
        "    try:\n",
        "        # Load the master dataset\n",
        "        master_data = pl.read_parquet(\"data/master_transactions.parquet\")\n",
        "        print(f\"Master data loaded successfully!\")\n",
        "        print(f\"Shape: {master_data.shape[0]:,} rows × {master_data.shape[1]} columns\")\n",
        "\n",
        "        # Display column info with data types\n",
        "        print(f\"\\nAvailable Columns with Data Types:\")\n",
        "        for i, (col, dtype) in enumerate(zip(master_data.columns, master_data.dtypes), 1):\n",
        "            print(f\"   {i:2d}. {col} ({dtype})\")\n",
        "\n",
        "        # Check key columns for regional analysis\n",
        "        key_columns = ['Customer ID', 'City', 'Country', 'Gender', 'Date Of Birth', 'Line_Total_USD']\n",
        "        print(f\"\\nChecking Key Columns for Regional Analysis:\")\n",
        "        for col in key_columns:\n",
        "            if col in master_data.columns:\n",
        "                dtype = master_data[col].dtype\n",
        "                print(f\"   {col} - Available ({dtype})\")\n",
        "            else:\n",
        "                print(f\"   {col} - Missing\")\n",
        "\n",
        "        # Show sample data for key columns\n",
        "        if all(col in master_data.columns for col in ['City', 'Country', 'Customer ID']):\n",
        "            print(f\"\\nSample Geographic Data:\")\n",
        "            sample_geo = master_data.select(['Customer ID', 'City', 'Country', 'Gender', 'Date Of Birth']).head(5)\n",
        "            print(sample_geo)\n",
        "\n",
        "        return master_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def process_regional_data(master_data):\n",
        "    \"\"\"Process the data for regional demographics analysis\"\"\"\n",
        "    print(\"Processing Data for Regional Demographics Analysis...\")\n",
        "\n",
        "    try:\n",
        "        # Get current year for age calculation\n",
        "        current_year = datetime.now().year\n",
        "\n",
        "        # Check the data type of Date Of Birth column\n",
        "        dob_dtype = master_data['Date Of Birth'].dtype\n",
        "        print(f\"   Date Of Birth column type: {dob_dtype}\")\n",
        "\n",
        "        # Process the data step by step\n",
        "        print(\"   Processing dates and calculating ages...\")\n",
        "\n",
        "        # Handle Date Of Birth based on its actual data type\n",
        "        if str(dob_dtype) == 'Date':\n",
        "            # If it's already a Date type, extract year directly\n",
        "            processed_data = master_data.with_columns([\n",
        "                pl.when(pl.col(\"Date Of Birth\").is_not_null())\n",
        "                .then(pl.lit(current_year) - pl.col(\"Date Of Birth\").dt.year())\n",
        "                .otherwise(None)\n",
        "                .alias(\"Age\")\n",
        "            ])\n",
        "        elif 'Utf8' in str(dob_dtype) or 'String' in str(dob_dtype):\n",
        "            # If it's a string type, parse it first\n",
        "            processed_data = master_data.with_columns([\n",
        "                pl.when(pl.col(\"Date Of Birth\").is_not_null())\n",
        "                .then(\n",
        "                    pl.lit(current_year) -\n",
        "                    pl.col(\"Date Of Birth\").str.strptime(pl.Date, \"%Y-%m-%d\", strict=False).dt.year()\n",
        "                )\n",
        "                .otherwise(None)\n",
        "                .alias(\"Age\")\n",
        "            ])\n",
        "        else:\n",
        "            # For other types, try to convert to string first then parse\n",
        "            processed_data = master_data.with_columns([\n",
        "                pl.when(pl.col(\"Date Of Birth\").is_not_null())\n",
        "                .then(\n",
        "                    pl.lit(current_year) -\n",
        "                    pl.col(\"Date Of Birth\").cast(pl.Utf8).str.strptime(pl.Date, \"%Y-%m-%d\", strict=False).dt.year()\n",
        "                )\n",
        "                .otherwise(None)\n",
        "                .alias(\"Age\")\n",
        "            ])\n",
        "\n",
        "        print(f\"   Age calculation completed!\")\n",
        "\n",
        "        # Add age groups\n",
        "        print(\"   Creating age groups...\")\n",
        "        processed_data = processed_data.with_columns([\n",
        "            pl.when(pl.col(\"Age\").is_null()).then(pl.lit(\"Unknown\"))\n",
        "            .when(pl.col(\"Age\") < 18).then(pl.lit(\"Under 18\"))\n",
        "            .when(pl.col(\"Age\") < 25).then(pl.lit(\"18-24\"))\n",
        "            .when(pl.col(\"Age\") < 35).then(pl.lit(\"25-34\"))\n",
        "            .when(pl.col(\"Age\") < 45).then(pl.lit(\"35-44\"))\n",
        "            .when(pl.col(\"Age\") < 55).then(pl.lit(\"45-54\"))\n",
        "            .when(pl.col(\"Age\") < 65).then(pl.lit(\"55-64\"))\n",
        "            .otherwise(pl.lit(\"65+\"))\n",
        "            .alias(\"Age_Group\")\n",
        "        ])\n",
        "\n",
        "        # Add spending categories based on Line_Total_USD\n",
        "        print(\"   Creating spending categories...\")\n",
        "        processed_data = processed_data.with_columns([\n",
        "            pl.when(pl.col(\"Line_Total_USD\").is_null()).then(pl.lit(\"Unknown\"))\n",
        "            .when(pl.col(\"Line_Total_USD\") < 50).then(pl.lit(\"Low Spender\"))\n",
        "            .when(pl.col(\"Line_Total_USD\") < 150).then(pl.lit(\"Medium Spender\"))\n",
        "            .when(pl.col(\"Line_Total_USD\") < 300).then(pl.lit(\"High Spender\"))\n",
        "            .otherwise(pl.lit(\"Premium Spender\"))\n",
        "            .alias(\"Spending_Category\")\n",
        "        ])\n",
        "\n",
        "        # Add purchase categories based on Invoice_Total_USD\n",
        "        print(\"   Creating purchase categories...\")\n",
        "        processed_data = processed_data.with_columns([\n",
        "            pl.when(pl.col(\"Invoice_Total_USD\").is_null()).then(pl.lit(\"Unknown\"))\n",
        "            .when(pl.col(\"Invoice_Total_USD\") < 100).then(pl.lit(\"Small Purchase\"))\n",
        "            .when(pl.col(\"Invoice_Total_USD\") < 500).then(pl.lit(\"Medium Purchase\"))\n",
        "            .when(pl.col(\"Invoice_Total_USD\") < 1000).then(pl.lit(\"Large Purchase\"))\n",
        "            .otherwise(pl.lit(\"Premium Purchase\"))\n",
        "            .alias(\"Purchase_Category\")\n",
        "        ])\n",
        "\n",
        "        print(f\"Data processing completed!\")\n",
        "        print(f\"Processed {processed_data.shape[0]:,} records\")\n",
        "\n",
        "        # Show processing summary\n",
        "        print(f\"\\nProcessing Summary:\")\n",
        "        print(f\"   Countries: {processed_data['Country'].n_unique()}\")\n",
        "        print(f\"   Cities: {processed_data['City'].n_unique()}\")\n",
        "        print(f\"   Unique Customers: {processed_data['Customer ID'].n_unique():,}\")\n",
        "        print(f\"   Total Transactions: {processed_data.shape[0]:,}\")\n",
        "\n",
        "        # Show age distribution\n",
        "        age_stats = processed_data.group_by(\"Age_Group\").agg(pl.count().alias(\"Count\")).sort(\"Count\", descending=True)\n",
        "        print(f\"\\nAge Group Distribution:\")\n",
        "        for row in age_stats.iter_rows():\n",
        "            print(f\"   {row[0]}: {row[1]:,} customers\")\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data: {str(e)}\")\n",
        "        print(f\"   Debug info: Date Of Birth dtype = {master_data['Date Of Birth'].dtype}\")\n",
        "        return None\n",
        "\n",
        "def analyze_regional_demographics(data):\n",
        "    \"\"\"Analyze regional demographics\"\"\"\n",
        "    print(\"Analyzing Regional Demographics...\")\n",
        "\n",
        "    try:\n",
        "        # City-level analysis\n",
        "        print(\"   Analyzing city-level demographics...\")\n",
        "        city_stats = data.group_by([\"City\", \"Country\"]).agg([\n",
        "            pl.col(\"Customer ID\").n_unique().alias(\"Unique_Customers\"),\n",
        "            pl.count().alias(\"Total_Transactions\"),\n",
        "            pl.col(\"Age\").mean().alias(\"Avg_Age\"),\n",
        "            pl.col(\"Line_Total_USD\").sum().alias(\"Total_Revenue_USD\"),\n",
        "            pl.col(\"Line_Total_USD\").mean().alias(\"Avg_Transaction_Value_USD\"),\n",
        "            pl.col(\"Invoice_Total_USD\").mean().alias(\"Avg_Invoice_Value_USD\"),\n",
        "            pl.col(\"Quantity\").sum().alias(\"Total_Items_Sold\")\n",
        "        ]).with_columns([\n",
        "            (pl.col(\"Total_Transactions\") / pl.col(\"Unique_Customers\")).alias(\"Transactions_Per_Customer\"),\n",
        "            (pl.col(\"Total_Revenue_USD\") / pl.col(\"Unique_Customers\")).alias(\"Revenue_Per_Customer\")\n",
        "        ]).sort(\"Total_Revenue_USD\", descending=True)\n",
        "\n",
        "        # Country-level analysis\n",
        "        print(\"   Analyzing country-level demographics...\")\n",
        "        country_stats = data.group_by(\"Country\").agg([\n",
        "            pl.col(\"Customer ID\").n_unique().alias(\"Unique_Customers\"),\n",
        "            pl.count().alias(\"Total_Transactions\"),\n",
        "            pl.col(\"City\").n_unique().alias(\"Unique_Cities\"),\n",
        "            pl.col(\"Age\").mean().alias(\"Avg_Age\"),\n",
        "            pl.col(\"Line_Total_USD\").sum().alias(\"Total_Revenue_USD\"),\n",
        "            pl.col(\"Line_Total_USD\").mean().alias(\"Avg_Transaction_Value_USD\"),\n",
        "            pl.col(\"Store ID\").n_unique().alias(\"Unique_Stores\")\n",
        "        ]).sort(\"Total_Revenue_USD\", descending=True)\n",
        "\n",
        "        print(f\"Regional analysis completed!\")\n",
        "        print(f\"   {city_stats.shape[0]} cities analyzed\")\n",
        "        print(f\"   {country_stats.shape[0]} countries analyzed\")\n",
        "\n",
        "        # Show top 5 cities\n",
        "        print(f\"\\nTop 5 Cities by Revenue:\")\n",
        "        top_cities = city_stats.head(5)\n",
        "        for row in top_cities.iter_rows():\n",
        "            print(f\"   {row[0]}, {row[1]}: ${row[4]:,.2f} USD ({row[2]:,} customers)\")\n",
        "\n",
        "        return city_stats, country_stats\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in regional analysis: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "def save_analysis_data(data, city_stats, country_stats):\n",
        "    \"\"\"Save analysis data to CSV files\"\"\"\n",
        "    print(\"Saving Analysis Data...\")\n",
        "\n",
        "    try:\n",
        "        # Create output directory\n",
        "        os.makedirs(\"regional_demographics\", exist_ok=True)\n",
        "\n",
        "        # Save processed data (sample)\n",
        "        print(\"   Saving processed data sample...\")\n",
        "        data_sample = data.select([\n",
        "            \"Customer ID\", \"City\", \"Country\", \"Gender\", \"Age\", \"Age_Group\",\n",
        "            \"Line_Total_USD\", \"Spending_Category\", \"Invoice_Total_USD\", \"Purchase_Category\"\n",
        "        ]).head(10000)  # Save first 10k records as sample\n",
        "        data_sample.write_csv(\"regional_demographics/processed_data_sample.csv\")\n",
        "\n",
        "        # Save city statistics\n",
        "        print(\"   Saving city statistics...\")\n",
        "        city_stats.write_csv(\"regional_demographics/city_statistics.csv\")\n",
        "\n",
        "        # Save country statistics\n",
        "        print(\"   Saving country statistics...\")\n",
        "        country_stats.write_csv(\"regional_demographics/country_statistics.csv\")\n",
        "\n",
        "        # Create summary statistics\n",
        "        print(\"   Creating summary statistics...\")\n",
        "        summary_stats = {\n",
        "            'analysis_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            'total_countries': country_stats.shape[0],\n",
        "            'total_cities': city_stats.shape[0],\n",
        "            'total_customers': data['Customer ID'].n_unique(),\n",
        "            'total_transactions': data.shape[0],\n",
        "            'total_revenue_usd': float(data['Line_Total_USD'].sum())\n",
        "        }\n",
        "\n",
        "        # Save summary as text file\n",
        "        with open(\"regional_demographics/summary_statistics.txt\", \"w\") as f:\n",
        "            f.write(\"REGIONAL DEMOGRAPHICS ANALYSIS SUMMARY\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "            for key, value in summary_stats.items():\n",
        "                f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "        print(\"All analysis data saved successfully!\")\n",
        "        return summary_stats\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving data: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def create_regional_visualizations(data, city_stats, country_stats):\n",
        "    \"\"\"Create regional demographic visualizations\"\"\"\n",
        "    print(\"Creating Regional Demographics Visualizations...\")\n",
        "\n",
        "    figures = []\n",
        "\n",
        "    try:\n",
        "        # 1. Top Cities by Revenue\n",
        "        print(\"   Creating top cities by revenue chart...\")\n",
        "        top_20_cities = city_stats.head(20).to_pandas()\n",
        "        top_20_cities['City_Country'] = top_20_cities['City'] + ', ' + top_20_cities['Country']\n",
        "\n",
        "        fig_cities_revenue = px.bar(\n",
        "            top_20_cities,\n",
        "            x='Total_Revenue_USD',\n",
        "            y='City_Country',\n",
        "            orientation='h',\n",
        "            title='Top 20 Cities by Total Revenue (USD)',\n",
        "            color='Avg_Transaction_Value_USD',\n",
        "            color_continuous_scale='Viridis',\n",
        "            text='Total_Revenue_USD'\n",
        "        )\n",
        "        fig_cities_revenue.update_traces(texttemplate='$%{text:,.0f}', textposition='outside')\n",
        "        fig_cities_revenue.update_layout(\n",
        "            height=700,\n",
        "            title_font_size=18,\n",
        "            yaxis={'categoryorder': 'total ascending'},\n",
        "            xaxis_title=\"Total Revenue (USD)\"\n",
        "        )\n",
        "        figures.append(fig_cities_revenue)\n",
        "\n",
        "        # 2. Country Distribution\n",
        "        print(\"   Creating country distribution chart...\")\n",
        "        country_data = country_stats.to_pandas()\n",
        "\n",
        "        fig_countries = px.pie(\n",
        "            country_data,\n",
        "            values='Unique_Customers',\n",
        "            names='Country',\n",
        "            title='Customer Distribution by Country',\n",
        "            color_discrete_sequence=px.colors.qualitative.Set3\n",
        "        )\n",
        "        fig_countries.update_traces(textposition='inside', textinfo='percent+label')\n",
        "        fig_countries.update_layout(height=500, title_font_size=18)\n",
        "        figures.append(fig_countries)\n",
        "\n",
        "        # 3. Customer Performance Scatter\n",
        "        print(\"   Creating customer performance scatter plot...\")\n",
        "        fig_performance = px.scatter(\n",
        "            top_20_cities,\n",
        "            x='Unique_Customers',\n",
        "            y='Total_Revenue_USD',\n",
        "            size='Avg_Invoice_Value_USD',\n",
        "            color='Revenue_Per_Customer',\n",
        "            hover_name='City_Country',\n",
        "            title='Customer Performance Metrics by City',\n",
        "            labels={\n",
        "                'Unique_Customers': 'Number of Unique Customers',\n",
        "                'Total_Revenue_USD': 'Total Revenue (USD)',\n",
        "                'Revenue_Per_Customer': 'Revenue per Customer (USD)'\n",
        "            },\n",
        "            color_continuous_scale='Viridis'\n",
        "        )\n",
        "        fig_performance.update_layout(height=600, title_font_size=18)\n",
        "        figures.append(fig_performance)\n",
        "\n",
        "        # 4. Age Distribution by Top Cities\n",
        "        print(\"   Creating age distribution chart...\")\n",
        "        top_10_cities_list = city_stats.head(10).select([\"City\", \"Country\"]).to_pandas()\n",
        "\n",
        "        age_city_data = []\n",
        "        for _, row in top_10_cities_list.iterrows():\n",
        "            city_data = data.filter(\n",
        "                (pl.col(\"City\") == row['City']) & (pl.col(\"Country\") == row['Country'])\n",
        "            )\n",
        "            age_dist = city_data.group_by(\"Age_Group\").agg(pl.count().alias(\"Count\")).to_pandas()\n",
        "            total_count = age_dist['Count'].sum()\n",
        "            for _, age_row in age_dist.iterrows():\n",
        "                percentage = round((age_row['Count'] / total_count) * 100)\n",
        "                age_city_data.append({\n",
        "                    'City_Country': f\"{row['City']}, {row['Country']}\",\n",
        "                    'Age_Group': age_row['Age_Group'],\n",
        "                    'Count': age_row['Count'],\n",
        "                    'Percentage': f\"{percentage}%\"\n",
        "                })\n",
        "\n",
        "        if age_city_data:\n",
        "            age_city_df = pl.DataFrame(age_city_data).to_pandas()\n",
        "\n",
        "            fig_age_cities = px.bar(\n",
        "                age_city_df,\n",
        "                x='City_Country',\n",
        "                y='Count',\n",
        "                color='Age_Group',\n",
        "                title='Age Distribution by Top 10 Cities',\n",
        "                color_discrete_sequence=px.colors.qualitative.Pastel,\n",
        "                text='Percentage'\n",
        "            )\n",
        "            fig_age_cities.update_traces(textposition='inside')\n",
        "            fig_age_cities.update_layout(\n",
        "                height=600,\n",
        "                title_font_size=18,\n",
        "                xaxis_tickangle=-45\n",
        "            )\n",
        "            figures.append(fig_age_cities)\n",
        "\n",
        "        # 5. Gender Distribution\n",
        "        print(\"   Creating gender distribution chart...\")\n",
        "        gender_city_data = []\n",
        "        for _, row in top_10_cities_list.iterrows():\n",
        "            city_data = data.filter(\n",
        "                (pl.col(\"City\") == row['City']) & (pl.col(\"Country\") == row['Country'])\n",
        "            )\n",
        "            gender_dist = city_data.group_by(\"Gender\").agg(pl.count().alias(\"Count\")).to_pandas()\n",
        "            total_count = gender_dist['Count'].sum()\n",
        "            for _, gender_row in gender_dist.iterrows():\n",
        "                percentage = round((gender_row['Count'] / total_count) * 100)\n",
        "                gender_city_data.append({\n",
        "                    'City_Country': f\"{row['City']}, {row['Country']}\",\n",
        "                    'Gender': gender_row['Gender'],\n",
        "                    'Count': gender_row['Count'],\n",
        "                    'Percentage': f\"{percentage}%\"\n",
        "                })\n",
        "\n",
        "        if gender_city_data:\n",
        "            gender_city_df = pl.DataFrame(gender_city_data).to_pandas()\n",
        "\n",
        "            fig_gender_cities = px.bar(\n",
        "                gender_city_df,\n",
        "                x='City_Country',\n",
        "                y='Count',\n",
        "                color='Gender',\n",
        "                title='Gender Distribution by Top 10 Cities',\n",
        "                color_discrete_sequence=['#FF69B4', '#4169E1', '#32CD32'],\n",
        "                text='Percentage'\n",
        "            )\n",
        "            fig_gender_cities.update_traces(textposition='inside')\n",
        "            fig_gender_cities.update_layout(\n",
        "                height=600,\n",
        "                title_font_size=18,\n",
        "                xaxis_tickangle=-45\n",
        "            )\n",
        "            figures.append(fig_gender_cities)\n",
        "\n",
        "        # 6. Revenue Heatmap by Country and Age Group\n",
        "        print(\"   Creating revenue heatmap...\")\n",
        "\n",
        "        # Get top 10 countries and create heatmap data\n",
        "        top_countries = country_stats.head(10).select(\"Country\").to_series().to_list()\n",
        "        age_groups = ['18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
        "\n",
        "        heatmap_data = []\n",
        "        heatmap_countries = []\n",
        "\n",
        "        for country in top_countries:\n",
        "            country_data = data.filter(pl.col(\"Country\") == country)\n",
        "            row_data = []\n",
        "            for age_group in age_groups:\n",
        "                revenue = country_data.filter(pl.col(\"Age_Group\") == age_group)[\"Line_Total_USD\"].sum()\n",
        "                row_data.append(float(revenue))\n",
        "            heatmap_data.append(row_data)\n",
        "            heatmap_countries.append(country)\n",
        "\n",
        "        fig_heatmap = go.Figure(data=go.Heatmap(\n",
        "            z=heatmap_data,\n",
        "            x=age_groups,\n",
        "            y=heatmap_countries,\n",
        "            colorscale='Viridis',\n",
        "            text=[[f\"${val:,.0f}\" for val in row] for row in heatmap_data],\n",
        "            texttemplate=\"%{text}\",\n",
        "            textfont={\"size\": 10}\n",
        "        ))\n",
        "\n",
        "        fig_heatmap.update_layout(\n",
        "            title='Revenue Heatmap by Country and Age Group',\n",
        "            title_font_size=18,\n",
        "            height=500,\n",
        "            xaxis_title=\"Age Group\",\n",
        "            yaxis_title=\"Country\"\n",
        "        )\n",
        "        figures.append(fig_heatmap)\n",
        "\n",
        "        print(f\"Created {len(figures)} visualizations successfully!\")\n",
        "        return figures\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating visualizations: {str(e)}\")\n",
        "        return figures\n",
        "\n",
        "def create_dashboard(figures, summary_stats):\n",
        "    \"\"\"Create HTML dashboard\"\"\"\n",
        "    print(\"Creating Regional Demographics Dashboard...\")\n",
        "\n",
        "    try:\n",
        "        html_content = f\"\"\"\n",
        "        <!DOCTYPE html>\n",
        "        <html>\n",
        "        <head>\n",
        "            <title>Regional Demographics Analysis Dashboard</title>\n",
        "            <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
        "            <style>\n",
        "                body {{\n",
        "                    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "                    margin: 0;\n",
        "                    padding: 20px;\n",
        "                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "                    min-height: 100vh;\n",
        "                }}\n",
        "                .container {{\n",
        "                    max-width: 1400px;\n",
        "                    margin: 0 auto;\n",
        "                    background: white;\n",
        "                    border-radius: 15px;\n",
        "                    padding: 30px;\n",
        "                    box-shadow: 0 20px 40px rgba(0,0,0,0.1);\n",
        "                }}\n",
        "                .header {{\n",
        "                    text-align: center;\n",
        "                    margin-bottom: 40px;\n",
        "                    padding: 20px;\n",
        "                    background: linear-gradient(135deg, #4CAF50, #45a049);\n",
        "                    border-radius: 10px;\n",
        "                    color: white;\n",
        "                }}\n",
        "                .stats-grid {{\n",
        "                    display: grid;\n",
        "                    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n",
        "                    gap: 20px;\n",
        "                    margin-bottom: 30px;\n",
        "                }}\n",
        "                .stat-card {{\n",
        "                    background: #f8f9fa;\n",
        "                    padding: 20px;\n",
        "                    border-radius: 10px;\n",
        "                    text-align: center;\n",
        "                    border-left: 4px solid #4CAF50;\n",
        "                }}\n",
        "                .chart-container {{\n",
        "                    margin: 30px 0;\n",
        "                    padding: 20px;\n",
        "                    border: 2px solid #e0e0e0;\n",
        "                    border-radius: 10px;\n",
        "                    background: #fafafa;\n",
        "                }}\n",
        "                .chart {{\n",
        "                    width: 100%;\n",
        "                    height: 600px;\n",
        "                }}\n",
        "                .footer {{\n",
        "                    text-align: center;\n",
        "                    margin-top: 40px;\n",
        "                    padding: 20px;\n",
        "                    background: #f5f5f5;\n",
        "                    border-radius: 10px;\n",
        "                    color: #666;\n",
        "                }}\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <div class=\"container\">\n",
        "                <div class=\"header\">\n",
        "                    <h1>Regional Demographics Analysis Dashboard</h1>\n",
        "                    <p>Comprehensive Geographic Analysis of Customer Data</p>\n",
        "                    <p>Generated on: {summary_stats['analysis_date']}</p>\n",
        "                </div>\n",
        "\n",
        "                <div class=\"stats-grid\">\n",
        "                    <div class=\"stat-card\">\n",
        "                        <h3>Countries</h3>\n",
        "                        <h2>{summary_stats['total_countries']}</h2>\n",
        "                    </div>\n",
        "                    <div class=\"stat-card\">\n",
        "                        <h3>Cities</h3>\n",
        "                        <h2>{summary_stats['total_cities']}</h2>\n",
        "                    </div>\n",
        "                    <div class=\"stat-card\">\n",
        "                        <h3>Customers</h3>\n",
        "                        <h2>{summary_stats['total_customers']:,}</h2>\n",
        "                    </div>\n",
        "                    <div class=\"stat-card\">\n",
        "                        <h3>Transactions</h3>\n",
        "                        <h2>{summary_stats['total_transactions']:,}</h2>\n",
        "                    </div>\n",
        "                    <div class=\"stat-card\">\n",
        "                        <h3>Total Revenue</h3>\n",
        "                        <h2>${summary_stats['total_revenue_usd']:,.0f}</h2>\n",
        "                    </div>\n",
        "                </div>\n",
        "        \"\"\"\n",
        "\n",
        "        # Add chart containers\n",
        "        for i in range(len(figures)):\n",
        "            html_content += f\"\"\"\n",
        "                <div class=\"chart-container\">\n",
        "                    <div id=\"chart{i+1}\" class=\"chart\"></div>\n",
        "                </div>\n",
        "            \"\"\"\n",
        "\n",
        "        html_content += \"\"\"\n",
        "                <div class=\"footer\">\n",
        "                    <p>Regional Demographics Analysis | Based on Actual Transaction Data</p>\n",
        "                </div>\n",
        "            </div>\n",
        "            <script>\n",
        "        \"\"\"\n",
        "\n",
        "        # Add JavaScript for each plot\n",
        "        for i, fig in enumerate(figures):\n",
        "            plot_json = fig.to_json()\n",
        "            html_content += f\"\"\"\n",
        "            var plotData{i+1} = {plot_json};\n",
        "            Plotly.newPlot('chart{i+1}', plotData{i+1}.data, plotData{i+1}.layout, {{responsive: true}});\n",
        "            \"\"\"\n",
        "\n",
        "        html_content += \"\"\"\n",
        "            </script>\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\"\n",
        "\n",
        "        # Save dashboard\n",
        "        with open(\"regional_demographics/regional_demographics_dashboard.html\", \"w\", encoding='utf-8') as f:\n",
        "            f.write(html_content)\n",
        "\n",
        "        print(\"Dashboard created successfully!\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating dashboard: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def run_regional_demographics_analysis():\n",
        "    \"\"\"Run complete regional demographics analysis\"\"\"\n",
        "    print(\"Starting Regional Demographics Analysis...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Step 1: Load and explore data\n",
        "    master_data = load_and_explore_data()\n",
        "    if master_data is None:\n",
        "        return None\n",
        "\n",
        "    # Step 2: Process data\n",
        "    processed_data = process_regional_data(master_data)\n",
        "    if processed_data is None:\n",
        "        return None\n",
        "\n",
        "    # Step 3: Analyze regional demographics\n",
        "    city_stats, country_stats = analyze_regional_demographics(processed_data)\n",
        "    if city_stats is None or country_stats is None:\n",
        "        return None\n",
        "\n",
        "    # Step 4: Save analysis data\n",
        "    summary_stats = save_analysis_data(processed_data, city_stats, country_stats)\n",
        "    if summary_stats is None:\n",
        "        return None\n",
        "\n",
        "    # Step 5: Create visualizations\n",
        "    figures = create_regional_visualizations(processed_data, city_stats, country_stats)\n",
        "\n",
        "    # Step 6: Create dashboard\n",
        "    dashboard_success = create_dashboard(figures, summary_stats)\n",
        "\n",
        "    if dashboard_success:\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"REGIONAL DEMOGRAPHICS ANALYSIS COMPLETED!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"\\nFILES CREATED:\")\n",
        "        print(\"   regional_demographics/regional_demographics_dashboard.html\")\n",
        "        print(\"   regional_demographics/city_statistics.csv\")\n",
        "        print(\"   regional_demographics/country_statistics.csv\")\n",
        "        print(\"   regional_demographics/processed_data_sample.csv\")\n",
        "        print(\"   regional_demographics/summary_statistics.txt\")\n",
        "\n",
        "        print(f\"\\nANALYSIS SUMMARY:\")\n",
        "        print(f\"   Countries Analyzed: {summary_stats['total_countries']}\")\n",
        "        print(f\"   Cities Analyzed: {summary_stats['total_cities']}\")\n",
        "        print(f\"   Unique Customers: {summary_stats['total_customers']:,}\")\n",
        "        print(f\"   Total Transactions: {summary_stats['total_transactions']:,}\")\n",
        "        print(f\"   Total Revenue: ${summary_stats['total_revenue_usd']:,.2f} USD\")\n",
        "\n",
        "        print(f\"\\nOpen 'regional_demographics/regional_demographics_dashboard.html' to view the complete analysis!\")\n",
        "\n",
        "        return {\n",
        "            'processed_data': processed_data,\n",
        "            'city_stats': city_stats,\n",
        "            'country_stats': country_stats,\n",
        "            'figures': figures,\n",
        "            'summary_stats': summary_stats\n",
        "        }\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Execute Regional Demographics Analysis\n",
        "print(\"REGIONAL DEMOGRAPHICS INTEGRATION\")\n",
        "print(\"=\" * 60)\n",
        "regional_results = run_regional_demographics_analysis()\n",
        "\n",
        "if regional_results:\n",
        "    print(\"\\nRegional Demographics Analysis completed successfully!\")\n",
        "else:\n",
        "    print(\"\\nRegional Demographics Analysis failed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGp2vbfCFC9E",
        "outputId": "af4fc2c3-27c7-4189-9860-ec8a93a70ccc"
      },
      "id": "NGp2vbfCFC9E",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REGIONAL DEMOGRAPHICS INTEGRATION\n",
            "============================================================\n",
            "Starting Regional Demographics Analysis...\n",
            "============================================================\n",
            "Loading Master Sales Data for Regional Demographics Analysis...\n",
            "Master data loaded successfully!\n",
            "Shape: 6,416,029 rows × 41 columns\n",
            "\n",
            "Available Columns with Data Types:\n",
            "    1. Invoice ID (String)\n",
            "    2. Line (Int64)\n",
            "    3. Customer ID (Int64)\n",
            "    4. Product ID (Int64)\n",
            "    5. Size (String)\n",
            "    6. Color (String)\n",
            "    7. Unit Price (Float64)\n",
            "    8. Quantity (Int64)\n",
            "    9. Date (Date)\n",
            "   10. Discount (Float64)\n",
            "   11. Line Total (Float64)\n",
            "   12. Store ID (Int64)\n",
            "   13. Employee ID (Int64)\n",
            "   14. Currency (String)\n",
            "   15. Currency Symbol (String)\n",
            "   16. SKU (String)\n",
            "   17. Transaction Type (String)\n",
            "   18. Payment Method (String)\n",
            "   19. Invoice Total (Float64)\n",
            "   20. Exchange_Rate_to_USD (Float64)\n",
            "   21. Unit_Price_USD (Float64)\n",
            "   22. Line_Total_USD (Float64)\n",
            "   23. Invoice_Total_USD (Float64)\n",
            "   24. Category (String)\n",
            "   25. Sub Category (String)\n",
            "   26. Description EN (String)\n",
            "   27. Color_right (String)\n",
            "   28. Sizes (String)\n",
            "   29. Production Cost (Float64)\n",
            "   30. Name (String)\n",
            "   31. Email (String)\n",
            "   32. Gender (String)\n",
            "   33. Date Of Birth (Date)\n",
            "   34. Job Title (String)\n",
            "   35. Country (String)\n",
            "   36. City (String)\n",
            "   37. Store Name (String)\n",
            "   38. Number of Employees (Int64)\n",
            "   39. Store ID_right (Int64)\n",
            "   40. Name_right (String)\n",
            "   41. Position (String)\n",
            "\n",
            "Checking Key Columns for Regional Analysis:\n",
            "   Customer ID - Available (Int64)\n",
            "   City - Available (String)\n",
            "   Country - Available (String)\n",
            "   Gender - Available (String)\n",
            "   Date Of Birth - Available (Date)\n",
            "   Line_Total_USD - Available (Float64)\n",
            "\n",
            "Sample Geographic Data:\n",
            "shape: (5, 5)\n",
            "┌─────────────┬──────────┬───────────────┬────────┬───────────────┐\n",
            "│ Customer ID ┆ City     ┆ Country       ┆ Gender ┆ Date Of Birth │\n",
            "│ ---         ┆ ---      ┆ ---           ┆ ---    ┆ ---           │\n",
            "│ i64         ┆ str      ┆ str           ┆ str    ┆ date          │\n",
            "╞═════════════╪══════════╪═══════════════╪════════╪═══════════════╡\n",
            "│ 47162       ┆ New York ┆ United States ┆ F      ┆ 1983-12-25    │\n",
            "│ 47162       ┆ New York ┆ United States ┆ F      ┆ 1983-12-25    │\n",
            "│ 47162       ┆ New York ┆ United States ┆ F      ┆ 1983-12-25    │\n",
            "│ 10142       ┆ New York ┆ United States ┆ F      ┆ 1974-12-12    │\n",
            "│ 10142       ┆ New York ┆ United States ┆ F      ┆ 1974-12-12    │\n",
            "└─────────────┴──────────┴───────────────┴────────┴───────────────┘\n",
            "Processing Data for Regional Demographics Analysis...\n",
            "   Date Of Birth column type: Date\n",
            "   Processing dates and calculating ages...\n",
            "   Age calculation completed!\n",
            "   Creating age groups...\n",
            "   Creating spending categories...\n",
            "   Creating purchase categories...\n",
            "Data processing completed!\n",
            "Processed 6,416,029 records\n",
            "\n",
            "Processing Summary:\n",
            "   Countries: 7\n",
            "   Cities: 35\n",
            "   Unique Customers: 1,283,707\n",
            "   Total Transactions: 6,416,029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4111830008.py:142: DeprecationWarning:\n",
            "\n",
            "`pl.count()` is deprecated. Please use `pl.len()` instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Age Group Distribution:\n",
            "   18-24: 2,302,156 customers\n",
            "   25-34: 1,669,823 customers\n",
            "   35-44: 1,411,650 customers\n",
            "   45-54: 669,500 customers\n",
            "   55-64: 294,461 customers\n",
            "   65+: 68,439 customers\n",
            "Analyzing Regional Demographics...\n",
            "   Analyzing city-level demographics...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4111830008.py:163: DeprecationWarning:\n",
            "\n",
            "`pl.count()` is deprecated. Please use `pl.len()` instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Analyzing country-level demographics...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4111830008.py:178: DeprecationWarning:\n",
            "\n",
            "`pl.count()` is deprecated. Please use `pl.len()` instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regional analysis completed!\n",
            "   35 cities analyzed\n",
            "   7 countries analyzed\n",
            "\n",
            "Top 5 Cities by Revenue:\n",
            "   New York, United States: $29.97 USD (102,758 customers)\n",
            "   Los Angeles, United States: $29.97 USD (94,317 customers)\n",
            "   上海, China: $30.54 USD (84,955 customers)\n",
            "   广州, China: $30.39 USD (92,209 customers)\n",
            "   深圳, China: $30.53 USD (90,888 customers)\n",
            "Saving Analysis Data...\n",
            "   Saving processed data sample...\n",
            "   Saving city statistics...\n",
            "   Saving country statistics...\n",
            "   Creating summary statistics...\n",
            "All analysis data saved successfully!\n",
            "Creating Regional Demographics Visualizations...\n",
            "   Creating top cities by revenue chart...\n",
            "   Creating country distribution chart...\n",
            "   Creating customer performance scatter plot...\n",
            "   Creating age distribution chart...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4111830008.py:326: DeprecationWarning:\n",
            "\n",
            "`pl.count()` is deprecated. Please use `pl.len()` instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Creating gender distribution chart...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4111830008.py:364: DeprecationWarning:\n",
            "\n",
            "`pl.count()` is deprecated. Please use `pl.len()` instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Creating revenue heatmap...\n",
            "Created 6 visualizations successfully!\n",
            "Creating Regional Demographics Dashboard...\n",
            "Dashboard created successfully!\n",
            "\n",
            "============================================================\n",
            "REGIONAL DEMOGRAPHICS ANALYSIS COMPLETED!\n",
            "============================================================\n",
            "\n",
            "FILES CREATED:\n",
            "   regional_demographics/regional_demographics_dashboard.html\n",
            "   regional_demographics/city_statistics.csv\n",
            "   regional_demographics/country_statistics.csv\n",
            "   regional_demographics/processed_data_sample.csv\n",
            "   regional_demographics/summary_statistics.txt\n",
            "\n",
            "ANALYSIS SUMMARY:\n",
            "   Countries Analyzed: 7\n",
            "   Cities Analyzed: 35\n",
            "   Unique Customers: 1,283,707\n",
            "   Total Transactions: 6,416,029\n",
            "   Total Revenue: $288,786,449.85 USD\n",
            "\n",
            "Open 'regional_demographics/regional_demographics_dashboard.html' to view the complete analysis!\n",
            "\n",
            "Regional Demographics Analysis completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.offline as pyo\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "\n",
        "def load_master_data_for_demographics():\n",
        "    \"\"\"Load master data for demographics analysis\"\"\"\n",
        "    print(\"Loading master data for demographics analysis...\")\n",
        "\n",
        "    try:\n",
        "        if os.path.exists(\"data/master_transactions_with_coords.parquet\"):\n",
        "            master_data = pl.read_parquet(\"data/master_transactions_with_coords.parquet\")\n",
        "        else:\n",
        "            master_data = pl.read_parquet(\"data/master_transactions.parquet\")\n",
        "\n",
        "        print(f\"Master data loaded: {master_data.shape}\")\n",
        "        print(f\"Available columns: {master_data.columns}\")\n",
        "\n",
        "        # Check for demographic columns\n",
        "        demographic_cols = ['Date Of Birth', 'Gender', 'Age', 'Age Group', 'Occupation', 'Income Level']\n",
        "        available_demo_cols = [col for col in demographic_cols if col in master_data.columns]\n",
        "        print(f\"Available demographic columns: {available_demo_cols}\")\n",
        "\n",
        "        return master_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading master data: {e}\")\n",
        "        return None\n",
        "\n",
        "def prepare_demographics_analysis_data(master_data):\n",
        "    \"\"\"Prepare comprehensive demographics analysis data with age calculation\"\"\"\n",
        "    print(\"Preparing demographics analysis data...\")\n",
        "\n",
        "    # Filter for sales only\n",
        "    sales_data = master_data.filter(pl.col(\"Transaction Type\") == \"Sale\")\n",
        "    print(f\"Sales data shape: {sales_data.shape}\")\n",
        "\n",
        "    # Calculate age from Date Of Birth\n",
        "    current_date = datetime.now().date()\n",
        "    print(f\"Calculating age from Date Of Birth (current date: {current_date})\")\n",
        "\n",
        "    # Add age calculations and demographics\n",
        "    sales_data = sales_data.with_columns([\n",
        "        # Calculate age from Date Of Birth\n",
        "        ((pl.lit(current_date) - pl.col(\"Date Of Birth\")).dt.total_days() / 365.25).floor().cast(pl.Int32).alias(\"Age\"),\n",
        "\n",
        "        # Add time dimensions\n",
        "        pl.col(\"Date\").dt.strftime(\"%Y-%m\").alias(\"Year_Month\"),\n",
        "        pl.col(\"Date\").dt.quarter().alias(\"Quarter\"),\n",
        "        pl.col(\"Date\").dt.weekday().alias(\"Day_of_Week\"),\n",
        "        pl.col(\"Date\").dt.strftime(\"%A\").alias(\"Day_Name\"),\n",
        "\n",
        "        # Calculate discount metrics\n",
        "        (pl.col(\"Unit_Price_USD\") * pl.col(\"Quantity\") - pl.col(\"Line_Total_USD\")).alias(\"Discount_Amount_USD\"),\n",
        "        ((pl.col(\"Unit_Price_USD\") * pl.col(\"Quantity\") - pl.col(\"Line_Total_USD\")) /\n",
        "         (pl.col(\"Unit_Price_USD\") * pl.col(\"Quantity\")) * 100).alias(\"Discount_Percent\"),\n",
        "\n",
        "        # Customer identifier for aggregation\n",
        "        pl.col(\"Customer ID\").alias(\"Customer_ID\")\n",
        "    ])\n",
        "\n",
        "    # Create age groups\n",
        "    sales_data = sales_data.with_columns([\n",
        "        pl.when(pl.col(\"Age\") < 18)\n",
        "        .then(pl.lit(\"Under 18\"))\n",
        "        .when(pl.col(\"Age\") < 25)\n",
        "        .then(pl.lit(\"18-24\"))\n",
        "        .when(pl.col(\"Age\") < 35)\n",
        "        .then(pl.lit(\"25-34\"))\n",
        "        .when(pl.col(\"Age\") < 45)\n",
        "        .then(pl.lit(\"35-44\"))\n",
        "        .when(pl.col(\"Age\") < 55)\n",
        "        .then(pl.lit(\"45-54\"))\n",
        "        .when(pl.col(\"Age\") < 65)\n",
        "        .then(pl.lit(\"55-64\"))\n",
        "        .otherwise(pl.lit(\"65+\"))\n",
        "        .alias(\"Age_Group\")\n",
        "    ])\n",
        "\n",
        "    # Check age calculation results\n",
        "    age_stats = sales_data.select([\n",
        "        pl.col(\"Age\").min().alias(\"Min_Age\"),\n",
        "        pl.col(\"Age\").max().alias(\"Max_Age\"),\n",
        "        pl.col(\"Age\").mean().alias(\"Avg_Age\"),\n",
        "        # pl.col(\"Age_Group\").value_counts().alias(\"Age_Group_Counts\")\n",
        "    ])\n",
        "\n",
        "    print(\"Age calculation results:\")\n",
        "    print(f\"Age range: {age_stats.select('Min_Age').item()} - {age_stats.select('Max_Age').item()}\")\n",
        "    print(f\"Average age: {age_stats.select('Avg_Age').item():.1f}\")\n",
        "\n",
        "    # Show age group distribution\n",
        "    age_group_dist = sales_data.group_by(\"Age_Group\").agg(pl.count().alias(\"count\")).sort(\"count\", descending=True)\n",
        "    print(\"Age group distribution:\")\n",
        "    for row in age_group_dist.iter_rows():\n",
        "        print(f\"   {row[0]}: {row[1]:,} transactions\")\n",
        "\n",
        "    print(\"Creating age-based demographics analysis...\")\n",
        "\n",
        "    # 1. Age Demographics Analysis\n",
        "    age_demographics = sales_data.group_by([\"Age\", \"Age_Group\"]).agg([\n",
        "        pl.col(\"Customer_ID\").n_unique().alias(\"Unique_Customers\"),\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Total_Revenue_USD\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Total_Quantity\"),\n",
        "        pl.col(\"Line_Total_USD\").count().alias(\"Total_Transactions\"),\n",
        "        pl.col(\"Unit_Price_USD\").mean().alias(\"Avg_Unit_Price_USD\"),\n",
        "        (pl.col(\"Discount_Amount_USD\").sum() / (pl.col(\"Unit_Price_USD\") * pl.col(\"Quantity\")).sum() * 100).alias(\"Avg_Discount_Percent\"),\n",
        "        pl.col(\"Category\").n_unique().alias(\"Categories_Purchased\"),\n",
        "        pl.col(\"Store ID\").n_unique().alias(\"Stores_Visited\")\n",
        "    ]).with_columns([\n",
        "        (pl.col(\"Total_Revenue_USD\") / pl.col(\"Unique_Customers\")).alias(\"Revenue_Per_Customer\"),\n",
        "        (pl.col(\"Total_Transactions\") / pl.col(\"Unique_Customers\")).alias(\"Transactions_Per_Customer\"),\n",
        "        (pl.col(\"Total_Revenue_USD\") / pl.col(\"Total_Transactions\")).alias(\"Average_Order_Value\")\n",
        "    ]).sort(\"Age\")\n",
        "\n",
        "    # 2. Age Group Summary (for easier visualization)\n",
        "    age_group_demographics = sales_data.group_by(\"Age_Group\").agg([\n",
        "        pl.col(\"Customer_ID\").n_unique().alias(\"Unique_Customers\"),\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Total_Revenue_USD\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Total_Quantity\"),\n",
        "        pl.col(\"Line_Total_USD\").count().alias(\"Total_Transactions\"),\n",
        "        pl.col(\"Unit_Price_USD\").mean().alias(\"Avg_Unit_Price_USD\"),\n",
        "        (pl.col(\"Discount_Amount_USD\").sum() / (pl.col(\"Unit_Price_USD\") * pl.col(\"Quantity\")).sum() * 100).alias(\"Avg_Discount_Percent\"),\n",
        "        pl.col(\"Category\").n_unique().alias(\"Categories_Purchased\"),\n",
        "        pl.col(\"Store ID\").n_unique().alias(\"Stores_Visited\"),\n",
        "        pl.col(\"Age\").mean().alias(\"Average_Age\")\n",
        "    ]).with_columns([\n",
        "        (pl.col(\"Total_Revenue_USD\") / pl.col(\"Unique_Customers\")).alias(\"Revenue_Per_Customer\"),\n",
        "        (pl.col(\"Total_Transactions\") / pl.col(\"Unique_Customers\")).alias(\"Transactions_Per_Customer\"),\n",
        "        (pl.col(\"Total_Revenue_USD\") / pl.col(\"Total_Transactions\")).alias(\"Average_Order_Value\")\n",
        "    ])\n",
        "\n",
        "    # Sort age groups in logical order\n",
        "    age_group_order = [\"Under 18\", \"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65+\"]\n",
        "    age_group_demographics = age_group_demographics.with_columns([\n",
        "        pl.col(\"Age_Group\").map_elements(\n",
        "            lambda x: age_group_order.index(x) if x in age_group_order else 999,\n",
        "            return_dtype=pl.Int32\n",
        "        ).alias(\"Age_Group_Order\")\n",
        "    ]).sort(\"Age_Group_Order\").drop(\"Age_Group_Order\")\n",
        "\n",
        "    # 3. Gender Demographics Analysis\n",
        "    if \"Gender\" in sales_data.columns:\n",
        "        gender_demographics = sales_data.group_by(\"Gender\").agg([\n",
        "            pl.col(\"Customer_ID\").n_unique().alias(\"Unique_Customers\"),\n",
        "            pl.col(\"Line_Total_USD\").sum().alias(\"Total_Revenue_USD\"),\n",
        "            pl.col(\"Quantity\").sum().alias(\"Total_Quantity\"),\n",
        "            pl.col(\"Line_Total_USD\").count().alias(\"Total_Transactions\"),\n",
        "            pl.col(\"Unit_Price_USD\").mean().alias(\"Avg_Unit_Price_USD\"),\n",
        "            (pl.col(\"Discount_Amount_USD\").sum() / (pl.col(\"Unit_Price_USD\") * pl.col(\"Quantity\")).sum() * 100).alias(\"Avg_Discount_Percent\"),\n",
        "            pl.col(\"Category\").n_unique().alias(\"Categories_Purchased\"),\n",
        "            pl.col(\"Store ID\").n_unique().alias(\"Stores_Visited\"),\n",
        "            pl.col(\"Age\").mean().alias(\"Average_Age\")\n",
        "        ]).with_columns([\n",
        "            (pl.col(\"Total_Revenue_USD\") / pl.col(\"Unique_Customers\")).alias(\"Revenue_Per_Customer\"),\n",
        "            (pl.col(\"Total_Transactions\") / pl.col(\"Unique_Customers\")).alias(\"Transactions_Per_Customer\"),\n",
        "            (pl.col(\"Total_Revenue_USD\") / pl.col(\"Total_Transactions\")).alias(\"Average_Order_Value\")\n",
        "        ])\n",
        "    else:\n",
        "        print(\"Gender column not found - creating placeholder\")\n",
        "        gender_demographics = None\n",
        "\n",
        "    print(\"Creating category preferences by demographics...\")\n",
        "\n",
        "    # 4. Age Group vs Category Analysis\n",
        "    age_category_analysis = sales_data.group_by([\"Age_Group\", \"Category\"]).agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Revenue_USD\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Quantity_Sold\"),\n",
        "        pl.col(\"Customer_ID\").n_unique().alias(\"Unique_Customers\"),\n",
        "        pl.col(\"Line_Total_USD\").count().alias(\"Transactions\")\n",
        "    ]).with_columns([\n",
        "        (pl.col(\"Revenue_USD\") / pl.col(\"Unique_Customers\")).alias(\"Revenue_Per_Customer\"),\n",
        "        (pl.col(\"Revenue_USD\") / pl.col(\"Transactions\")).alias(\"Average_Order_Value\")\n",
        "    ]).sort([\"Age_Group\", \"Revenue_USD\"], descending=[False, True])\n",
        "\n",
        "    # 5. Gender vs Category Analysis (if gender available)\n",
        "    if \"Gender\" in sales_data.columns:\n",
        "        gender_category_analysis = sales_data.group_by([\"Gender\", \"Category\"]).agg([\n",
        "            pl.col(\"Line_Total_USD\").sum().alias(\"Revenue_USD\"),\n",
        "            pl.col(\"Quantity\").sum().alias(\"Quantity_Sold\"),\n",
        "            pl.col(\"Customer_ID\").n_unique().alias(\"Unique_Customers\"),\n",
        "            pl.col(\"Line_Total_USD\").count().alias(\"Transactions\")\n",
        "        ]).with_columns([\n",
        "            (pl.col(\"Revenue_USD\") / pl.col(\"Unique_Customers\")).alias(\"Revenue_Per_Customer\"),\n",
        "            (pl.col(\"Revenue_USD\") / pl.col(\"Transactions\")).alias(\"Average_Order_Value\")\n",
        "        ]).sort([\"Gender\", \"Revenue_USD\"], descending=[False, True])\n",
        "    else:\n",
        "        gender_category_analysis = None\n",
        "\n",
        "    print(\"Creating customer lifetime value by demographics...\")\n",
        "\n",
        "    # 6. Customer-Level Demographics with CLV\n",
        "    customer_demographics_clv = sales_data.group_by([\n",
        "        \"Customer_ID\", \"Age\", \"Age_Group\", \"Gender\" if \"Gender\" in sales_data.columns else pl.lit(\"Unknown\").alias(\"Gender\")\n",
        "    ]).agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Total_Spent_USD\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Total_Items\"),\n",
        "        pl.col(\"Line_Total_USD\").count().alias(\"Total_Transactions\"),\n",
        "        pl.col(\"Date\").min().alias(\"First_Purchase\"),\n",
        "        pl.col(\"Date\").max().alias(\"Last_Purchase\"),\n",
        "        pl.col(\"Category\").n_unique().alias(\"Categories_Explored\"),\n",
        "        pl.col(\"Store ID\").n_unique().alias(\"Stores_Visited\"),\n",
        "        pl.col(\"Unit_Price_USD\").mean().alias(\"Avg_Unit_Price\"),\n",
        "        (pl.col(\"Discount_Amount_USD\").sum() / (pl.col(\"Unit_Price_USD\") * pl.col(\"Quantity\")).sum() * 100).alias(\"Avg_Discount_Percent\")\n",
        "    ]).with_columns([\n",
        "        (pl.col(\"Total_Spent_USD\") / pl.col(\"Total_Transactions\")).alias(\"Average_Order_Value\"),\n",
        "        (pl.col(\"Last_Purchase\") - pl.col(\"First_Purchase\")).dt.total_days().alias(\"Customer_Lifespan_Days\")\n",
        "    ])\n",
        "\n",
        "    # Convert to pandas for advanced CLV calculations\n",
        "    customer_clv_df = customer_demographics_clv.to_pandas()\n",
        "    customer_clv_df['Customer_Lifespan_Days'] = customer_clv_df['Customer_Lifespan_Days'].fillna(1).replace(0, 1)\n",
        "    customer_clv_df['Monthly_Frequency'] = (customer_clv_df['Total_Transactions'] / customer_clv_df['Customer_Lifespan_Days'] * 30).fillna(1.0)\n",
        "\n",
        "    # Simple CLV calculation\n",
        "    customer_clv_df['Predicted_CLV'] = customer_clv_df['Average_Order_Value'] * customer_clv_df['Monthly_Frequency'] * 12\n",
        "    customer_clv_df['Total_CLV'] = customer_clv_df['Total_Spent_USD'] + customer_clv_df['Predicted_CLV']\n",
        "\n",
        "    print(\"Creating temporal demographics patterns...\")\n",
        "\n",
        "    # 7. Monthly Demographics Trends\n",
        "    monthly_age_trends = sales_data.group_by([\"Year_Month\", \"Age_Group\"]).agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Monthly_Revenue_USD\"),\n",
        "        pl.col(\"Customer_ID\").n_unique().alias(\"Active_Customers\"),\n",
        "        pl.col(\"Line_Total_USD\").count().alias(\"Transactions\")\n",
        "    ]).sort([\"Year_Month\", \"Age_Group\"])\n",
        "\n",
        "    # 8. Day of Week Demographics Patterns\n",
        "    dow_demographics = sales_data.group_by([\"Day_Name\", \"Age_Group\"]).agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Revenue_USD\"),\n",
        "        pl.col(\"Customer_ID\").n_unique().alias(\"Unique_Customers\"),\n",
        "        pl.col(\"Line_Total_USD\").count().alias(\"Transactions\")\n",
        "    ])\n",
        "\n",
        "    print(\"Creating discount sensitivity by demographics...\")\n",
        "\n",
        "    # 9. Discount Sensitivity by Demographics\n",
        "    discount_sensitivity = sales_data.with_columns([\n",
        "        pl.when(pl.col(\"Discount_Percent\") <= 5)\n",
        "        .then(pl.lit(\"0-5%\"))\n",
        "        .when(pl.col(\"Discount_Percent\") <= 15)\n",
        "        .then(pl.lit(\"6-15%\"))\n",
        "        .when(pl.col(\"Discount_Percent\") <= 25)\n",
        "        .then(pl.lit(\"16-25%\"))\n",
        "        .when(pl.col(\"Discount_Percent\") <= 35)\n",
        "        .then(pl.lit(\"26-35%\"))\n",
        "        .otherwise(pl.lit(\"35%+\"))\n",
        "        .alias(\"Discount_Bucket\")\n",
        "    ]).group_by([\"Age_Group\", \"Discount_Bucket\"]).agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Revenue_USD\"),\n",
        "        pl.col(\"Customer_ID\").n_unique().alias(\"Customers\"),\n",
        "        pl.col(\"Line_Total_USD\").count().alias(\"Transactions\")\n",
        "    ])\n",
        "\n",
        "    print(\"Demographics analysis data prepared successfully!\")\n",
        "\n",
        "    return {\n",
        "        \"sales_data\": sales_data,\n",
        "        \"age_demographics\": age_demographics,\n",
        "        \"age_group_demographics\": age_group_demographics,\n",
        "        \"gender_demographics\": gender_demographics,\n",
        "        \"age_category_analysis\": age_category_analysis,\n",
        "        \"gender_category_analysis\": gender_category_analysis,\n",
        "        \"customer_demographics_clv\": pl.from_pandas(customer_clv_df),\n",
        "        \"monthly_age_trends\": monthly_age_trends,\n",
        "        \"dow_demographics\": dow_demographics,\n",
        "        \"discount_sensitivity\": discount_sensitivity\n",
        "    }\n",
        "\n",
        "def create_demographics_dashboard(data_dict):\n",
        "    \"\"\"Create comprehensive demographics analysis dashboard\"\"\"\n",
        "    print(\"Creating Demographics Analysis Dashboard...\")\n",
        "\n",
        "    # Convert to pandas for plotting\n",
        "    age_demo_df = data_dict[\"age_group_demographics\"].to_pandas()\n",
        "    age_category_df = data_dict[\"age_category_analysis\"].to_pandas()\n",
        "    customer_clv_df = data_dict[\"customer_demographics_clv\"].to_pandas()\n",
        "    monthly_trends_df = data_dict[\"monthly_age_trends\"].to_pandas()\n",
        "    dow_demo_df = data_dict[\"dow_demographics\"].to_pandas()\n",
        "    discount_sens_df = data_dict[\"discount_sensitivity\"].to_pandas()\n",
        "\n",
        "    # Handle gender data\n",
        "    if data_dict[\"gender_demographics\"] is not None:\n",
        "        gender_demo_df = data_dict[\"gender_demographics\"].to_pandas()\n",
        "        gender_category_df = data_dict[\"gender_category_analysis\"].to_pandas()\n",
        "    else:\n",
        "        gender_demo_df = None\n",
        "        gender_category_df = None\n",
        "\n",
        "    # Create HTML structure\n",
        "    html_content = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Customer Demographics Analysis Dashboard</title>\n",
        "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
        "        <style>\n",
        "            body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }\n",
        "            .chart-container { background-color: white; margin: 20px 0; padding: 20px; border-radius: 10px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }\n",
        "            .chart-title { font-size: 24px; font-weight: bold; text-align: center; margin-bottom: 20px; color: #333; }\n",
        "            .dashboard-title { font-size: 36px; font-weight: bold; text-align: center; margin-bottom: 30px; color: #2c3e50; }\n",
        "            .insights-box { background-color: #e8f4f8; padding: 15px; margin: 10px 0; border-radius: 8px; font-size: 14px; }\n",
        "            .metric-highlight { background-color: #fff3cd; padding: 10px; margin: 5px 0; border-radius: 5px; font-weight: bold; }\n",
        "            .demo-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 15px; margin: 20px 0; }\n",
        "            .demo-card { background-color: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #007bff; }\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <div class=\"dashboard-title\">Customer Demographics Analysis Dashboard</div>\n",
        "\n",
        "        <div class=\"insights-box\">\n",
        "            <strong>Demographics Analysis Insights:</strong><br>\n",
        "            • <strong>Age Distribution</strong>: Customer behavior patterns across different age groups<br>\n",
        "            • <strong>Gender Preferences</strong>: Product and category preferences by gender<br>\n",
        "            • <strong>Demographic CLV</strong>: Customer lifetime value analysis by demographics<br>\n",
        "            • <strong>Purchase Patterns</strong>: Temporal and behavioral patterns by demographic segments<br>\n",
        "            • <strong>Discount Sensitivity</strong>: How different demographics respond to pricing strategies\n",
        "        </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # Create figures (same as before but without emojis in titles)\n",
        "    # 1. Age Distribution Analysis\n",
        "    fig1 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "\n",
        "    fig1.add_trace(\n",
        "        go.Bar(\n",
        "            x=age_demo_df['Age_Group'],\n",
        "            y=age_demo_df['Unique_Customers'],\n",
        "            name='Number of Customers',\n",
        "            marker_color='lightblue',\n",
        "            yaxis='y'\n",
        "        ),\n",
        "        secondary_y=False,\n",
        "    )\n",
        "\n",
        "    fig1.add_trace(\n",
        "        go.Scatter(\n",
        "            x=age_demo_df['Age_Group'],\n",
        "            y=age_demo_df['Revenue_Per_Customer'],\n",
        "            mode='lines+markers',\n",
        "            name='Revenue Per Customer (USD)',\n",
        "            line=dict(color='red', width=3),\n",
        "            marker=dict(size=8),\n",
        "            yaxis='y2'\n",
        "        ),\n",
        "        secondary_y=True,\n",
        "    )\n",
        "\n",
        "    fig1.update_xaxes(title_text=\"Age Group\")\n",
        "    fig1.update_yaxes(title_text=\"Number of Customers\", secondary_y=False)\n",
        "    fig1.update_yaxes(title_text=\"Revenue Per Customer (USD)\", secondary_y=True)\n",
        "    fig1.update_layout(\n",
        "        title_text=\"Customer Distribution and Revenue by Age Group\",\n",
        "        height=600,\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "\n",
        "    # 2. Age vs Category Heatmap\n",
        "    age_cat_pivot = age_category_df.pivot_table(\n",
        "        values='Revenue_USD',\n",
        "        index='Age_Group',\n",
        "        columns='Category',\n",
        "        fill_value=0\n",
        "    )\n",
        "\n",
        "    fig2 = px.imshow(\n",
        "        age_cat_pivot.values,\n",
        "        labels=dict(x=\"Category\", y=\"Age Group\", color=\"Revenue (USD)\"),\n",
        "        x=age_cat_pivot.columns,\n",
        "        y=age_cat_pivot.index,\n",
        "        title=\"Category Preferences by Age Group (Revenue Heatmap)\",\n",
        "        color_continuous_scale=\"Viridis\",\n",
        "        height=600\n",
        "    )\n",
        "    fig2.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # 3. Customer Lifetime Value by Age\n",
        "    fig3 = px.box(\n",
        "        customer_clv_df,\n",
        "        x=\"Age_Group\",\n",
        "        y=\"Total_CLV\",\n",
        "        title=\"Customer Lifetime Value Distribution by Age Group\",\n",
        "        height=600\n",
        "    )\n",
        "    fig3.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # 4. Gender Analysis (if available)\n",
        "    if gender_demo_df is not None:\n",
        "        fig4 = px.pie(\n",
        "            gender_demo_df,\n",
        "            values='Unique_Customers',\n",
        "            names='Gender',\n",
        "            title=\"Customer Distribution by Gender\",\n",
        "            height=500\n",
        "        )\n",
        "        fig4.update_traces(textposition='inside', textinfo='percent+label')\n",
        "        fig4.update_layout(template=\"plotly_white\")\n",
        "    else:\n",
        "        fig4 = go.Figure()\n",
        "        fig4.add_annotation(\n",
        "            text=\"Gender data not available in dataset\",\n",
        "            xref=\"paper\", yref=\"paper\",\n",
        "            x=0.5, y=0.5, xanchor='center', yanchor='middle',\n",
        "            showarrow=False, font=dict(size=20)\n",
        "        )\n",
        "        fig4.update_layout(\n",
        "            title=\"Gender Analysis - Data Not Available\",\n",
        "            height=500,\n",
        "            template=\"plotly_white\"\n",
        "        )\n",
        "\n",
        "    # 5. Monthly Trends by Age Group\n",
        "    fig5 = px.line(\n",
        "        monthly_trends_df,\n",
        "        x=\"Year_Month\",\n",
        "        y=\"Monthly_Revenue_USD\",\n",
        "        color=\"Age_Group\",\n",
        "        title=\"Monthly Revenue Trends by Age Group\",\n",
        "        markers=True,\n",
        "        height=600\n",
        "    )\n",
        "    fig5.update_xaxes(tickangle=45)\n",
        "    fig5.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # 6. Day of Week Patterns by Age\n",
        "    fig6 = px.bar(\n",
        "        dow_demo_df,\n",
        "        x=\"Day_Name\",\n",
        "        y=\"Revenue_USD\",\n",
        "        color=\"Age_Group\",\n",
        "        title=\"Daily Purchase Patterns by Age Group\",\n",
        "        height=600\n",
        "    )\n",
        "    fig6.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # 7. Discount Sensitivity by Age Group\n",
        "    fig7 = px.bar(\n",
        "        discount_sens_df,\n",
        "        x=\"Age_Group\",\n",
        "        y=\"Revenue_USD\",\n",
        "        color=\"Discount_Bucket\",\n",
        "        title=\"Discount Sensitivity by Age Group\",\n",
        "        height=600\n",
        "    )\n",
        "    fig7.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # 8. Age vs Purchase Behavior Scatter\n",
        "    fig8 = px.scatter(\n",
        "        customer_clv_df,\n",
        "        x=\"Average_Order_Value\",\n",
        "        y=\"Total_Transactions\",\n",
        "        color=\"Age_Group\",\n",
        "        size=\"Total_CLV\",\n",
        "        hover_data={\n",
        "            \"Customer_ID\": True,\n",
        "            \"Total_Spent_USD\": \":,.2f\",\n",
        "            \"Categories_Explored\": True,\n",
        "            \"Stores_Visited\": True\n",
        "        },\n",
        "        title=\"Purchase Behavior: Order Value vs Frequency by Age\",\n",
        "        height=700\n",
        "    )\n",
        "    fig8.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # 9. Gender vs Category Analysis (if available)\n",
        "    if gender_category_df is not None:\n",
        "        fig9 = px.bar(\n",
        "            gender_category_df,\n",
        "            x=\"Category\",\n",
        "            y=\"Revenue_USD\",\n",
        "            color=\"Gender\",\n",
        "            title=\"Category Preferences by Gender\",\n",
        "            height=600\n",
        "        )\n",
        "        fig9.update_xaxes(tickangle=45)\n",
        "        fig9.update_layout(template=\"plotly_white\")\n",
        "    else:\n",
        "        fig9 = go.Figure()\n",
        "        fig9.add_annotation(\n",
        "            text=\"Gender category analysis not available\",\n",
        "            xref=\"paper\", yref=\"paper\",\n",
        "            x=0.5, y=0.5, xanchor='center', yanchor='middle',\n",
        "            showarrow=False, font=dict(size=20)\n",
        "        )\n",
        "        fig9.update_layout(\n",
        "            title=\"Gender vs Category Analysis - Data Not Available\",\n",
        "            height=500,\n",
        "            template=\"plotly_white\"\n",
        "        )\n",
        "\n",
        "    # 10. Demographics Summary Metrics\n",
        "    fig10 = go.Figure()\n",
        "\n",
        "    fig10.add_trace(go.Bar(\n",
        "        x=age_demo_df['Age_Group'],\n",
        "        y=age_demo_df['Total_Revenue_USD'],\n",
        "        name='Total Revenue',\n",
        "        marker_color='skyblue'\n",
        "    ))\n",
        "\n",
        "    fig10.update_layout(\n",
        "        title=\"Total Revenue by Age Group\",\n",
        "        xaxis_title=\"Age Group\",\n",
        "        yaxis_title=\"Total Revenue (USD)\",\n",
        "        height=500,\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "\n",
        "    # Convert all figures to HTML\n",
        "    figures = [fig1, fig2, fig3, fig4, fig5, fig6, fig7, fig8, fig9, fig10]\n",
        "    titles = [\n",
        "        \"Customer Distribution and Revenue by Age Group\",\n",
        "        \"Category Preferences by Age Group (Revenue Heatmap)\",\n",
        "        \"Customer Lifetime Value Distribution by Age Group\",\n",
        "        \"Customer Distribution by Gender\",\n",
        "        \"Monthly Revenue Trends by Age Group\",\n",
        "        \"Daily Purchase Patterns by Age Group\",\n",
        "        \"Discount Sensitivity by Age Group\",\n",
        "        \"Purchase Behavior: Order Value vs Frequency by Age\",\n",
        "        \"Category Preferences by Gender\",\n",
        "        \"Total Revenue by Age Group\"\n",
        "    ]\n",
        "\n",
        "    for i, (fig, title) in enumerate(zip(figures, titles)):\n",
        "        html_content += f\"\"\"\n",
        "        <div class=\"chart-container\">\n",
        "            <div class=\"chart-title\">{title}</div>\n",
        "            <div id=\"chart{i+1}\"></div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "    # Add demographic insights summary\n",
        "    total_customers = age_demo_df['Unique_Customers'].sum()\n",
        "    total_revenue = age_demo_df['Total_Revenue_USD'].sum()\n",
        "    highest_clv_age = customer_clv_df.groupby('Age_Group')['Total_CLV'].mean().idxmax()\n",
        "    most_active_age = age_demo_df.loc[age_demo_df['Transactions_Per_Customer'].idxmax(), 'Age_Group']\n",
        "\n",
        "    html_content += f\"\"\"\n",
        "        <div class=\"insights-box\">\n",
        "            <strong>Key Demographics Insights:</strong><br>\n",
        "            <div class=\"metric-highlight\">Total Customers Analyzed: {total_customers:,}</div>\n",
        "            <div class=\"metric-highlight\">Total Revenue: ${total_revenue:,.0f}</div>\n",
        "            <div class=\"metric-highlight\">Highest CLV Age Group: {highest_clv_age}</div>\n",
        "            <div class=\"metric-highlight\">Most Active Age Group: {most_active_age}</div>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"chart-container\">\n",
        "            <div class=\"chart-title\">Age Group Performance Summary</div>\n",
        "            <div class=\"demo-grid\">\n",
        "    \"\"\"\n",
        "\n",
        "    # Add age group summary cards\n",
        "    for _, row in age_demo_df.iterrows():\n",
        "        html_content += f\"\"\"\n",
        "            <div class=\"demo-card\">\n",
        "                <h3>{row['Age_Group']}</h3>\n",
        "                <p><strong>Customers:</strong> {row['Unique_Customers']:,}</p>\n",
        "                <p><strong>Revenue:</strong> ${row['Total_Revenue_USD']:,.0f}</p>\n",
        "                <p><strong>Avg Order Value:</strong> ${row['Average_Order_Value']:.2f}</p>\n",
        "                <p><strong>Revenue/Customer:</strong> ${row['Revenue_Per_Customer']:,.2f}</p>\n",
        "                <p><strong>Avg Discount:</strong> {row['Avg_Discount_Percent']:.1f}%</p>\n",
        "                <p><strong>Avg Age:</strong> {row['Average_Age']:.1f} years</p>\n",
        "            </div>\n",
        "        \"\"\"\n",
        "\n",
        "    html_content += \"\"\"\n",
        "            </div>\n",
        "        </div>\n",
        "    \"\"\"\n",
        "\n",
        "    html_content += \"\"\"\n",
        "    <script>\n",
        "    \"\"\"\n",
        "\n",
        "    # Add JavaScript for each plot\n",
        "    for i, fig in enumerate(figures):\n",
        "        plot_json = fig.to_json()\n",
        "        html_content += f\"\"\"\n",
        "        var plotData{i+1} = {plot_json};\n",
        "        Plotly.newPlot('chart{i+1}', plotData{i+1}.data, plotData{i+1}.layout, {{responsive: true}});\n",
        "        \"\"\"\n",
        "\n",
        "    html_content += \"\"\"\n",
        "    </script>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    return html_content\n",
        "\n",
        "\n",
        "def save_demographics_analysis_data(data_dict):\n",
        "    \"\"\"Save demographics analysis data files\"\"\"\n",
        "    print(\"Saving demographics analysis data files...\")\n",
        "\n",
        "    os.makedirs(\"demographics_analysis/data\", exist_ok=True)\n",
        "\n",
        "    # Save all datasets\n",
        "    data_dict[\"age_demographics\"].write_csv(\"demographics_analysis/data/age_demographics_detailed.csv\")\n",
        "    data_dict[\"age_group_demographics\"].write_csv(\"demographics_analysis/data/age_group_demographics_summary.csv\")\n",
        "    data_dict[\"age_category_analysis\"].write_csv(\"demographics_analysis/data/age_category_preferences.csv\")\n",
        "    data_dict[\"customer_demographics_clv\"].write_csv(\"demographics_analysis/data/customer_demographics_clv.csv\")\n",
        "    data_dict[\"monthly_age_trends\"].write_csv(\"demographics_analysis/data/monthly_age_trends.csv\")\n",
        "    data_dict[\"dow_demographics\"].write_csv(\"demographics_analysis/data/day_of_week_demographics.csv\")\n",
        "    data_dict[\"discount_sensitivity\"].write_csv(\"demographics_analysis/data/discount_sensitivity_by_age.csv\")\n",
        "\n",
        "    # Save gender data if available\n",
        "    if data_dict[\"gender_demographics\"] is not None:\n",
        "        data_dict[\"gender_demographics\"].write_csv(\"demographics_analysis/data/gender_demographics_analysis.csv\")\n",
        "        data_dict[\"gender_category_analysis\"].write_csv(\"demographics_analysis/data/gender_category_preferences.csv\")\n",
        "\n",
        "    print(\"Demographics analysis data files saved!\")\n",
        "\n",
        "def run_demographics_analysis():\n",
        "    \"\"\"Run comprehensive demographics analysis\"\"\"\n",
        "    print(\"Starting Customer Demographics Analysis...\")\n",
        "\n",
        "    # Load data\n",
        "    master_data = load_master_data_for_demographics()\n",
        "    if master_data is None:\n",
        "        print(\"Failed to load master data!\")\n",
        "        return None\n",
        "\n",
        "    # Prepare analysis data\n",
        "    data_dict = prepare_demographics_analysis_data(master_data)\n",
        "    if data_dict is None:\n",
        "        print(\"Failed to prepare demographics data!\")\n",
        "        return None\n",
        "\n",
        "    # Save data files\n",
        "    save_demographics_analysis_data(data_dict)\n",
        "\n",
        "    # Create dashboard\n",
        "    html_content = create_demographics_dashboard(data_dict)\n",
        "\n",
        "    # Create directory and save HTML\n",
        "    os.makedirs(\"demographics_analysis\", exist_ok=True)\n",
        "\n",
        "    with open(\"demographics_analysis/demographics_analysis_dashboard.html\", \"w\", encoding='utf-8') as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "    # Print summary insights\n",
        "    age_demo_df = data_dict[\"age_group_demographics\"].to_pandas()\n",
        "    customer_clv_df = data_dict[\"customer_demographics_clv\"].to_pandas()\n",
        "\n",
        "    print(\"\\nCUSTOMER DEMOGRAPHICS ANALYSIS COMPLETE!\")\n",
        "    print(f\"Total Customers Analyzed: {age_demo_df['Unique_Customers'].sum():,}\")\n",
        "    print(f\"Total Revenue Analyzed: ${age_demo_df['Total_Revenue_USD'].sum():,.0f}\")\n",
        "    print(f\"Age Groups Analyzed: {len(age_demo_df)}\")\n",
        "\n",
        "    # Age group insights\n",
        "    top_age_group = age_demo_df.loc[age_demo_df['Total_Revenue_USD'].idxmax()]\n",
        "    print(f\"Top Revenue Age Group: {top_age_group['Age_Group']} (${top_age_group['Total_Revenue_USD']:,.0f})\")\n",
        "\n",
        "    highest_aov_age = age_demo_df.loc[age_demo_df['Average_Order_Value'].idxmax()]\n",
        "    print(f\"Highest AOV Age Group: {highest_aov_age['Age_Group']} (${highest_aov_age['Average_Order_Value']:.2f})\")\n",
        "\n",
        "    # CLV insights\n",
        "    avg_clv_by_age = customer_clv_df.groupby('Age_Group')['Total_CLV'].mean()\n",
        "    highest_clv_age = avg_clv_by_age.idxmax()\n",
        "    print(f\"Highest CLV Age Group: {highest_clv_age} (${avg_clv_by_age[highest_clv_age]:,.2f})\")\n",
        "\n",
        "    print(\"\\nDashboard and Data Files Created:\")\n",
        "    print(\"   demographics_analysis/demographics_analysis_dashboard.html\")\n",
        "    print(\"   demographics_analysis/data/ (CSV files for detailed analysis)\")\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "def create_advanced_demographic_insights(data_dict):\n",
        "    \"\"\"Create advanced demographic insights and recommendations\"\"\"\n",
        "    print(\"Generating advanced demographic insights...\")\n",
        "\n",
        "    # Convert key datasets to pandas for analysis\n",
        "    age_demo_df = data_dict[\"age_group_demographics\"].to_pandas()\n",
        "    customer_clv_df = data_dict[\"customer_demographics_clv\"].to_pandas()\n",
        "    age_category_df = data_dict[\"age_category_analysis\"].to_pandas()\n",
        "\n",
        "    insights = {}\n",
        "\n",
        "    # 1. Age Group Performance Analysis\n",
        "    insights['age_performance'] = {}\n",
        "    for _, row in age_demo_df.iterrows():\n",
        "        age_group = row['Age_Group']\n",
        "        insights['age_performance'][age_group] = {\n",
        "            'customers': int(row['Unique_Customers']),\n",
        "            'revenue': float(row['Total_Revenue_USD']),\n",
        "            'revenue_per_customer': float(row['Revenue_Per_Customer']),\n",
        "            'avg_order_value': float(row['Average_Order_Value']),\n",
        "            'transactions_per_customer': float(row['Transactions_Per_Customer']),\n",
        "            'discount_sensitivity': float(row['Avg_Discount_Percent']),\n",
        "            'market_share': float(row['Unique_Customers'] / age_demo_df['Unique_Customers'].sum() * 100),\n",
        "            'average_age': float(row['Average_Age'])\n",
        "        }\n",
        "\n",
        "    # 2. Category Preferences by Age\n",
        "    insights['category_preferences'] = {}\n",
        "    for age_group in age_category_df['Age_Group'].unique():\n",
        "        age_data = age_category_df[age_category_df['Age_Group'] == age_group]\n",
        "        top_categories = age_data.nlargest(3, 'Revenue_USD')\n",
        "\n",
        "        insights['category_preferences'][age_group] = {\n",
        "            'top_categories': top_categories[['Category', 'Revenue_USD']].to_dict('records'),\n",
        "            'category_diversity': len(age_data),\n",
        "            'total_revenue': float(age_data['Revenue_USD'].sum())\n",
        "        }\n",
        "\n",
        "    # 3. CLV Analysis by Demographics\n",
        "    clv_by_age = customer_clv_df.groupby('Age_Group').agg({\n",
        "        'Total_CLV': ['mean', 'median', 'std', 'count'],\n",
        "        'Total_Spent_USD': 'mean',\n",
        "        'Predicted_CLV': 'mean',\n",
        "        'Average_Order_Value': 'mean',\n",
        "        'Total_Transactions': 'mean'\n",
        "    }).round(2)\n",
        "\n",
        "    insights['clv_analysis'] = clv_by_age.to_dict()\n",
        "\n",
        "    # 4. Strategic Recommendations\n",
        "    insights['recommendations'] = {}\n",
        "\n",
        "    # Find highest value age groups\n",
        "    top_revenue_age = age_demo_df.loc[age_demo_df['Total_Revenue_USD'].idxmax(), 'Age_Group']\n",
        "    top_clv_age = customer_clv_df.groupby('Age_Group')['Total_CLV'].mean().idxmax()\n",
        "    highest_aov_age = age_demo_df.loc[age_demo_df['Average_Order_Value'].idxmax(), 'Age_Group']\n",
        "    most_frequent_age = age_demo_df.loc[age_demo_df['Transactions_Per_Customer'].idxmax(), 'Age_Group']\n",
        "\n",
        "    insights['recommendations'] = {\n",
        "        'focus_segments': {\n",
        "            'primary_revenue_driver': top_revenue_age,\n",
        "            'highest_lifetime_value': top_clv_age,\n",
        "            'premium_customers': highest_aov_age,\n",
        "            'most_engaged': most_frequent_age\n",
        "        },\n",
        "        'strategies': {\n",
        "            top_revenue_age: \"Primary revenue driver - focus on retention and expansion\",\n",
        "            top_clv_age: \"Highest CLV - invest in premium experiences and loyalty programs\",\n",
        "            highest_aov_age: \"Premium segment - target with high-value products\",\n",
        "            most_frequent_age: \"Most engaged - leverage for referrals and advocacy\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # 5. Market Opportunities\n",
        "    insights['opportunities'] = {\n",
        "        'underperforming_segments': [],\n",
        "        'growth_potential': [],\n",
        "        'category_expansion': {}\n",
        "    }\n",
        "\n",
        "    # Identify underperforming segments (low revenue per customer)\n",
        "    median_rpc = age_demo_df['Revenue_Per_Customer'].median()\n",
        "    underperforming = age_demo_df[age_demo_df['Revenue_Per_Customer'] < median_rpc * 0.8]\n",
        "\n",
        "    for _, row in underperforming.iterrows():\n",
        "        insights['opportunities']['underperforming_segments'].append({\n",
        "            'age_group': row['Age_Group'],\n",
        "            'customers': int(row['Unique_Customers']),\n",
        "            'current_rpc': float(row['Revenue_Per_Customer']),\n",
        "            'potential_uplift': float(median_rpc - row['Revenue_Per_Customer'])\n",
        "        })\n",
        "\n",
        "    return insights\n",
        "\n",
        "def generate_demographic_report(insights):\n",
        "    \"\"\"Generate a comprehensive demographic report\"\"\"\n",
        "    print(\"Generating demographic insights report...\")\n",
        "\n",
        "    report = f\"\"\"\n",
        "\n",
        "    Customer Demographics Analysis Report\n",
        "    ====================================\n",
        "\n",
        "    EXECUTIVE SUMMARY\n",
        "    =================\n",
        "\n",
        "    KEY FINDINGS:\n",
        "    • Primary Revenue Driver: {insights['recommendations']['focus_segments']['primary_revenue_driver']}\n",
        "    • Highest CLV Segment: {insights['recommendations']['focus_segments']['highest_lifetime_value']}\n",
        "    • Premium Customer Group: {insights['recommendations']['focus_segments']['premium_customers']}\n",
        "    • Most Engaged Segment: {insights['recommendations']['focus_segments']['most_engaged']}\n",
        "\n",
        "    AGE GROUP PERFORMANCE ANALYSIS\n",
        "    ==============================\n",
        "    \"\"\"\n",
        "\n",
        "    # Add age group details\n",
        "    for age_group, data in insights['age_performance'].items():\n",
        "        report += f\"\"\"\n",
        "    {age_group.upper()}:\n",
        "       • Customers: {data['customers']:,} ({data['market_share']:.1f}% of total)\n",
        "       • Average Age: {data['average_age']:.1f} years\n",
        "       • Revenue: ${data['revenue']:,.0f}\n",
        "       • Revenue per Customer: ${data['revenue_per_customer']:,.2f}\n",
        "       • Average Order Value: ${data['avg_order_value']:.2f}\n",
        "       • Purchase Frequency: {data['transactions_per_customer']:.1f} transactions/customer\n",
        "       • Discount Sensitivity: {data['discount_sensitivity']:.1f}%\n",
        "    \"\"\"\n",
        "\n",
        "    report += f\"\"\"\n",
        "\n",
        "    CATEGORY PREFERENCES BY AGE GROUP\n",
        "    =================================\n",
        "    \"\"\"\n",
        "\n",
        "    # Add category preferences\n",
        "    for age_group, prefs in insights['category_preferences'].items():\n",
        "        report += f\"\"\"\n",
        "    {age_group.upper()}:\n",
        "       • Category Diversity: {prefs['category_diversity']} categories explored\n",
        "       • Total Category Revenue: ${prefs['total_revenue']:,.0f}\n",
        "       • Top 3 Categories:\"\"\"\n",
        "\n",
        "        for i, cat in enumerate(prefs['top_categories'][:3], 1):\n",
        "            report += f\"\"\"\n",
        "         {i}. {cat['Category']}: ${cat['Revenue_USD']:,.0f}\"\"\"\n",
        "\n",
        "    report += f\"\"\"\n",
        "\n",
        "    STRATEGIC RECOMMENDATIONS\n",
        "    =========================\n",
        "    \"\"\"\n",
        "\n",
        "    for age_group, strategy in insights['recommendations']['strategies'].items():\n",
        "        report += f\"\"\"\n",
        "    • {age_group}: {strategy}\n",
        "    \"\"\"\n",
        "\n",
        "    # Add opportunities section\n",
        "    if insights['opportunities']['underperforming_segments']:\n",
        "        report += f\"\"\"\n",
        "\n",
        "    GROWTH OPPORTUNITIES\n",
        "    ===================\n",
        "\n",
        "    UNDERPERFORMING SEGMENTS (Revenue Enhancement Opportunities):\n",
        "    \"\"\"\n",
        "\n",
        "        for segment in insights['opportunities']['underperforming_segments']:\n",
        "            potential_revenue = segment['customers'] * segment['potential_uplift']\n",
        "            report += f\"\"\"\n",
        "    • {segment['age_group']}: {segment['customers']:,} customers\n",
        "      Current RPC: ${segment['current_rpc']:.2f}\n",
        "      Potential Uplift: ${segment['potential_uplift']:.2f}/customer\n",
        "      Total Opportunity: ${potential_revenue:,.0f}\n",
        "    \"\"\"\n",
        "\n",
        "    report += f\"\"\"\n",
        "\n",
        "    END OF REPORT\n",
        "    =============\n",
        "    \"\"\"\n",
        "\n",
        "    return report\n",
        "\n",
        "# Enhanced run function with insights\n",
        "def run_complete_demographics_analysis():\n",
        "    \"\"\"Run complete demographics analysis with insights\"\"\"\n",
        "    print(\"Starting Complete Customer Demographics Analysis...\")\n",
        "\n",
        "    # Run main analysis\n",
        "    results = run_demographics_analysis()\n",
        "\n",
        "    if results:\n",
        "        print(\"\\nGenerating Advanced Insights...\")\n",
        "\n",
        "        # Generate advanced insights\n",
        "        insights = create_advanced_demographic_insights(results)\n",
        "\n",
        "        # Generate comprehensive report\n",
        "        report = generate_demographic_report(insights)\n",
        "\n",
        "        # Save insights and report\n",
        "        os.makedirs(\"demographics_analysis/reports\", exist_ok=True)\n",
        "\n",
        "        # Save report as text\n",
        "        with open(\"demographics_analysis/reports/demographic_analysis_report.txt\", \"w\", encoding='utf-8') as f:\n",
        "            f.write(report)\n",
        "\n",
        "        # Print the report\n",
        "        print(report)\n",
        "\n",
        "        print(\"\\nADDITIONAL FILES CREATED:\")\n",
        "        print(\"   demographics_analysis/reports/demographic_analysis_report.txt\")\n",
        "\n",
        "        return results, insights\n",
        "\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "# Run the complete analysis\n",
        "print(\"Executing Complete Demographics Analysis Pipeline...\")\n",
        "results, insights = run_complete_demographics_analysis()\n",
        "\n",
        "if results and insights:\n",
        "    print(\"\\nDEMOGRAPHICS ANALYSIS PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"\\nANALYSIS OUTPUTS:\")\n",
        "    print(\"   Interactive Dashboard: demographics_analysis/demographics_analysis_dashboard.html\")\n",
        "    print(\"   Raw Data Files: demographics_analysis/data/\")\n",
        "    print(\"   Insights Report: demographics_analysis/reports/\")\n",
        "    print(\"   Strategic Recommendations: Available in report\")\n",
        "else:\n",
        "    print(\"\\nDemographics Analysis Pipeline failed!\")\n",
        "    print(\"Please check your data files and try again.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K65fmwHjFDAD",
        "outputId": "419e9aca-cb3d-44f6-accb-718e23d6878f"
      },
      "id": "K65fmwHjFDAD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing Complete Demographics Analysis Pipeline...\n",
            "Starting Complete Customer Demographics Analysis...\n",
            "Starting Customer Demographics Analysis...\n",
            "Loading master data for demographics analysis...\n",
            "Master data loaded: (6416029, 43)\n",
            "Available columns: ['Invoice ID', 'Line', 'Customer ID', 'Product ID', 'Size', 'Color', 'Unit Price', 'Quantity', 'Date', 'Discount', 'Line Total', 'Store ID', 'Employee ID', 'Currency', 'Currency Symbol', 'SKU', 'Transaction Type', 'Payment Method', 'Invoice Total', 'Exchange_Rate_to_USD', 'Unit_Price_USD', 'Line_Total_USD', 'Invoice_Total_USD', 'Category', 'Sub Category', 'Description EN', 'Color_right', 'Sizes', 'Production Cost', 'Name', 'Email', 'Gender', 'Date Of Birth', 'Job Title', 'Country', 'City', 'Store Name', 'Number of Employees', 'Store ID_right', 'Name_right', 'Position', 'Latitude', 'Longitude']\n",
            "Available demographic columns: ['Date Of Birth', 'Gender']\n",
            "Preparing demographics analysis data...\n",
            "Sales data shape: (6077200, 43)\n",
            "Calculating age from Date Of Birth (current date: 2025-08-03)\n",
            "Age calculation results:\n",
            "Age range: 18 - 76\n",
            "Average age: 31.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2420069177.py:98: DeprecationWarning:\n",
            "\n",
            "`pl.count()` is deprecated. Please use `pl.len()` instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Age group distribution:\n",
            "   18-24: 2,276,879 transactions\n",
            "   25-34: 1,540,131 transactions\n",
            "   35-44: 1,316,033 transactions\n",
            "   45-54: 617,724 transactions\n",
            "   55-64: 266,589 transactions\n",
            "   65+: 59,844 transactions\n",
            "Creating age-based demographics analysis...\n",
            "Creating category preferences by demographics...\n",
            "Creating customer lifetime value by demographics...\n",
            "Creating temporal demographics patterns...\n",
            "Creating discount sensitivity by demographics...\n",
            "Demographics analysis data prepared successfully!\n",
            "Saving demographics analysis data files...\n",
            "Demographics analysis data files saved!\n",
            "Creating Demographics Analysis Dashboard...\n",
            "\n",
            "CUSTOMER DEMOGRAPHICS ANALYSIS COMPLETE!\n",
            "Total Customers Analyzed: 1,283,707\n",
            "Total Revenue Analyzed: $305,884,837\n",
            "Age Groups Analyzed: 6\n",
            "Top Revenue Age Group: 18-24 ($115,657,079)\n",
            "Highest AOV Age Group: 65+ ($52.99)\n",
            "Highest CLV Age Group: 25-34 ($7,455.27)\n",
            "\n",
            "Dashboard and Data Files Created:\n",
            "   demographics_analysis/demographics_analysis_dashboard.html\n",
            "   demographics_analysis/data/ (CSV files for detailed analysis)\n",
            "\n",
            "Generating Advanced Insights...\n",
            "Generating advanced demographic insights...\n",
            "Generating demographic insights report...\n",
            "\n",
            "    \n",
            "    Customer Demographics Analysis Report\n",
            "    ====================================\n",
            "    \n",
            "    EXECUTIVE SUMMARY\n",
            "    =================\n",
            "    \n",
            "    KEY FINDINGS:\n",
            "    • Primary Revenue Driver: 18-24\n",
            "    • Highest CLV Segment: 25-34\n",
            "    • Premium Customer Group: 65+\n",
            "    • Most Engaged Segment: 35-44\n",
            "    \n",
            "    AGE GROUP PERFORMANCE ANALYSIS\n",
            "    ==============================\n",
            "    \n",
            "    18-24:\n",
            "       • Customers: 500,150 (39.0% of total)\n",
            "       • Average Age: 20.9 years\n",
            "       • Revenue: $115,657,079\n",
            "       • Revenue per Customer: $231.24\n",
            "       • Average Order Value: $50.80\n",
            "       • Purchase Frequency: 4.6 transactions/customer\n",
            "       • Discount Sensitivity: 12.6%\n",
            "    \n",
            "    25-34:\n",
            "       • Customers: 320,725 (25.0% of total)\n",
            "       • Average Age: 29.1 years\n",
            "       • Revenue: $76,378,119\n",
            "       • Revenue per Customer: $238.14\n",
            "       • Average Order Value: $49.59\n",
            "       • Purchase Frequency: 4.8 transactions/customer\n",
            "       • Discount Sensitivity: 12.8%\n",
            "    \n",
            "    35-44:\n",
            "       • Customers: 263,950 (20.6% of total)\n",
            "       • Average Age: 39.2 years\n",
            "       • Revenue: $65,450,708\n",
            "       • Revenue per Customer: $247.97\n",
            "       • Average Order Value: $49.73\n",
            "       • Purchase Frequency: 5.0 transactions/customer\n",
            "       • Discount Sensitivity: 12.9%\n",
            "    \n",
            "    45-54:\n",
            "       • Customers: 127,129 (9.9% of total)\n",
            "       • Average Age: 48.9 years\n",
            "       • Revenue: $31,469,179\n",
            "       • Revenue per Customer: $247.54\n",
            "       • Average Order Value: $50.94\n",
            "       • Purchase Frequency: 4.9 transactions/customer\n",
            "       • Discount Sensitivity: 12.6%\n",
            "    \n",
            "    55-64:\n",
            "       • Customers: 59,020 (4.6% of total)\n",
            "       • Average Age: 58.4 years\n",
            "       • Revenue: $13,758,900\n",
            "       • Revenue per Customer: $233.12\n",
            "       • Average Order Value: $51.61\n",
            "       • Purchase Frequency: 4.5 transactions/customer\n",
            "       • Discount Sensitivity: 12.5%\n",
            "    \n",
            "    65+:\n",
            "       • Customers: 12,733 (1.0% of total)\n",
            "       • Average Age: 69.0 years\n",
            "       • Revenue: $3,170,852\n",
            "       • Revenue per Customer: $249.03\n",
            "       • Average Order Value: $52.99\n",
            "       • Purchase Frequency: 4.7 transactions/customer\n",
            "       • Discount Sensitivity: 12.5%\n",
            "    \n",
            "    \n",
            "    CATEGORY PREFERENCES BY AGE GROUP\n",
            "    =================================\n",
            "    \n",
            "    18-24:\n",
            "       • Category Diversity: 3 categories explored\n",
            "       • Total Category Revenue: $115,657,079\n",
            "       • Top 3 Categories:\n",
            "         1. Feminine: $54,740,852\n",
            "         2. Masculine: $50,647,822\n",
            "         3. Children: $10,268,405\n",
            "    25-34:\n",
            "       • Category Diversity: 3 categories explored\n",
            "       • Total Category Revenue: $76,378,119\n",
            "       • Top 3 Categories:\n",
            "         1. Feminine: $38,275,040\n",
            "         2. Masculine: $31,321,874\n",
            "         3. Children: $6,781,206\n",
            "    35-44:\n",
            "       • Category Diversity: 3 categories explored\n",
            "       • Total Category Revenue: $65,450,708\n",
            "       • Top 3 Categories:\n",
            "         1. Feminine: $33,847,363\n",
            "         2. Masculine: $25,774,903\n",
            "         3. Children: $5,828,443\n",
            "    45-54:\n",
            "       • Category Diversity: 3 categories explored\n",
            "       • Total Category Revenue: $31,469,179\n",
            "       • Top 3 Categories:\n",
            "         1. Feminine: $16,437,848\n",
            "         2. Masculine: $12,223,046\n",
            "         3. Children: $2,808,285\n",
            "    55-64:\n",
            "       • Category Diversity: 3 categories explored\n",
            "       • Total Category Revenue: $13,758,900\n",
            "       • Top 3 Categories:\n",
            "         1. Feminine: $7,040,925\n",
            "         2. Masculine: $5,497,925\n",
            "         3. Children: $1,220,050\n",
            "    65+:\n",
            "       • Category Diversity: 3 categories explored\n",
            "       • Total Category Revenue: $3,170,852\n",
            "       • Top 3 Categories:\n",
            "         1. Feminine: $1,532,365\n",
            "         2. Masculine: $1,354,369\n",
            "         3. Children: $284,118\n",
            "    \n",
            "    STRATEGIC RECOMMENDATIONS\n",
            "    =========================\n",
            "    \n",
            "    • 18-24: Primary revenue driver - focus on retention and expansion\n",
            "    \n",
            "    • 25-34: Highest CLV - invest in premium experiences and loyalty programs\n",
            "    \n",
            "    • 65+: Premium segment - target with high-value products\n",
            "    \n",
            "    • 35-44: Most engaged - leverage for referrals and advocacy\n",
            "    \n",
            "    \n",
            "    END OF REPORT\n",
            "    =============\n",
            "    \n",
            "\n",
            "ADDITIONAL FILES CREATED:\n",
            "   demographics_analysis/reports/demographic_analysis_report.txt\n",
            "\n",
            "DEMOGRAPHICS ANALYSIS PIPELINE COMPLETED SUCCESSFULLY!\n",
            "\n",
            "ANALYSIS OUTPUTS:\n",
            "   Interactive Dashboard: demographics_analysis/demographics_analysis_dashboard.html\n",
            "   Raw Data Files: demographics_analysis/data/\n",
            "   Insights Report: demographics_analysis/reports/\n",
            "   Strategic Recommendations: Available in report\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QxKZNk91FDEc"
      },
      "id": "QxKZNk91FDEc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6B9ygRD3HRgz"
      },
      "id": "6B9ygRD3HRgz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zgFmYk-fHRkU"
      },
      "id": "zgFmYk-fHRkU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customer Segmentation (RFM, CLV, Cohort)"
      ],
      "metadata": {
        "id": "HXKjxww0HSsJ"
      },
      "id": "HXKjxww0HSsJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e838ba1c",
      "metadata": {
        "id": "e838ba1c"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.offline as pyo\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "from operator import attrgetter\n",
        "\n",
        "def load_master_data_for_customer_segmentation():\n",
        "    \"\"\"Load master data for customer segmentation analysis\"\"\"\n",
        "    print(\" Loading master data for customer segmentation analysis...\")\n",
        "\n",
        "    try:\n",
        "        if os.path.exists(\"data/master_transactions_with_coords.parquet\"):\n",
        "            master_data = pl.read_parquet(\"data/master_transactions_with_coords.parquet\")\n",
        "        else:\n",
        "            master_data = pl.read_parquet(\"data/master_transactions.parquet\")\n",
        "\n",
        "        print(f\" Master data loaded: {master_data.shape}\")\n",
        "        return master_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error loading master data: {e}\")\n",
        "        return None\n",
        "\n",
        "def calculate_advanced_rfm_analysis(master_data):\n",
        "    \"\"\"Calculate advanced RFM analysis with additional customer metrics\"\"\"\n",
        "    print(\"Calculating Advanced RFM Analysis...\")\n",
        "\n",
        "    # Filter for sales only\n",
        "    sales_data = master_data.filter(pl.col(\"Transaction Type\") == \"Sale\")\n",
        "\n",
        "    # Get the latest date in the dataset as reference point\n",
        "    latest_date = sales_data.select(pl.col(\"Date\").max()).item()\n",
        "    print(f\"Analysis reference date: {latest_date}\")\n",
        "\n",
        "    # Calculate comprehensive RFM metrics\n",
        "    # Calculate comprehensive RFM metrics\n",
        "    rfm_data = sales_data.group_by(\"Customer ID\").agg([\n",
        "        # Core RFM metrics\n",
        "        (pl.lit(latest_date) - pl.col(\"Date\").max()).dt.total_days().alias(\"Recency_Days\"),\n",
        "        pl.col(\"Invoice ID\").n_unique().alias(\"Frequency\"),\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Monetary_Value\"),\n",
        "\n",
        "        # Additional customer behavior metrics\n",
        "        pl.col(\"Date\").min().alias(\"First_Purchase_Date\"),\n",
        "        pl.col(\"Date\").max().alias(\"Last_Purchase_Date\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Total_Items_Purchased\"),\n",
        "        pl.col(\"Product ID\").n_unique().alias(\"Unique_Products_Purchased\"),\n",
        "        pl.col(\"Category\").n_unique().alias(\"Unique_Categories_Purchased\"),\n",
        "        pl.col(\"Store ID\").n_unique().alias(\"Stores_Visited\"),\n",
        "        pl.col(\"Line_Total_USD\").mean().alias(\"Average_Order_Value\"),\n",
        "        (pl.col(\"Line_Total_USD\").sum() / pl.col(\"Quantity\").sum()).alias(\"Average_Unit_Price\"),\n",
        "\n",
        "        # Seasonal behavior (simplified)\n",
        "        pl.col(\"Date\").dt.quarter().first().alias(\"First_Quarter\"),\n",
        "        pl.col(\"Date\").dt.weekday().first().alias(\"First_Weekday\"),\n",
        "        pl.col(\"Date\").dt.month().first().alias(\"First_Month\"),\n",
        "\n",
        "        # Product preferences\n",
        "        pl.col(\"Category\").first().alias(\"First_Category\"),  # Changed from mode() to first()\n",
        "        pl.col(\"Sub Category\").first().alias(\"First_Sub_Category\")  # Changed from mode() to first()\n",
        "    ])\n",
        "\n",
        "\n",
        "    # Convert to pandas for advanced calculations\n",
        "    rfm_pandas = rfm_data.to_pandas()\n",
        "\n",
        "    # Calculate customer lifetime and additional metrics\n",
        "    rfm_pandas['Customer_Lifetime_Days'] = (rfm_pandas['Last_Purchase_Date'] - rfm_pandas['First_Purchase_Date']).dt.days\n",
        "    rfm_pandas['Customer_Lifetime_Days'] = rfm_pandas['Customer_Lifetime_Days'].fillna(1).replace(0, 1)\n",
        "\n",
        "    # Advanced behavioral metrics\n",
        "    rfm_pandas['Monthly_Purchase_Frequency'] = (rfm_pandas['Frequency'] / rfm_pandas['Customer_Lifetime_Days'] * 30).fillna(1.0)\n",
        "    rfm_pandas['Items_Per_Transaction'] = rfm_pandas['Total_Items_Purchased'] / rfm_pandas['Frequency']\n",
        "    rfm_pandas['Categories_Per_Transaction'] = rfm_pandas['Unique_Categories_Purchased'] / rfm_pandas['Frequency']\n",
        "    rfm_pandas['Store_Loyalty_Score'] = 1 / rfm_pandas['Stores_Visited']  # Higher = more loyal to specific stores\n",
        "    rfm_pandas['Product_Diversity_Score'] = rfm_pandas['Unique_Products_Purchased'] / rfm_pandas['Frequency']\n",
        "\n",
        "    # Calculate percentile-based RFM scores (1-5 scale)\n",
        "    rfm_pandas['R_Score'] = pd.qcut(rfm_pandas['Recency_Days'], 5, labels=[5,4,3,2,1], duplicates='drop')  # Lower recency = higher score\n",
        "    rfm_pandas['F_Score'] = pd.qcut(rfm_pandas['Frequency'].rank(method='first'), 5, labels=[1,2,3,4,5], duplicates='drop')\n",
        "    rfm_pandas['M_Score'] = pd.qcut(rfm_pandas['Monetary_Value'], 5, labels=[1,2,3,4,5], duplicates='drop')\n",
        "\n",
        "    # Handle any NaN values in scores\n",
        "    rfm_pandas['R_Score'] = rfm_pandas['R_Score'].fillna(3)\n",
        "    rfm_pandas['F_Score'] = rfm_pandas['F_Score'].fillna(3)\n",
        "    rfm_pandas['M_Score'] = rfm_pandas['M_Score'].fillna(3)\n",
        "\n",
        "    # Create RFM segment string\n",
        "    rfm_pandas['RFM_Score'] = rfm_pandas['R_Score'].astype(str) + rfm_pandas['F_Score'].astype(str) + rfm_pandas['M_Score'].astype(str)\n",
        "\n",
        "    print(f\" Advanced RFM analysis completed for {len(rfm_pandas):,} customers\")\n",
        "\n",
        "    return rfm_pandas\n",
        "\n",
        "def create_customer_segments(rfm_data):\n",
        "    \"\"\"Create detailed customer segments based on RFM and behavioral data - OPTIMIZED\"\"\"\n",
        "    print(\" Creating detailed customer segments...\")\n",
        "\n",
        "    # Convert R_Score, F_Score, M_Score to integers for faster operations\n",
        "    rfm_data['R_Score'] = rfm_data['R_Score'].astype(int)\n",
        "    rfm_data['F_Score'] = rfm_data['F_Score'].astype(int)\n",
        "    rfm_data['M_Score'] = rfm_data['M_Score'].astype(int)\n",
        "\n",
        "    # Vectorized RFM segmentation using numpy conditions\n",
        "    conditions = [\n",
        "        # Champions: High value, frequent, recent\n",
        "        (rfm_data['R_Score'] >= 4) & (rfm_data['F_Score'] >= 4) & (rfm_data['M_Score'] >= 4),\n",
        "\n",
        "        # Loyal Customers: High frequency, good monetary\n",
        "        (rfm_data['F_Score'] >= 4) & (rfm_data['M_Score'] >= 3),\n",
        "\n",
        "        # Potential Loyalists: Recent customers with good frequency\n",
        "        (rfm_data['R_Score'] >= 3) & (rfm_data['F_Score'] >= 2) & (rfm_data['F_Score'] <= 3),\n",
        "\n",
        "        # New Customers: Recent but low frequency\n",
        "        (rfm_data['R_Score'] >= 4) & (rfm_data['F_Score'] <= 2),\n",
        "\n",
        "        # Promising: Recent customers with potential\n",
        "        (rfm_data['R_Score'] >= 3) & (rfm_data['F_Score'] <= 2) & (rfm_data['M_Score'] >= 2),\n",
        "\n",
        "        # Need Attention: Above average recency, frequency & monetary\n",
        "        (rfm_data['R_Score'] >= 2) & (rfm_data['F_Score'] >= 2) & (rfm_data['M_Score'] >= 2),\n",
        "\n",
        "        # About to Sleep: Below average recency but good frequency\n",
        "        (rfm_data['R_Score'] <= 2) & (rfm_data['F_Score'] >= 3),\n",
        "\n",
        "        # At Risk: Good customers who haven't purchased recently\n",
        "        (rfm_data['R_Score'] <= 2) & (rfm_data['F_Score'] >= 2) & (rfm_data['M_Score'] >= 3),\n",
        "\n",
        "        # Cannot Lose Them: High value but low recency and frequency\n",
        "        (rfm_data['F_Score'] >= 4) & (rfm_data['M_Score'] >= 4),\n",
        "\n",
        "        # Hibernating: Low recency, frequency & monetary\n",
        "        (rfm_data['R_Score'] <= 2) & (rfm_data['F_Score'] <= 2) & (rfm_data['M_Score'] <= 2)\n",
        "    ]\n",
        "\n",
        "    choices = [\n",
        "        \"Champions\", \"Loyal Customers\", \"Potential Loyalists\", \"New Customers\",\n",
        "        \"Promising\", \"Need Attention\", \"About to Sleep\", \"At Risk\",\n",
        "        \"Cannot Lose Them\", \"Hibernating\"\n",
        "    ]\n",
        "\n",
        "\n",
        "    rfm_data['Customer_Segment'] = np.select(conditions, choices, default=\"Lost\")\n",
        "\n",
        "    # Vectorized behavioral segmentation\n",
        "    aov_threshold = rfm_data['Average_Order_Value'].quantile(0.75)\n",
        "    freq_threshold = rfm_data['Monthly_Purchase_Frequency'].quantile(0.75)\n",
        "    cat_threshold = rfm_data['Unique_Categories_Purchased'].quantile(0.75)\n",
        "    loyalty_threshold = rfm_data['Store_Loyalty_Score'].quantile(0.75)\n",
        "\n",
        "    behavioral_conditions = [\n",
        "        (rfm_data['Average_Order_Value'] >= aov_threshold) & (rfm_data['Monthly_Purchase_Frequency'] >= freq_threshold),\n",
        "        (rfm_data['Average_Order_Value'] >= aov_threshold) & (rfm_data['Monthly_Purchase_Frequency'] < freq_threshold),\n",
        "        rfm_data['Monthly_Purchase_Frequency'] >= freq_threshold,\n",
        "        rfm_data['Unique_Categories_Purchased'] >= cat_threshold,\n",
        "        rfm_data['Store_Loyalty_Score'] >= loyalty_threshold\n",
        "    ]\n",
        "\n",
        "    behavioral_choices = [\n",
        "        \"High Value Frequent\", \"High Value Occasional\", \"Frequent Shoppers\",\n",
        "        \"Category Explorers\", \"Store Loyalists\"\n",
        "    ]\n",
        "\n",
        "    rfm_data['Behavioral_Segment'] = np.select(behavioral_conditions, behavioral_choices, default=\"Regular Customers\")\n",
        "\n",
        "    # Vectorized value segmentation\n",
        "    high_value = rfm_data['Monetary_Value'].quantile(0.8)\n",
        "    medium_value = rfm_data['Monetary_Value'].quantile(0.5)\n",
        "    high_freq = rfm_data['Frequency'].quantile(0.7)\n",
        "\n",
        "    value_conditions = [\n",
        "        (rfm_data['Monetary_Value'] >= high_value) & (rfm_data['Frequency'] >= high_freq),\n",
        "        (rfm_data['Monetary_Value'] >= high_value) & (rfm_data['Frequency'] < high_freq),\n",
        "        (rfm_data['Monetary_Value'] >= medium_value) & (rfm_data['Frequency'] >= high_freq),\n",
        "        (rfm_data['Monetary_Value'] >= medium_value) & (rfm_data['Frequency'] < high_freq),\n",
        "        (rfm_data['Monetary_Value'] < medium_value) & (rfm_data['Frequency'] >= high_freq)\n",
        "    ]\n",
        "\n",
        "    value_choices = [\n",
        "        \"VIP Customers\", \"Big Spenders\", \"Loyal Regulars\",\n",
        "        \"Medium Value\", \"Frequent Low Spenders\"\n",
        "    ]\n",
        "\n",
        "    rfm_data['Value_Segment'] = np.select(value_conditions, value_choices, default=\"Low Value\")\n",
        "\n",
        "    print(f\" Customer segmentation completed\")\n",
        "    print(f\" RFM Segments: {rfm_data['Customer_Segment'].nunique()}\")\n",
        "    print(f\" Behavioral Segments: {rfm_data['Behavioral_Segment'].nunique()}\")\n",
        "    print(f\" Value Segments: {rfm_data['Value_Segment'].nunique()}\")\n",
        "\n",
        "    return rfm_data\n",
        "\n",
        "\n",
        "def calculate_customer_lifetime_value(rfm_data):\n",
        "    \"\"\"Calculate Customer Lifetime Value (CLV) using computational methods\"\"\"\n",
        "    print(\" Calculating Customer Lifetime Value...\")\n",
        "\n",
        "    # Historical CLV (what customer has already spent)\n",
        "    rfm_data['Historical_CLV'] = rfm_data['Monetary_Value']\n",
        "\n",
        "    # Predicted CLV based on purchase patterns\n",
        "    # Method 1: Simple frequency-based prediction\n",
        "    rfm_data['Avg_Days_Between_Purchases'] = rfm_data['Customer_Lifetime_Days'] / (rfm_data['Frequency'] - 1)\n",
        "    rfm_data['Avg_Days_Between_Purchases'] = rfm_data['Avg_Days_Between_Purchases'].fillna(rfm_data['Customer_Lifetime_Days'])\n",
        "\n",
        "    # Estimate future purchases in next 12 months\n",
        "    rfm_data['Estimated_Future_Purchases'] = np.where(\n",
        "        rfm_data['Avg_Days_Between_Purchases'] > 0,\n",
        "        365 / rfm_data['Avg_Days_Between_Purchases'],\n",
        "        rfm_data['Monthly_Purchase_Frequency'] * 12\n",
        "    )\n",
        "\n",
        "    # Predicted CLV (conservative estimate)\n",
        "    rfm_data['Predicted_CLV'] = rfm_data['Average_Order_Value'] * rfm_data['Estimated_Future_Purchases']\n",
        "\n",
        "    # Total CLV (Historical + Predicted)\n",
        "    rfm_data['Total_CLV'] = rfm_data['Historical_CLV'] + rfm_data['Predicted_CLV']\n",
        "\n",
        "    # CLV segments based on total CLV\n",
        "    rfm_data['CLV_Segment'] = pd.qcut(\n",
        "        rfm_data['Total_CLV'],\n",
        "        q=5,\n",
        "        labels=['Very Low CLV', 'Low CLV', 'Medium CLV', 'High CLV', 'Very High CLV'],\n",
        "        duplicates='drop'\n",
        "    )\n",
        "\n",
        "    # Customer risk assessment based on recency and frequency trends\n",
        "    def risk_assessment(row):\n",
        "        recency = row['Recency_Days']\n",
        "        avg_gap = row['Avg_Days_Between_Purchases']\n",
        "\n",
        "        if recency > avg_gap * 2:\n",
        "            return \"High Risk\"\n",
        "        elif recency > avg_gap * 1.5:\n",
        "            return \"Medium Risk\"\n",
        "        else:\n",
        "            return \"Low Risk\"\n",
        "\n",
        "    rfm_data['Churn_Risk'] = rfm_data.apply(risk_assessment, axis=1)\n",
        "\n",
        "    print(\" CLV calculation completed\")\n",
        "\n",
        "    return rfm_data\n",
        "\n",
        "def create_cohort_analysis(master_data):\n",
        "    \"\"\"Create customer cohort analysis\"\"\"\n",
        "    print(\" Creating cohort analysis...\")\n",
        "\n",
        "    # Filter for sales only\n",
        "    sales_data = master_data.filter(pl.col(\"Transaction Type\") == \"Sale\")\n",
        "\n",
        "    # Convert to pandas for cohort analysis\n",
        "    df = sales_data.select([\n",
        "        \"Customer ID\", \"Date\", \"Line_Total_USD\"\n",
        "    ]).to_pandas()\n",
        "\n",
        "    # Get customer's first purchase date\n",
        "    df['Order_Period'] = df['Date'].dt.to_period('M')\n",
        "    df['Cohort_Group'] = df.groupby('Customer ID')['Date'].transform('min').dt.to_period('M')\n",
        "\n",
        "    # Calculate period number\n",
        "    df['Period_Number'] = (df['Order_Period'] - df['Cohort_Group']).apply(attrgetter('n'))\n",
        "\n",
        "    # Create cohort table for customer retention\n",
        "    cohort_data = df.groupby(['Cohort_Group', 'Period_Number'])['Customer ID'].nunique().reset_index()\n",
        "    cohort_counts = cohort_data.pivot(index='Cohort_Group', columns='Period_Number', values='Customer ID')\n",
        "\n",
        "    # Calculate cohort sizes (first month customers)\n",
        "    cohort_sizes = df.groupby('Cohort_Group')['Customer ID'].nunique()\n",
        "    cohort_table = cohort_counts.divide(cohort_sizes, axis=0)\n",
        "\n",
        "    # Revenue cohort analysis\n",
        "    revenue_cohort_data = df.groupby(['Cohort_Group', 'Period_Number'])['Line_Total_USD'].sum().reset_index()\n",
        "    revenue_cohort_table = revenue_cohort_data.pivot(index='Cohort_Group', columns='Period_Number', values='Line_Total_USD')\n",
        "\n",
        "    print(\" Cohort analysis completed\")\n",
        "\n",
        "    return cohort_table, cohort_counts, revenue_cohort_table\n",
        "\n",
        "def analyze_customer_journey(master_data):\n",
        "    \"\"\"Analyze customer journey and purchase patterns\"\"\"\n",
        "    print(\" Analyzing customer journey patterns...\")\n",
        "\n",
        "    # Filter for sales only\n",
        "    sales_data = master_data.filter(pl.col(\"Transaction Type\") == \"Sale\")\n",
        "\n",
        "    # Customer journey metrics\n",
        "    journey_analysis = sales_data.group_by(\"Customer ID\").agg([\n",
        "        pl.col(\"Date\").min().alias(\"First_Purchase\"),\n",
        "        pl.col(\"Date\").max().alias(\"Last_Purchase\"),\n",
        "        pl.col(\"Invoice ID\").n_unique().alias(\"Total_Transactions\"),\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Total_Spent\"),\n",
        "        pl.col(\"Store ID\").n_unique().alias(\"Stores_Visited\"),\n",
        "        pl.col(\"Category\").n_unique().alias(\"Categories_Explored\"),\n",
        "\n",
        "        # Purchase evolution\n",
        "        pl.col(\"Line_Total_USD\").first().alias(\"First_Purchase_Amount\"),\n",
        "        pl.col(\"Line_Total_USD\").last().alias(\"Last_Purchase_Amount\"),\n",
        "        pl.col(\"Line_Total_USD\").mean().alias(\"Average_Purchase_Amount\"),\n",
        "        pl.col(\"Line_Total_USD\").max().alias(\"Highest_Purchase_Amount\")\n",
        "    ]).to_pandas()\n",
        "\n",
        "    # Calculate journey metrics\n",
        "    journey_analysis['Customer_Lifespan_Days'] = (journey_analysis['Last_Purchase'] - journey_analysis['First_Purchase']).dt.days\n",
        "    journey_analysis['Purchase_Growth'] = ((journey_analysis['Last_Purchase_Amount'] - journey_analysis['First_Purchase_Amount']) /\n",
        "                                         journey_analysis['First_Purchase_Amount'] * 100).fillna(0)\n",
        "\n",
        "    # Journey stages\n",
        "    def journey_stage(row):\n",
        "        transactions = row['Total_Transactions']\n",
        "        lifespan = row['Customer_Lifespan_Days']\n",
        "\n",
        "        if transactions == 1:\n",
        "            return \"One-Time Buyer\"\n",
        "        elif transactions <= 3 and lifespan <= 90:\n",
        "            return \"Early Stage\"\n",
        "        elif transactions <= 5 and lifespan <= 180:\n",
        "            return \"Developing\"\n",
        "        elif transactions <= 10:\n",
        "            return \"Established\"\n",
        "        else:\n",
        "            return \"Mature\"\n",
        "\n",
        "    journey_analysis['Journey_Stage'] = journey_analysis.apply(journey_stage, axis=1)\n",
        "\n",
        "    print(\" Customer journey analysis completed\")\n",
        "\n",
        "    return journey_analysis\n",
        "\n",
        "def create_customer_segmentation_dashboard(rfm_data, cohort_table, revenue_cohort_table, journey_analysis):\n",
        "    \"\"\"Create comprehensive customer segmentation dashboard\"\"\"\n",
        "    print(\" Creating Customer Segmentation Dashboard...\")\n",
        "\n",
        "    # Create HTML structure\n",
        "    html_content = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Advanced Customer Segmentation Analysis Dashboard</title>\n",
        "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
        "        <style>\n",
        "            body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }\n",
        "            .chart-container { background-color: white; margin: 20px 0; padding: 20px; border-radius: 10px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }\n",
        "            .chart-title { font-size: 24px; font-weight: bold; text-align: center; margin-bottom: 20px; color: #333; }\n",
        "            .dashboard-title { font-size: 36px; font-weight: bold; text-align: center; margin-bottom: 30px; color: #2c3e50; }\n",
        "            .insights-box { background-color: #e8f4f8; padding: 15px; margin: 10px 0; border-radius: 8px; font-size: 14px; }\n",
        "            .metric-highlight { background-color: #fff3cd; padding: 10px; margin: 5px 0; border-radius: 5px; font-weight: bold; }\n",
        "            .segment-summary { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 20px 0; }\n",
        "            .segment-card { background-color: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #007bff; }\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <div class=\"dashboard-title\"> Advanced Customer Segmentation Analysis</div>\n",
        "\n",
        "        <div class=\"insights-box\">\n",
        "            <strong> Customer Segmentation Insights:</strong><br>\n",
        "            • <strong>RFM Analysis</strong>: Segments customers based on Recency, Frequency, and Monetary value<br>\n",
        "            • <strong>Behavioral Segmentation</strong>: Groups customers by shopping patterns and preferences<br>\n",
        "            • <strong>Customer Lifetime Value</strong>: Predicts future value and identifies high-value customers<br>\n",
        "            • <strong>Cohort Analysis</strong>: Tracks customer retention and revenue patterns over time<br>\n",
        "            • <strong>Journey Analysis</strong>: Maps customer evolution and purchase progression\n",
        "        </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. RFM Analysis 3D Scatter Plot\n",
        "    fig1 = px.scatter_3d(\n",
        "        rfm_data,\n",
        "        x='Recency_Days',\n",
        "        y='Frequency',\n",
        "        z='Monetary_Value',\n",
        "        color='Customer_Segment',\n",
        "        size='Total_CLV',\n",
        "        hover_name='Customer ID',\n",
        "        hover_data={\n",
        "            'Average_Order_Value': ':.2f',\n",
        "            'Total_Items_Purchased': ':,',\n",
        "            'Stores_Visited': True,\n",
        "            'CLV_Segment': True\n",
        "        },\n",
        "        title=\" 3D RFM Analysis: Customer Segmentation\",\n",
        "        labels={\n",
        "            'Recency_Days': 'Recency (Days)',\n",
        "            'Frequency': 'Purchase Frequency',\n",
        "            'Monetary_Value': 'Total Spent (USD)'\n",
        "        },\n",
        "        height=700\n",
        "    )\n",
        "    fig1.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # 2. Customer Segment Distribution\n",
        "    segment_counts = rfm_data['Customer_Segment'].value_counts()\n",
        "    fig2 = px.pie(\n",
        "        values=segment_counts.values,\n",
        "        names=segment_counts.index,\n",
        "        title=\" Customer Segment Distribution\",\n",
        "        height=600\n",
        "    )\n",
        "    fig2.update_traces(textposition='inside', textinfo='percent+label')\n",
        "    fig2.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # 3. CLV Analysis by Segment\n",
        "    clv_by_segment = rfm_data.groupby('Customer_Segment').agg({\n",
        "        'Total_CLV': ['mean', 'sum', 'count'],\n",
        "        'Historical_CLV': 'mean',\n",
        "        'Predicted_CLV': 'mean'\n",
        "    }).round(2)\n",
        "\n",
        "    fig3 = px.bar(\n",
        "        x=clv_by_segment.index,\n",
        "        y=clv_by_segment[('Total_CLV', 'mean')],\n",
        "        title=\" Average Customer Lifetime Value by Segment\",\n",
        "        labels={'x': 'Customer Segment', 'y': 'Average CLV (USD)'},\n",
        "        height=600\n",
        "    )\n",
        "    fig3.update_layout(template=\"plotly_white\", xaxis_tickangle=45)\n",
        "\n",
        "    # 4. Behavioral Segmentation\n",
        "    behavioral_counts = rfm_data['Behavioral_Segment'].value_counts()\n",
        "    fig4 = px.bar(\n",
        "        x=behavioral_counts.index,\n",
        "        y=behavioral_counts.values,\n",
        "        title=\" Behavioral Segmentation Distribution\",\n",
        "        labels={'x': 'Behavioral Segment', 'y': 'Number of Customers'},\n",
        "        height=600\n",
        "    )\n",
        "    fig4.update_layout(template=\"plotly_white\", xaxis_tickangle=45)\n",
        "\n",
        "    # 5. Customer Journey Stage Analysis\n",
        "    journey_counts = journey_analysis['Journey_Stage'].value_counts()\n",
        "    fig5 = px.funnel(\n",
        "        x=journey_counts.values,\n",
        "        y=journey_counts.index,\n",
        "        title=\" Customer Journey Stage Distribution\",\n",
        "        height=600\n",
        "    )\n",
        "    fig5.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # 6. Cohort Retention Heatmap\n",
        "    fig6 = px.imshow(\n",
        "        cohort_table.iloc[:12, :12].values,  # Show first 12 months of first 12 cohorts\n",
        "        labels=dict(x=\"Period\", y=\"Cohort Month\", color=\"Retention Rate\"),\n",
        "        x=[f\"Month {i}\" for i in range(12)],\n",
        "        y=[str(cohort)[:7] for cohort in cohort_table.index[:12]],\n",
        "        title=\" Customer Retention Cohort Analysis\",\n",
        "        color_continuous_scale=\"RdYlGn\",\n",
        "        height=600\n",
        "    )\n",
        "    fig6.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # 7. Revenue Cohort Analysis\n",
        "    fig7 = px.imshow(\n",
        "        revenue_cohort_table.iloc[:12, :12].fillna(0).values,\n",
        "        labels=dict(x=\"Period\", y=\"Cohort Month\", color=\"Revenue (USD)\"),\n",
        "        x=[f\"Month {i}\" for i in range(12)],\n",
        "        y=[str(cohort)[:7] for cohort in revenue_cohort_table.index[:12]],\n",
        "        title=\" Revenue Cohort Analysis\",\n",
        "        color_continuous_scale=\"Blues\",\n",
        "        height=600\n",
        "    )\n",
        "    fig7.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # 8. Customer Value vs Risk Matrix\n",
        "    fig8 = px.scatter(\n",
        "        rfm_data,\n",
        "        x='Total_CLV',\n",
        "        y='Recency_Days',\n",
        "        color='Churn_Risk',\n",
        "        size='Frequency',\n",
        "        hover_name='Customer ID',\n",
        "        hover_data={\n",
        "            'Customer_Segment': True,\n",
        "            'Average_Order_Value': ':.2f',\n",
        "            'Monthly_Purchase_Frequency': ':.2f'\n",
        "        },\n",
        "        title=\" Customer Value vs Churn Risk Matrix\",\n",
        "        labels={\n",
        "            'Total_CLV': 'Customer Lifetime Value (USD)',\n",
        "            'Recency_Days': 'Days Since Last Purchase'\n",
        "        },\n",
        "        height=700\n",
        "    )\n",
        "    fig8.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # 9. Purchase Behavior Analysis\n",
        "    fig9 = px.scatter(\n",
        "        rfm_data,\n",
        "        x='Average_Order_Value',\n",
        "        y='Monthly_Purchase_Frequency',\n",
        "        color='Value_Segment',\n",
        "        size='Total_Items_Purchased',\n",
        "        hover_name='Customer ID',\n",
        "        title=\" Purchase Behavior Analysis\",\n",
        "        labels={\n",
        "            'Average_Order_Value': 'Average Order Value (USD)',\n",
        "            'Monthly_Purchase_Frequency': 'Monthly Purchase Frequency'\n",
        "        },\n",
        "        height=600\n",
        "    )\n",
        "    fig9.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # 10. Customer Loyalty Analysis\n",
        "    fig10 = px.scatter(\n",
        "        rfm_data,\n",
        "        x='Store_Loyalty_Score',\n",
        "        y='Product_Diversity_Score',\n",
        "        color='Behavioral_Segment',\n",
        "        size='Monetary_Value',\n",
        "        hover_name='Customer ID',\n",
        "        title=\" Customer Loyalty vs Product Diversity\",\n",
        "        labels={\n",
        "            'Store_Loyalty_Score': 'Store Loyalty Score',\n",
        "            'Product_Diversity_Score': 'Product Diversity Score'\n",
        "        },\n",
        "        height=600\n",
        "    )\n",
        "    fig10.update_layout(template=\"plotly_white\")\n",
        "\n",
        "    # Convert all figures to HTML\n",
        "    figures = [fig1, fig2, fig3, fig4, fig5, fig6, fig7, fig8, fig9, fig10]\n",
        "    titles = [\n",
        "        \"3D RFM Analysis: Customer Segmentation\",\n",
        "        \"Customer Segment Distribution\",\n",
        "        \"Average Customer Lifetime Value by Segment\",\n",
        "        \"Behavioral Segmentation Distribution\",\n",
        "        \"Customer Journey Stage Distribution\",\n",
        "        \"Customer Retention Cohort Analysis\",\n",
        "        \"Revenue Cohort Analysis\",\n",
        "        \"Customer Value vs Churn Risk Matrix\",\n",
        "        \"Purchase Behavior Analysis\",\n",
        "        \"Customer Loyalty vs Product Diversity\"\n",
        "    ]\n",
        "\n",
        "    for i, (fig, title) in enumerate(zip(figures, titles)):\n",
        "        html_content += f\"\"\"\n",
        "        <div class=\"chart-container\">\n",
        "            <div class=\"chart-title\">{title}</div>\n",
        "            <div id=\"chart{i+1}\"></div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "    # Add segment summary cards\n",
        "    segment_summary = rfm_data.groupby('Customer_Segment').agg({\n",
        "        'Customer ID': 'count',\n",
        "        'Total_CLV': 'mean',\n",
        "        'Average_Order_Value': 'mean',\n",
        "        'Monthly_Purchase_Frequency': 'mean',\n",
        "        'Recency_Days': 'mean'\n",
        "    }).round(2)\n",
        "\n",
        "    html_content += \"\"\"\n",
        "        <div class=\"chart-container\">\n",
        "            <div class=\"chart-title\"> Customer Segment Summary</div>\n",
        "            <div class=\"segment-summary\">\n",
        "    \"\"\"\n",
        "\n",
        "    for segment in segment_summary.index:\n",
        "        data = segment_summary.loc[segment]\n",
        "        html_content += f\"\"\"\n",
        "            <div class=\"segment-card\">\n",
        "                <h3>{segment}</h3>\n",
        "                <p><strong>Customers:</strong> {data['Customer ID']:,}</p>\n",
        "                <p><strong>Avg CLV:</strong> ${data['Total_CLV']:,.2f}</p>\n",
        "                <p><strong>Avg Order Value:</strong> ${data['Average_Order_Value']:,.2f}</p>\n",
        "                <p><strong>Monthly Frequency:</strong> {data['Monthly_Purchase_Frequency']:.2f}</p>\n",
        "                <p><strong>Avg Recency:</strong> {data['Recency_Days']:.0f} days</p>\n",
        "            </div>\n",
        "        \"\"\"\n",
        "\n",
        "    html_content += \"\"\"\n",
        "            </div>\n",
        "        </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # Add key insights\n",
        "    total_customers = len(rfm_data)\n",
        "    total_clv = rfm_data['Total_CLV'].sum()\n",
        "    avg_clv = rfm_data['Total_CLV'].mean()\n",
        "    champions = len(rfm_data[rfm_data['Customer_Segment'] == 'Champions'])\n",
        "    at_risk = len(rfm_data[rfm_data['Customer_Segment'] == 'At Risk'])\n",
        "\n",
        "    html_content += f\"\"\"\n",
        "        <div class=\"insights-box\">\n",
        "            <strong> Key Customer Insights:</strong><br>\n",
        "            <div class=\"metric-highlight\"> Total Customers Analyzed: {total_customers:,}</div>\n",
        "            <div class=\"metric-highlight\"> Total Customer Lifetime Value: ${total_clv:,.0f}</div>\n",
        "            <div class=\"metric-highlight\"> Average CLV per Customer: ${avg_clv:,.2f}</div>\n",
        "            <div class=\"metric-highlight\"> Champion Customers: {champions:,} ({champions/total_customers*100:.1f}%)</div>\n",
        "            <div class=\"metric-highlight\"> At-Risk Customers: {at_risk:,} ({at_risk/total_customers*100:.1f}%)</div>\n",
        "        </div>\n",
        "    \"\"\"\n",
        "\n",
        "    html_content += \"\"\"\n",
        "    <script>\n",
        "    \"\"\"\n",
        "\n",
        "    # Add JavaScript for each plot\n",
        "    for i, fig in enumerate(figures):\n",
        "        plot_json = fig.to_json()\n",
        "        html_content += f\"\"\"\n",
        "        var plotData{i+1} = {plot_json};\n",
        "        Plotly.newPlot('chart{i+1}',\n",
        "        plotData{i+1}.data, plotData{i+1}.layout, {{responsive: true}});\n",
        "        \"\"\"\n",
        "\n",
        "    html_content += \"\"\"\n",
        "    </script>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    return html_content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dec86085",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dec86085",
        "outputId": "27a8bf6a-9445-44ea-90b0-4ee1a3405f04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Initializing Advanced Customer Segmentation Analysis...\n",
            " Starting Advanced Customer Segmentation Analysis...\n",
            " Loading master data for customer segmentation analysis...\n",
            " Master data loaded: (6416029, 41)\n",
            "\n",
            " Step 1: Calculating RFM Analysis...\n",
            "Calculating Advanced RFM Analysis...\n",
            "Analysis reference date: 2025-03-18\n",
            " Advanced RFM analysis completed for 1,283,707 customers\n",
            "\n",
            " Step 2: Creating Customer Segments...\n",
            " Creating detailed customer segments...\n",
            " Customer segmentation completed\n",
            " RFM Segments: 10\n",
            " Behavioral Segments: 6\n",
            " Value Segments: 6\n",
            "\n",
            " Step 3: Calculating Customer Lifetime Value...\n",
            " Calculating Customer Lifetime Value...\n",
            " CLV calculation completed\n",
            "\n",
            " Step 4: Performing Cohort Analysis...\n",
            " Creating cohort analysis...\n",
            " Cohort analysis completed\n",
            "\n",
            " Step 5: Analyzing Customer Journey...\n",
            " Analyzing customer journey patterns...\n",
            " Customer journey analysis completed\n",
            "\n",
            " Step 6: Generating Insights and Recommendations...\n",
            " Generating customer insights and recommendations...\n",
            "\n",
            " Step 7: Saving Analysis Data...\n",
            " Saving customer segmentation analysis data files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-892873010.py:38: FutureWarning:\n",
            "\n",
            "The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Customer segmentation data files saved!\n",
            "\n",
            " Step 8: Creating Interactive Dashboard...\n",
            " Creating Customer Segmentation Dashboard...\n",
            "\n",
            "================================================================================\n",
            " CUSTOMER SEGMENTATION ANALYSIS COMPLETE!\n",
            "================================================================================\n",
            "\n",
            " CUSTOMER OVERVIEW:\n",
            "    Total Customers Analyzed: 1,283,707\n",
            "    Total Revenue: $305,884,836.55\n",
            "    Average CLV: $410.86\n",
            "    Average Order Value: $50.16\n",
            "\n",
            " SEGMENT DISTRIBUTION:\n",
            "   • Potential Loyalists: 268,689 customers (20.9%)\n",
            "   • Champions: 250,673 customers (19.5%)\n",
            "   • Loyal Customers: 241,294 customers (18.8%)\n",
            "   • Hibernating: 229,024 customers (17.8%)\n",
            "   • Need Attention: 126,280 customers (9.8%)\n",
            "\n",
            " HIGH-VALUE CUSTOMERS:\n",
            "    Champions: 250,673 customers\n",
            "  Champions Revenue: $126,623,570.15\n",
            "   Champions CLV: $696.92\n",
            "\n",
            " AT-RISK CUSTOMERS:\n",
            "    High Risk: 203,138 customers (15.8%)\n",
            "    Potential Revenue Loss: $123,512,118.84\n",
            "\n",
            " BEHAVIORAL INSIGHTS:\n",
            "  Multi-Category Shoppers: 0.0%\n",
            "  Store Loyalists: 75.5%\n",
            "  Frequent Shoppers: 28.3%\n",
            "\n",
            " FILES CREATED:\n",
            "   customer_segmentation/customer_segmentation_dashboard.html\n",
            "   customer_segmentation/data/ (Multiple CSV files)\n",
            "  Segment-specific customer lists\n",
            "   Summary reports and analysis\n",
            "\n",
            " TOP RECOMMENDATIONS:\n",
            "   1. Focus retention efforts on 250,673 Champion customers\n",
            "   2. Immediate re-engagement for 203,138 at-risk customers\n",
            "   3. Develop loyalty programs for Potential Loyalists\n",
            "   4. Create win-back campaigns for hibernating customers\n",
            "\n",
            " Analysis completed successfully!\n",
            "Open the dashboard to explore detailed customer insights and segments\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def save_customer_segmentation_data(rfm_data, cohort_table, revenue_cohort_table, journey_analysis):\n",
        "    \"\"\"Save customer segmentation analysis data files\"\"\"\n",
        "    print(\" Saving customer segmentation analysis data files...\")\n",
        "\n",
        "    os.makedirs(\"customer_segmentation/data\", exist_ok=True)\n",
        "\n",
        "    # Save main datasets\n",
        "    rfm_data.to_csv(\"customer_segmentation/data/rfm_customer_analysis.csv\", index=False)\n",
        "    cohort_table.to_csv(\"customer_segmentation/data/customer_retention_cohorts.csv\")\n",
        "    revenue_cohort_table.to_csv(\"customer_segmentation/data/revenue_cohorts.csv\")\n",
        "    journey_analysis.to_csv(\"customer_segmentation/data/customer_journey_analysis.csv\", index=False)\n",
        "\n",
        "    # Create segment-specific files\n",
        "    for segment in rfm_data['Customer_Segment'].unique():\n",
        "        segment_data = rfm_data[rfm_data['Customer_Segment'] == segment]\n",
        "        filename = f\"customer_segmentation/data/segment_{segment.lower().replace(' ', '_')}.csv\"\n",
        "        segment_data.to_csv(filename, index=False)\n",
        "\n",
        "    # Create summary reports\n",
        "    segment_summary = rfm_data.groupby('Customer_Segment').agg({\n",
        "        'Customer ID': 'count',\n",
        "        'Total_CLV': ['mean', 'sum', 'std'],\n",
        "        'Historical_CLV': 'mean',\n",
        "        'Predicted_CLV': 'mean',\n",
        "        'Average_Order_Value': 'mean',\n",
        "        'Monthly_Purchase_Frequency': 'mean',\n",
        "        'Recency_Days': 'mean',\n",
        "        'Frequency': 'mean',\n",
        "        'Monetary_Value': 'mean',\n",
        "        'Total_Items_Purchased': 'mean',\n",
        "        'Unique_Products_Purchased': 'mean',\n",
        "        'Stores_Visited': 'mean'\n",
        "    }).round(2)\n",
        "\n",
        "    segment_summary.to_csv(\"customer_segmentation/data/segment_summary_report.csv\")\n",
        "\n",
        "    # CLV analysis by segment\n",
        "    clv_analysis = rfm_data.groupby(['Customer_Segment', 'CLV_Segment']).size().unstack(fill_value=0)\n",
        "    clv_analysis.to_csv(\"customer_segmentation/data/clv_by_segment_analysis.csv\")\n",
        "\n",
        "    # Risk analysis\n",
        "    risk_analysis = rfm_data.groupby(['Customer_Segment', 'Churn_Risk']).size().unstack(fill_value=0)\n",
        "    risk_analysis.to_csv(\"customer_segmentation/data/churn_risk_by_segment.csv\")\n",
        "\n",
        "    print(\" Customer segmentation data files saved!\")\n",
        "\n",
        "def generate_customer_insights_report(rfm_data, journey_analysis):\n",
        "    \"\"\"Generate detailed customer insights and recommendations\"\"\"\n",
        "    print(\" Generating customer insights and recommendations...\")\n",
        "\n",
        "    insights = {\n",
        "        'segment_insights': {},\n",
        "        'clv_insights': {},\n",
        "        'risk_insights': {},\n",
        "        'behavioral_insights': {},\n",
        "        'recommendations': {}\n",
        "    }\n",
        "\n",
        "    # Segment-specific insights\n",
        "    for segment in rfm_data['Customer_Segment'].unique():\n",
        "        segment_data = rfm_data[rfm_data['Customer_Segment'] == segment]\n",
        "\n",
        "        insights['segment_insights'][segment] = {\n",
        "            'count': len(segment_data),\n",
        "            'percentage': len(segment_data) / len(rfm_data) * 100,\n",
        "            'avg_clv': segment_data['Total_CLV'].mean(),\n",
        "            'avg_aov': segment_data['Average_Order_Value'].mean(),\n",
        "            'avg_frequency': segment_data['Monthly_Purchase_Frequency'].mean(),\n",
        "            'avg_recency': segment_data['Recency_Days'].mean(),\n",
        "            'revenue_contribution': segment_data['Monetary_Value'].sum() / rfm_data['Monetary_Value'].sum() * 100\n",
        "        }\n",
        "\n",
        "    # CLV insights\n",
        "    high_clv_customers = rfm_data[rfm_data['CLV_Segment'].isin(['High CLV', 'Very High CLV'])]\n",
        "    insights['clv_insights'] = {\n",
        "        'high_clv_count': len(high_clv_customers),\n",
        "        'high_clv_percentage': len(high_clv_customers) / len(rfm_data) * 100,\n",
        "        'high_clv_revenue_share': high_clv_customers['Monetary_Value'].sum() / rfm_data['Monetary_Value'].sum() * 100,\n",
        "        'avg_clv_all': rfm_data['Total_CLV'].mean(),\n",
        "        'top_10_percent_clv': rfm_data.nlargest(int(len(rfm_data) * 0.1), 'Total_CLV')['Total_CLV'].mean()\n",
        "    }\n",
        "\n",
        "    # Risk insights\n",
        "    at_risk_customers = rfm_data[rfm_data['Churn_Risk'] == 'High Risk']\n",
        "    insights['risk_insights'] = {\n",
        "        'high_risk_count': len(at_risk_customers),\n",
        "        'high_risk_percentage': len(at_risk_customers) / len(rfm_data) * 100,\n",
        "        'potential_revenue_loss': at_risk_customers['Predicted_CLV'].sum(),\n",
        "        'avg_recency_at_risk': at_risk_customers['Recency_Days'].mean()\n",
        "    }\n",
        "\n",
        "    # Behavioral insights\n",
        "    insights['behavioral_insights'] = {\n",
        "        'multi_category_shoppers': len(rfm_data[rfm_data['Unique_Categories_Purchased'] > 3]) / len(rfm_data) * 100,\n",
        "        'store_loyalists': len(rfm_data[rfm_data['Stores_Visited'] == 1]) / len(rfm_data) * 100,\n",
        "        'frequent_shoppers': len(rfm_data[rfm_data['Monthly_Purchase_Frequency'] > 2]) / len(rfm_data) * 100,\n",
        "        'high_aov_customers': len(rfm_data[rfm_data['Average_Order_Value'] > rfm_data['Average_Order_Value'].quantile(0.8)]) / len(rfm_data) * 100\n",
        "    }\n",
        "\n",
        "    # Generate recommendations\n",
        "    insights['recommendations'] = {\n",
        "        'champions': \"Focus on retention programs, VIP experiences, and referral incentives\",\n",
        "        'loyal_customers': \"Reward loyalty with exclusive offers and early access to new products\",\n",
        "        'potential_loyalists': \"Nurture with personalized recommendations and loyalty programs\",\n",
        "        'new_customers': \"Onboard with welcome series and product education\",\n",
        "        'at_risk': \"Immediate re-engagement campaigns with special offers\",\n",
        "        'cannot_lose_them': \"Win-back campaigns with premium service recovery\",\n",
        "        'hibernating': \"Reactivation campaigns with significant incentives\",\n",
        "        'lost': \"Final win-back attempt or remove from active marketing\"\n",
        "    }\n",
        "\n",
        "    return insights\n",
        "\n",
        "def run_customer_segmentation_analysis():\n",
        "    \"\"\"Run comprehensive customer segmentation analysis\"\"\"\n",
        "    print(\" Starting Advanced Customer Segmentation Analysis...\")\n",
        "\n",
        "    # Load data\n",
        "    master_data = load_master_data_for_customer_segmentation()\n",
        "    if master_data is None:\n",
        "        print(\" Failed to load master data!\")\n",
        "        return None\n",
        "\n",
        "    # Calculate RFM analysis\n",
        "    print(\"\\n Step 1: Calculating RFM Analysis...\")\n",
        "    rfm_data = calculate_advanced_rfm_analysis(master_data)\n",
        "\n",
        "    # Create customer segments\n",
        "    print(\"\\n Step 2: Creating Customer Segments...\")\n",
        "    rfm_data = create_customer_segments(rfm_data)\n",
        "\n",
        "    # Calculate CLV\n",
        "    print(\"\\n Step 3: Calculating Customer Lifetime Value...\")\n",
        "    rfm_data = calculate_customer_lifetime_value(rfm_data)\n",
        "\n",
        "    # Cohort analysis\n",
        "    print(\"\\n Step 4: Performing Cohort Analysis...\")\n",
        "    cohort_table, cohort_counts, revenue_cohort_table = create_cohort_analysis(master_data)\n",
        "\n",
        "    # Customer journey analysis\n",
        "    print(\"\\n Step 5: Analyzing Customer Journey...\")\n",
        "    journey_analysis = analyze_customer_journey(master_data)\n",
        "\n",
        "    # Generate insights\n",
        "    print(\"\\n Step 6: Generating Insights and Recommendations...\")\n",
        "    insights = generate_customer_insights_report(rfm_data, journey_analysis)\n",
        "\n",
        "    # Save data files\n",
        "    print(\"\\n Step 7: Saving Analysis Data...\")\n",
        "    save_customer_segmentation_data(rfm_data, cohort_table, revenue_cohort_table, journey_analysis)\n",
        "\n",
        "    # Create dashboard\n",
        "    print(\"\\n Step 8: Creating Interactive Dashboard...\")\n",
        "    html_content = create_customer_segmentation_dashboard(rfm_data, cohort_table, revenue_cohort_table, journey_analysis)\n",
        "\n",
        "    # Create directory and save HTML\n",
        "    os.makedirs(\"customer_segmentation\", exist_ok=True)\n",
        "\n",
        "    with open(\"customer_segmentation/customer_segmentation_dashboard.html\", \"w\", encoding='utf-8') as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "    # Print comprehensive summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" CUSTOMER SEGMENTATION ANALYSIS COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\n CUSTOMER OVERVIEW:\")\n",
        "    print(f\"    Total Customers Analyzed: {len(rfm_data):,}\")\n",
        "    print(f\"    Total Revenue: ${rfm_data['Monetary_Value'].sum():,.2f}\")\n",
        "    print(f\"    Average CLV: ${rfm_data['Total_CLV'].mean():,.2f}\")\n",
        "    print(f\"    Average Order Value: ${rfm_data['Average_Order_Value'].mean():.2f}\")\n",
        "\n",
        "    print(f\"\\n SEGMENT DISTRIBUTION:\")\n",
        "    segment_dist = rfm_data['Customer_Segment'].value_counts()\n",
        "    for segment, count in segment_dist.head(5).items():\n",
        "        percentage = count / len(rfm_data) * 100\n",
        "        print(f\"   • {segment}: {count:,} customers ({percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"\\n HIGH-VALUE CUSTOMERS:\")\n",
        "    champions = rfm_data[rfm_data['Customer_Segment'] == 'Champions']\n",
        "    print(f\"    Champions: {len(champions):,} customers\")\n",
        "    print(f\"  Champions Revenue: ${champions['Monetary_Value'].sum():,.2f}\")\n",
        "    print(f\"   Champions CLV: ${champions['Total_CLV'].mean():,.2f}\")\n",
        "\n",
        "    print(f\"\\n AT-RISK CUSTOMERS:\")\n",
        "    at_risk = rfm_data[rfm_data['Churn_Risk'] == 'High Risk']\n",
        "    print(f\"    High Risk: {len(at_risk):,} customers ({len(at_risk)/len(rfm_data)*100:.1f}%)\")\n",
        "    print(f\"    Potential Revenue Loss: ${at_risk['Predicted_CLV'].sum():,.2f}\")\n",
        "\n",
        "    print(f\"\\n BEHAVIORAL INSIGHTS:\")\n",
        "    print(f\"  Multi-Category Shoppers: {insights['behavioral_insights']['multi_category_shoppers']:.1f}%\")\n",
        "    print(f\"  Store Loyalists: {insights['behavioral_insights']['store_loyalists']:.1f}%\")\n",
        "    print(f\"  Frequent Shoppers: {insights['behavioral_insights']['frequent_shoppers']:.1f}%\")\n",
        "\n",
        "    print(f\"\\n FILES CREATED:\")\n",
        "    print(f\"   customer_segmentation/customer_segmentation_dashboard.html\")\n",
        "    print(f\"   customer_segmentation/data/ (Multiple CSV files)\")\n",
        "    print(f\"  Segment-specific customer lists\")\n",
        "    print(f\"   Summary reports and analysis\")\n",
        "\n",
        "    print(f\"\\n TOP RECOMMENDATIONS:\")\n",
        "    print(f\"   1. Focus retention efforts on {len(champions):,} Champion customers\")\n",
        "    print(f\"   2. Immediate re-engagement for {len(at_risk):,} at-risk customers\")\n",
        "    print(f\"   3. Develop loyalty programs for Potential Loyalists\")\n",
        "    print(f\"   4. Create win-back campaigns for hibernating customers\")\n",
        "\n",
        "    return {\n",
        "        'rfm_data': rfm_data,\n",
        "        'cohort_table': cohort_table,\n",
        "        'revenue_cohort_table': revenue_cohort_table,\n",
        "        'journey_analysis': journey_analysis,\n",
        "        'insights': insights\n",
        "    }\n",
        "\n",
        "# Run the customer segmentation analysis\n",
        "print(\" Initializing Advanced Customer Segmentation Analysis...\")\n",
        "results = run_customer_segmentation_analysis()\n",
        "\n",
        "if results:\n",
        "    print(\"\\n Analysis completed successfully!\")\n",
        "    print(\"Open the dashboard to explore detailed customer insights and segments\")\n",
        "else:\n",
        "    print(\"\\n Analysis failed. Please check the data and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "833f2881",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "833f2881",
        "outputId": "43773745-9764-49bd-a49e-2a2ed934948a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 Memory cleared: 0 objects\n"
          ]
        }
      ],
      "source": [
        "# Add this line before loading your data\n",
        "import gc; gc.collect(); print(f\"🧹 Memory cleared: {gc.collect()} objects\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "718fafba",
      "metadata": {
        "id": "718fafba"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d2e6c85",
      "metadata": {
        "id": "4d2e6c85"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistical Analysis (Correlation & Regression Analysis)"
      ],
      "metadata": {
        "id": "YwSFAqbMHpNE"
      },
      "id": "YwSFAqbMHpNE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "421cd8c4",
      "metadata": {
        "id": "421cd8c4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qty vs Sales (Employee, custoner, product etc)\n"
      ],
      "metadata": {
        "id": "fWQix_A2H-0B"
      },
      "id": "fWQix_A2H-0B"
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import statsmodels.api as sm\n",
        "import os\n",
        "\n",
        "def load_master_data():\n",
        "    \"\"\"Load master transaction data\"\"\"\n",
        "    print(\"Loading Master Data...\")\n",
        "    master_data = pl.read_parquet(\"data/master_transactions.parquet\")\n",
        "    sales_data = master_data.filter(pl.col(\"Transaction Type\") == \"Sale\")\n",
        "    print(f\"Loaded {sales_data.shape[0]:,} sales transactions\")\n",
        "    return sales_data\n",
        "\n",
        "def prepare_analysis_datasets(sales_data):\n",
        "    \"\"\"Prepare datasets for analysis\"\"\"\n",
        "    print(\"Preparing Analysis Datasets...\")\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(\"price_elasticity_analysis\", exist_ok=True)\n",
        "\n",
        "    # 1. Price Elasticity by Country\n",
        "    print(\"  Computing price elasticity...\")\n",
        "    countries = sales_data[\"Country\"].unique().to_list()\n",
        "    elasticity_results = []\n",
        "\n",
        "    for country in countries:\n",
        "        country_data = sales_data.filter(pl.col(\"Country\") == country)\n",
        "        df = country_data.select([\"Unit_Price_USD\", \"Quantity\"]).to_pandas().dropna()\n",
        "        df = df[(df['Unit_Price_USD'] > 0) & (df['Quantity'] > 0)]\n",
        "\n",
        "        if len(df) < 50:\n",
        "            continue\n",
        "\n",
        "        # Calculate elasticity using price deciles\n",
        "        df['Price_Decile'] = pd.qcut(df['Unit_Price_USD'], q=10, labels=False, duplicates='drop')\n",
        "        price_demand = df.groupby('Price_Decile').agg({\n",
        "            'Unit_Price_USD': 'mean',\n",
        "            'Quantity': 'sum'\n",
        "        }).reset_index()\n",
        "\n",
        "        if len(price_demand) < 5:\n",
        "            continue\n",
        "\n",
        "        # Log-log regression for elasticity\n",
        "        log_price = np.log(price_demand['Unit_Price_USD'])\n",
        "        log_qty = np.log(price_demand['Quantity'])\n",
        "        mask = np.isfinite(log_price) & np.isfinite(log_qty)\n",
        "\n",
        "        if mask.sum() >= 3:\n",
        "            from scipy import stats\n",
        "            slope, _, _, _, _ = stats.linregress(log_price[mask], log_qty[mask])\n",
        "            elasticity_results.append({\n",
        "                'Country': country,\n",
        "                'Price_Elasticity': round(abs(slope), 4)\n",
        "            })\n",
        "\n",
        "    elasticity_df = pl.DataFrame(elasticity_results)\n",
        "    elasticity_df.write_csv(\"price_elasticity_analysis/price_elasticity_data.csv\")\n",
        "\n",
        "    # 2. Sales vs Quantity Analysis Datasets\n",
        "    datasets = {}\n",
        "\n",
        "    # Employee Analysis\n",
        "    datasets['employee'] = sales_data.group_by(\"Employee ID\").agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Sales\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Qty\")\n",
        "    ]).filter(pl.col(\"Sales\") > 0)\n",
        "\n",
        "    # Country Analysis\n",
        "    datasets['country'] = sales_data.group_by(\"Country\").agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Sales\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Qty\"),\n",
        "        pl.col(\"Employee ID\").n_unique().alias(\"Staff_Count\")\n",
        "    ])\n",
        "\n",
        "    # City Analysis\n",
        "    datasets['city'] = sales_data.group_by([\"City\", \"Country\"]).agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Sales\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Qty\")\n",
        "    ])\n",
        "\n",
        "    # Customer Analysis\n",
        "    datasets['customer'] = sales_data.group_by(\"Customer ID\").agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Sales\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Qty\")\n",
        "    ]).filter(pl.col(\"Sales\") > 0)\n",
        "\n",
        "    # Product Analysis\n",
        "    datasets['product'] = sales_data.group_by(\"Name\").agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Sales\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Qty\")\n",
        "    ]).filter(pl.col(\"Sales\") > 0)\n",
        "\n",
        "    # Save all datasets\n",
        "    for name, data in datasets.items():\n",
        "        data.write_csv(f\"price_elasticity_analysis/{name}_analysis_data.csv\")\n",
        "\n",
        "    # 3. Multiple Regression Dataset (Country level)\n",
        "    multiple_reg_data = datasets['country'].filter(pl.col(\"Staff_Count\") > 0)\n",
        "    multiple_reg_data.write_csv(\"price_elasticity_analysis/multiple_regression_data.csv\")\n",
        "\n",
        "    print(f\"Prepared {len(datasets)} analysis datasets + elasticity + multiple regression\")\n",
        "\n",
        "def perform_regression_analysis():\n",
        "    \"\"\"Perform regression analysis from saved datasets\"\"\"\n",
        "    print(\"Performing Regression Analysis...\")\n",
        "\n",
        "    ols_summary_data = []\n",
        "\n",
        "    # Analysis types\n",
        "    analysis_types = ['employee', 'country', 'city', 'customer', 'product']\n",
        "\n",
        "    for analysis_type in analysis_types:\n",
        "        # Load data\n",
        "        df = pl.read_csv(f\"price_elasticity_analysis/{analysis_type}_analysis_data.csv\").to_pandas()\n",
        "        df = df.dropna()\n",
        "        df = df[(df['Sales'] > 0) & (df['Qty'] > 0)]\n",
        "\n",
        "        if len(df) < 3:\n",
        "            continue\n",
        "\n",
        "        # OLS Regression: Sales = coefficient * Qty + intercept\n",
        "        X = df['Qty'].values\n",
        "        y = df['Sales'].values\n",
        "        X_ols = sm.add_constant(X)\n",
        "        model = sm.OLS(y, X_ols).fit()\n",
        "\n",
        "        # Extract statistics\n",
        "        intercept = model.params[0]\n",
        "        coefficient = model.params[1]\n",
        "        r2 = model.rsquared\n",
        "        p_value = model.pvalues[1]\n",
        "\n",
        "        # Add to OLS summary\n",
        "        ols_summary_data.append({\n",
        "            'Analysis_Type': analysis_type.title(),\n",
        "            'Equation': f\"Sales = {coefficient:.1f} × Qty + {intercept:.0f}\",\n",
        "            'R_Squared': f\"{r2:.4f}\",\n",
        "            'P_Value': f\"{p_value:.6f}\",\n",
        "            'Coefficient': f\"{coefficient:.2f}\",\n",
        "            'Intercept': f\"{intercept:.2f}\",\n",
        "            'Data_Points': len(df)\n",
        "        })\n",
        "\n",
        "    # Multiple Regression\n",
        "    df_multi = pl.read_csv(\"price_elasticity_analysis/multiple_regression_data.csv\").to_pandas().dropna()\n",
        "\n",
        "    if len(df_multi) >= 3 and 'Staff_Count' in df_multi.columns:\n",
        "        X_multi = df_multi[['Staff_Count', 'Qty']].values\n",
        "        y_multi = df_multi['Sales'].values\n",
        "        X_multi_ols = sm.add_constant(X_multi)\n",
        "        model_multi = sm.OLS(y_multi, X_multi_ols).fit()\n",
        "\n",
        "        intercept_multi = model_multi.params[0]\n",
        "        coef_staff = model_multi.params[1]\n",
        "        coef_qty = model_multi.params[2]\n",
        "        r2_multi = model_multi.rsquared\n",
        "        p_value_multi = model_multi.f_pvalue\n",
        "\n",
        "        # Add to OLS summary\n",
        "        ols_summary_data.append({\n",
        "            'Analysis_Type': 'Multiple Regression',\n",
        "            'Equation': f\"Sales = {coef_staff:.1f} × Staff + {coef_qty:.1f} × Qty + {intercept_multi:.0f}\",\n",
        "            'R_Squared': f\"{r2_multi:.4f}\",\n",
        "            'P_Value': f\"{p_value_multi:.6f}\",\n",
        "            'Coefficient': f\"Staff:{coef_staff:.1f}, Qty:{coef_qty:.1f}\",\n",
        "            'Intercept': f\"{intercept_multi:.2f}\",\n",
        "            'Data_Points': len(df_multi)\n",
        "        })\n",
        "\n",
        "    # Save OLS Summary\n",
        "    ols_summary_df = pl.DataFrame(ols_summary_data)\n",
        "    ols_summary_df.write_csv(\"price_elasticity_analysis/ols_summary.csv\")\n",
        "\n",
        "    print(\"Regression analysis completed\")\n",
        "\n",
        "def create_dashboard():\n",
        "    \"\"\"Create dashboard using same logic as workforce productivity\"\"\"\n",
        "    print(\"Creating Dashboard...\")\n",
        "\n",
        "    # Load data from CSV files\n",
        "    elasticity_df = pl.read_csv(\"price_elasticity_analysis/price_elasticity_data.csv\").to_pandas()\n",
        "    ols_summary_df = pl.read_csv(\"price_elasticity_analysis/ols_summary.csv\").to_pandas()\n",
        "\n",
        "    # Create individual figures first (same as workforce dashboard)\n",
        "    figures = []\n",
        "\n",
        "    # 1. Price Elasticity Chart\n",
        "    fig1 = px.bar(\n",
        "        elasticity_df,\n",
        "        x='Country',\n",
        "        y='Price_Elasticity',\n",
        "        title='Price Elasticity by Country',\n",
        "        text='Price_Elasticity'\n",
        "    )\n",
        "    fig1.update_traces(texttemplate='%{text:.4f}', textposition='outside')\n",
        "    fig1.update_layout(height=400)\n",
        "    figures.append(fig1)\n",
        "\n",
        "    # 2. OLS Summary Table\n",
        "    fig2 = go.Figure(data=[go.Table(\n",
        "        header=dict(values=list(ols_summary_df.columns),\n",
        "                   fill_color='lightblue',\n",
        "                   align='center'),\n",
        "        cells=dict(values=[ols_summary_df[col] for col in ols_summary_df.columns],\n",
        "                  fill_color='white',\n",
        "                  align='center')\n",
        "    )])\n",
        "    fig2.update_layout(title=\"OLS Summary Statistics\", height=400)\n",
        "    figures.append(fig2)\n",
        "\n",
        "    # 3-7. Sales vs Quantity Regression Charts\n",
        "    analysis_types = ['employee', 'country', 'city', 'customer', 'product']\n",
        "\n",
        "    for analysis_type in analysis_types:\n",
        "        try:\n",
        "            df = pl.read_csv(f\"price_elasticity_analysis/{analysis_type}_analysis_data.csv\").to_pandas()\n",
        "            df = df.dropna()\n",
        "            df = df[(df['Sales'] > 0) & (df['Qty'] > 0)]\n",
        "\n",
        "            if len(df) < 3:\n",
        "                continue\n",
        "\n",
        "            # Calculate regression\n",
        "            X = df['Qty'].values\n",
        "            y = df['Sales'].values\n",
        "            X_ols = sm.add_constant(X)\n",
        "            model = sm.OLS(y, X_ols).fit()\n",
        "            y_pred = model.params[0] + model.params[1] * X\n",
        "\n",
        "            # Calculate medians\n",
        "            sales_median = df['Sales'].median()\n",
        "            qty_median = df['Qty'].median()\n",
        "\n",
        "            # Create scatter plot\n",
        "            fig = go.Figure()\n",
        "\n",
        "            # Add scatter points\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=df['Qty'],\n",
        "                y=df['Sales'],\n",
        "                mode='markers',\n",
        "                name='Data Points',\n",
        "                marker=dict(size=6, opacity=0.6)\n",
        "            ))\n",
        "\n",
        "            # Add regression line\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=df['Qty'],\n",
        "                y=y_pred,\n",
        "                mode='lines',\n",
        "                name='Regression Line',\n",
        "                line=dict(color='red', width=2)\n",
        "            ))\n",
        "\n",
        "            # Add median lines\n",
        "            fig.add_shape(\n",
        "                type=\"line\",\n",
        "                x0=qty_median, y0=df['Sales'].min(),\n",
        "                x1=qty_median, y1=df['Sales'].max(),\n",
        "                line=dict(color=\"gray\", width=1, dash=\"dash\")\n",
        "            )\n",
        "\n",
        "            fig.add_shape(\n",
        "                type=\"line\",\n",
        "                x0=df['Qty'].min(), y0=sales_median,\n",
        "                x1=df['Qty'].max(), y1=sales_median,\n",
        "                line=dict(color=\"gray\", width=1, dash=\"dash\")\n",
        "            )\n",
        "\n",
        "            # Add equation annotation\n",
        "            equation = f\"Sales = {model.params[1]:.1f} × Qty + {model.params[0]:.0f}\"\n",
        "            r2 = model.rsquared\n",
        "\n",
        "            fig.add_annotation(\n",
        "                x=0.05, y=0.95,\n",
        "                xref=\"paper\", yref=\"paper\",\n",
        "                text=f\"{equation}<br>R² = {r2:.3f}\",\n",
        "                showarrow=False,\n",
        "                font=dict(size=12),\n",
        "                bgcolor=\"rgba(255,255,255,0.8)\",\n",
        "                bordercolor=\"gray\",\n",
        "                borderwidth=1\n",
        "            )\n",
        "\n",
        "            fig.update_layout(\n",
        "                title=f'{analysis_type.title()}: Sales vs Quantity',\n",
        "                xaxis_title='Quantity',\n",
        "                yaxis_title='Sales (USD)',\n",
        "                height=400\n",
        "            )\n",
        "\n",
        "            figures.append(fig)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Could not create chart for {analysis_type}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # 8. Multiple Regression 3D\n",
        "    try:\n",
        "        multiple_reg_data = pl.read_csv(\"price_elasticity_analysis/multiple_regression_data.csv\").to_pandas()\n",
        "\n",
        "        if len(multiple_reg_data) >= 3:\n",
        "            fig3d = go.Figure(data=[go.Scatter3d(\n",
        "                x=multiple_reg_data['Staff_Count'],\n",
        "                y=multiple_reg_data['Qty'],\n",
        "                z=multiple_reg_data['Sales'],\n",
        "                mode='markers+text',\n",
        "                text=multiple_reg_data['Country'] if 'Country' in multiple_reg_data.columns else None,\n",
        "                marker=dict(\n",
        "                    size=8,\n",
        "                    color=multiple_reg_data['Sales'],\n",
        "                    colorscale='Viridis',\n",
        "                    showscale=True\n",
        "                )\n",
        "            )])\n",
        "\n",
        "            fig3d.update_layout(\n",
        "                title='Multiple Regression: Sales = f(Staff Count, Quantity)',\n",
        "                scene=dict(\n",
        "                    xaxis_title='Staff Count',\n",
        "                    yaxis_title='Quantity',\n",
        "                    zaxis_title='Sales (USD)'\n",
        "                ),\n",
        "                height=500\n",
        "            )\n",
        "\n",
        "            figures.append(fig3d)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Combine all figures into dashboard (same as workforce dashboard)\n",
        "    dashboard_html = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Price Elasticity & Regression Analysis Dashboard</title>\n",
        "        <style>\n",
        "            body { font-family: Arial, sans-serif; margin: 20px; }\n",
        "            .chart-container { margin-bottom: 30px; }\n",
        "            h1 { text-align: center; color: #2E86AB; }\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>Price Elasticity & Sales-Quantity Regression Analysis Dashboard</h1>\n",
        "    \"\"\"\n",
        "\n",
        "    # Add each figure to HTML\n",
        "    for i, fig in enumerate(figures):\n",
        "        chart_html = fig.to_html(include_plotlyjs='inline' if i == 0 else False, div_id=f\"chart_{i}\")\n",
        "        dashboard_html += f'<div class=\"chart-container\">{chart_html}</div>'\n",
        "\n",
        "    dashboard_html += \"\"\"\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    # Save dashboard\n",
        "    with open(\"price_elasticity_analysis/price_elasticity_dashboard.html\", \"w\", encoding='utf-8') as f:\n",
        "        f.write(dashboard_html)\n",
        "\n",
        "    print(\"Dashboard saved as: price_elasticity_dashboard.html\")\n",
        "\n",
        "    # Return first figure for display\n",
        "    return figures[0] if figures else None\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main analysis pipeline\"\"\"\n",
        "    print(\"PRICE ELASTICITY & REGRESSION ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Step 1: Load master data\n",
        "    sales_data = load_master_data()\n",
        "\n",
        "    # Step 2: Prepare analysis datasets and save to CSV\n",
        "    prepare_analysis_datasets(sales_data)\n",
        "\n",
        "    # Step 3: Perform regression analysis from CSV data\n",
        "    perform_regression_analysis()\n",
        "\n",
        "    # Step 4: Create dashboard from CSV data\n",
        "    dashboard_fig = create_dashboard()\n",
        "\n",
        "    # Step 5: Display first chart\n",
        "    if dashboard_fig:\n",
        "        print(\"\\nDisplaying Sample Chart...\")\n",
        "        dashboard_fig.show()\n",
        "\n",
        "    print(\"\\nANALYSIS COMPLETED!\")\n",
        "    print(\"Results saved in: price_elasticity_analysis/\")\n",
        "    print(\"Dashboard: price_elasticity_dashboard.html\")\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "0lvUcsz4IE66",
        "outputId": "1e0a0d58-ab43-4857-b14f-472d3427d114"
      },
      "id": "0lvUcsz4IE66",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRICE ELASTICITY & REGRESSION ANALYSIS\n",
            "============================================================\n",
            "Loading Master Data...\n",
            "Loaded 6,077,200 sales transactions\n",
            "Preparing Analysis Datasets...\n",
            "  Computing price elasticity...\n",
            "Prepared 5 analysis datasets + elasticity + multiple regression\n",
            "Performing Regression Analysis...\n",
            "Regression analysis completed\n",
            "Creating Dashboard...\n",
            "Dashboard saved as: price_elasticity_dashboard.html\n",
            "\n",
            "Displaying Sample Chart...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"f1498f13-2595-4501-a5a0-d8161dc2c2b8\" class=\"plotly-graph-div\" style=\"height:400px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f1498f13-2595-4501-a5a0-d8161dc2c2b8\")) {                    Plotly.newPlot(                        \"f1498f13-2595-4501-a5a0-d8161dc2c2b8\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Country=%{x}\\u003cbr\\u003ePrice_Elasticity=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"text\":[0.01,0.0242,0.001,0.0144,0.0216,0.0025,0.0039],\"textposition\":\"outside\",\"x\":[\"France\",\"United States\",\"Spain\",\"Germany\",\"United Kingdom\",\"China\",\"Portugal\"],\"xaxis\":\"x\",\"y\":[0.01,0.0242,0.001,0.0144,0.0216,0.0025,0.0039],\"yaxis\":\"y\",\"type\":\"bar\",\"texttemplate\":\"%{text:.4f}\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Country\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Price_Elasticity\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Price Elasticity by Country\"},\"barmode\":\"relative\",\"height\":400},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f1498f13-2595-4501-a5a0-d8161dc2c2b8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ANALYSIS COMPLETED!\n",
            "Results saved in: price_elasticity_analysis/\n",
            "Dashboard: price_elasticity_dashboard.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "import os\n",
        "\n",
        "def load_master_data():\n",
        "    \"\"\"Load master transaction data\"\"\"\n",
        "    print(\" Loading Master Data...\")\n",
        "    master_data = pl.read_parquet(\"data/master_transactions.parquet\")\n",
        "    sales_data = master_data.filter(pl.col(\"Transaction Type\") == \"Sale\")\n",
        "    print(f\"✓ Loaded {sales_data.shape[0]:,} sales transactions\")\n",
        "    return sales_data\n",
        "\n",
        "def prepare_correlation_data(sales_data):\n",
        "    \"\"\"Prepare data for correlation analysis - aggregated by Store ID\"\"\"\n",
        "    print(\" Preparing Correlation Dataset by Store ID...\")\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(\"correlation_analysis_results\", exist_ok=True)\n",
        "\n",
        "    # Aggregate by Store ID\n",
        "    correlation_data = sales_data.group_by(\"Store ID\").agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Total_Sales\"),\n",
        "        pl.col(\"Employee ID\").n_unique().alias(\"Staff_Count\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Total_Quantity\")\n",
        "    ]).filter(\n",
        "        (pl.col(\"Total_Sales\") > 0) &\n",
        "        (pl.col(\"Staff_Count\") > 0) &\n",
        "        (pl.col(\"Total_Quantity\") > 0)\n",
        "    )\n",
        "\n",
        "    # Save dataset\n",
        "    correlation_data.write_csv(\"correlation_analysis_results/correlation_data.csv\")\n",
        "\n",
        "    print(f\"✓ Dataset prepared: {correlation_data.shape[0]} stores\")\n",
        "    print(\"✓ Variables: Total_Sales, Staff_Count, Total_Quantity\")\n",
        "    return correlation_data\n",
        "\n",
        "def run_correlation_analysis():\n",
        "    \"\"\"Run correlation analysis between key variables\"\"\"\n",
        "    print(\" Running Correlation Analysis...\")\n",
        "\n",
        "    # Load data\n",
        "    df = pl.read_csv(\"correlation_analysis_results/correlation_data.csv\").to_pandas()\n",
        "    df = df.dropna()\n",
        "\n",
        "    print(f\"✓ Analysis dataset: {len(df)} stores\")\n",
        "\n",
        "    # Calculate correlations with p-values\n",
        "    corr_staff_sales, p_staff_sales = pearsonr(df['Staff_Count'], df['Total_Sales'])\n",
        "    corr_staff_quantity, p_staff_quantity = pearsonr(df['Staff_Count'], df['Total_Quantity'])\n",
        "    corr_quantity_sales, p_quantity_sales = pearsonr(df['Total_Quantity'], df['Total_Sales'])\n",
        "\n",
        "    # Print correlation results\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CORRELATION ANALYSIS RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\n CORRELATION COEFFICIENTS:\")\n",
        "    print(f\"   Staff Count vs Total Sales:    r = {corr_staff_sales:.3f} (p = {p_staff_sales:.3f})\")\n",
        "    print(f\"   Staff Count vs Total Quantity: r = {corr_staff_quantity:.3f} (p = {p_staff_quantity:.3f})\")\n",
        "    print(f\"   Total Quantity vs Total Sales: r = {corr_quantity_sales:.3f} (p = {p_quantity_sales:.3f})\")\n",
        "\n",
        "    # Interpretation\n",
        "    print(f\"\\n STATISTICAL SIGNIFICANCE (α = 0.05):\")\n",
        "    print(f\"   Staff vs Sales:    {'Significant' if p_staff_sales < 0.05 else 'Not Significant'}\")\n",
        "    print(f\"   Staff vs Quantity: {'Significant' if p_staff_quantity < 0.05 else 'Not Significant'}\")\n",
        "    print(f\"   Quantity vs Sales: {'Significant' if p_quantity_sales < 0.05 else 'Not Significant'}\")\n",
        "\n",
        "    # Correlation strength interpretation\n",
        "    def interpret_correlation(r):\n",
        "        abs_r = abs(r)\n",
        "        if abs_r >= 0.7:\n",
        "            return \"Strong\"\n",
        "        elif abs_r >= 0.3:\n",
        "            return \"Moderate\"\n",
        "        elif abs_r >= 0.1:\n",
        "            return \"Weak\"\n",
        "        else:\n",
        "            return \"Very Weak\"\n",
        "\n",
        "    print(f\"\\n CORRELATION STRENGTH:\")\n",
        "    print(f\"   Staff vs Sales:    {interpret_correlation(corr_staff_sales)} ({corr_staff_sales:.3f})\")\n",
        "    print(f\"   Staff vs Quantity: {interpret_correlation(corr_staff_quantity)} ({corr_staff_quantity:.3f})\")\n",
        "    print(f\"   Quantity vs Sales: {interpret_correlation(corr_quantity_sales)} ({corr_quantity_sales:.3f})\")\n",
        "\n",
        "    # Create correlation matrix\n",
        "    correlation_matrix = df[['Staff_Count', 'Total_Sales', 'Total_Quantity']].corr()\n",
        "\n",
        "    print(f\"\\n CORRELATION MATRIX:\")\n",
        "    print(correlation_matrix.round(3))\n",
        "\n",
        "    # Save results\n",
        "    results_summary = {\n",
        "        'Correlation_Pair': ['Staff_Count vs Total_Sales', 'Staff_Count vs Total_Quantity', 'Total_Quantity vs Total_Sales'],\n",
        "        'Correlation_Coefficient': [corr_staff_sales, corr_staff_quantity, corr_quantity_sales],\n",
        "        'P_Value': [p_staff_sales, p_staff_quantity, p_quantity_sales],\n",
        "        'Significance': [p_staff_sales < 0.05, p_staff_quantity < 0.05, p_quantity_sales < 0.05],\n",
        "        'Strength': [interpret_correlation(corr_staff_sales), interpret_correlation(corr_staff_quantity), interpret_correlation(corr_quantity_sales)]\n",
        "    }\n",
        "\n",
        "    results_df = pd.DataFrame(results_summary)\n",
        "    results_df.to_csv(\"correlation_analysis_results/correlation_results.csv\", index=False)\n",
        "\n",
        "    # Save correlation matrix\n",
        "    correlation_matrix.to_csv(\"correlation_analysis_results/correlation_matrix.csv\")\n",
        "\n",
        "    # Save detailed summary\n",
        "    with open(\"correlation_analysis_results/correlation_summary.txt\", \"w\") as f:\n",
        "        f.write(\"CORRELATION ANALYSIS SUMMARY\\n\")\n",
        "        f.write(\"=\"*50 + \"\\n\\n\")\n",
        "        f.write(f\"Dataset: {len(df)} stores analyzed\\n\\n\")\n",
        "        f.write(\"CORRELATION RESULTS:\\n\")\n",
        "        f.write(f\"Staff Count vs Total Sales:    r = {corr_staff_sales:.3f} (p = {p_staff_sales:.3f}) - {interpret_correlation(corr_staff_sales)}\\n\")\n",
        "        f.write(f\"Staff Count vs Total Quantity: r = {corr_staff_quantity:.3f} (p = {p_staff_quantity:.3f}) - {interpret_correlation(corr_staff_quantity)}\\n\")\n",
        "        f.write(f\"Total Quantity vs Total Sales: r = {corr_quantity_sales:.3f} (p = {p_quantity_sales:.3f}) - {interpret_correlation(corr_quantity_sales)}\\n\\n\")\n",
        "        f.write(\"CORRELATION MATRIX:\\n\")\n",
        "        f.write(str(correlation_matrix.round(3)))\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(f\"\\n DATASET SUMMARY:\")\n",
        "    print(f\"   Stores analyzed: {len(df)}\")\n",
        "    print(f\"   Total Sales range: ${df['Total_Sales'].min():,.0f} - ${df['Total_Sales'].max():,.0f}\")\n",
        "    print(f\"   Staff Count range: {df['Staff_Count'].min()} - {df['Staff_Count'].max()}\")\n",
        "    print(f\"   Total Quantity range: {df['Total_Quantity'].min():,.0f} - {df['Total_Quantity'].max():,.0f}\")\n",
        "\n",
        "    print(f\"\\n✓ Results saved to: correlation_analysis_results/\")\n",
        "    print(f\"✓ Summary: correlation_summary.txt\")\n",
        "    print(f\"✓ Results: correlation_results.csv\")\n",
        "    print(f\"✓ Matrix: correlation_matrix.csv\")\n",
        "\n",
        "    return {\n",
        "        'staff_sales_corr': corr_staff_sales,\n",
        "        'staff_sales_p': p_staff_sales,\n",
        "        'staff_quantity_corr': corr_staff_quantity,\n",
        "        'staff_quantity_p': p_staff_quantity,\n",
        "        'quantity_sales_corr': corr_quantity_sales,\n",
        "        'quantity_sales_p': p_quantity_sales,\n",
        "        'correlation_matrix': correlation_matrix,\n",
        "        'data': df\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function - Correlation analysis by Store ID\"\"\"\n",
        "    print(\" CORRELATION ANALYSIS BY STORE\")\n",
        "    print(\"Variables: Staff_Count, Total_Sales, Total_Quantity\")\n",
        "    print(\"Aggregation Level: Store ID\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load data\n",
        "    sales_data = load_master_data()\n",
        "\n",
        "    # Prepare correlation dataset by Store ID\n",
        "    correlation_data = prepare_correlation_data(sales_data)\n",
        "\n",
        "    # Run correlation analysis\n",
        "    results = run_correlation_analysis()\n",
        "\n",
        "    print(\"\\n CORRELATION ANALYSIS COMPLETED!\")\n",
        "\n",
        "    return results\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLnNKA23IFfB",
        "outputId": "09168cd3-54ee-4dd6-d7db-d5a17ab88e97"
      },
      "id": "wLnNKA23IFfB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " CORRELATION ANALYSIS BY STORE\n",
            "Variables: Staff_Count, Total_Sales, Total_Quantity\n",
            "Aggregation Level: Store ID\n",
            "======================================================================\n",
            " Loading Master Data...\n",
            "✓ Loaded 6,077,200 sales transactions\n",
            " Preparing Correlation Dataset by Store ID...\n",
            "✓ Dataset prepared: 35 stores\n",
            "✓ Variables: Total_Sales, Staff_Count, Total_Quantity\n",
            " Running Correlation Analysis...\n",
            "✓ Analysis dataset: 35 stores\n",
            "\n",
            "================================================================================\n",
            "CORRELATION ANALYSIS RESULTS\n",
            "================================================================================\n",
            "\n",
            " CORRELATION COEFFICIENTS:\n",
            "   Staff Count vs Total Sales:    r = 0.328 (p = 0.054)\n",
            "   Staff Count vs Total Quantity: r = 0.302 (p = 0.078)\n",
            "   Total Quantity vs Total Sales: r = 0.994 (p = 0.000)\n",
            "\n",
            " STATISTICAL SIGNIFICANCE (α = 0.05):\n",
            "   Staff vs Sales:    Not Significant\n",
            "   Staff vs Quantity: Not Significant\n",
            "   Quantity vs Sales: Significant\n",
            "\n",
            " CORRELATION STRENGTH:\n",
            "   Staff vs Sales:    Moderate (0.328)\n",
            "   Staff vs Quantity: Moderate (0.302)\n",
            "   Quantity vs Sales: Strong (0.994)\n",
            "\n",
            " CORRELATION MATRIX:\n",
            "                Staff_Count  Total_Sales  Total_Quantity\n",
            "Staff_Count           1.000        0.328           0.302\n",
            "Total_Sales           0.328        1.000           0.994\n",
            "Total_Quantity        0.302        0.994           1.000\n",
            "\n",
            " DATASET SUMMARY:\n",
            "   Stores analyzed: 35\n",
            "   Total Sales range: $3,212,839 - $22,479,566\n",
            "   Staff Count range: 6 - 9\n",
            "   Total Quantity range: 70,218 - 497,826\n",
            "\n",
            "✓ Results saved to: correlation_analysis_results/\n",
            "✓ Summary: correlation_summary.txt\n",
            "✓ Results: correlation_results.csv\n",
            "✓ Matrix: correlation_matrix.csv\n",
            "\n",
            " CORRELATION ANALYSIS COMPLETED!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'staff_sales_corr': np.float64(0.32814064237136814),\n",
              " 'staff_sales_p': np.float64(0.05429575349629119),\n",
              " 'staff_quantity_corr': np.float64(0.302315623709834),\n",
              " 'staff_quantity_p': np.float64(0.07754123765764515),\n",
              " 'quantity_sales_corr': np.float64(0.9944932462471471),\n",
              " 'quantity_sales_p': np.float64(6.511329730628192e-34),\n",
              " 'correlation_matrix':                 Staff_Count  Total_Sales  Total_Quantity\n",
              " Staff_Count        1.000000     0.328141        0.302316\n",
              " Total_Sales        0.328141     1.000000        0.994493\n",
              " Total_Quantity     0.302316     0.994493        1.000000,\n",
              " 'data':     Store ID   Total_Sales  Staff_Count  Total_Quantity\n",
              " 0         28  3.370185e+06            6           73413\n",
              " 1         30  5.766159e+06            8          125296\n",
              " 2         12  7.749670e+06            9          169491\n",
              " 3         10  1.025459e+07            9          207591\n",
              " 4         27  4.143102e+06            8           91110\n",
              " 5         33  3.212839e+06            8           70218\n",
              " 6          4  1.473058e+07            9          324470\n",
              " 7          7  1.477411e+07            9          297646\n",
              " 8         24  5.734203e+06            7          125953\n",
              " 9         19  3.548007e+06            8           92573\n",
              " 10        15  3.656242e+06            9           79929\n",
              " 11        22  5.389224e+06            6          117395\n",
              " 12        13  5.380471e+06            8          118065\n",
              " 13         3  1.123856e+07            8          248428\n",
              " 14        21  8.998855e+06            6          196301\n",
              " 15        25  5.047870e+06            8          109782\n",
              " 16        31  9.197025e+06            9          199140\n",
              " 17        18  3.224020e+06            6           85349\n",
              " 18        34  3.903762e+06            8           85108\n",
              " 19        16  7.322851e+06            6          196761\n",
              " 20         9  1.607692e+07            8          328097\n",
              " 21         1  2.247957e+07            9          497826\n",
              " 22         6  1.958413e+07            7          399454\n",
              " 23        11  1.303465e+07            8          284338\n",
              " 24        35  4.685165e+06            6          101865\n",
              " 25        23  5.767722e+06            7          126294\n",
              " 26        14  5.904459e+06            7          129749\n",
              " 27         2  2.142437e+07            7          474915\n",
              " 28        29  4.495504e+06            7           97179\n",
              " 29        20  5.238047e+06            6          137608\n",
              " 30        26  9.628973e+06            6          207968\n",
              " 31         8  1.915328e+07            9          386172\n",
              " 32        32  6.809921e+06            6          147190\n",
              " 33         5  9.591131e+06            8          212719\n",
              " 34        17  5.368664e+06            8          140731}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import os\n",
        "\n",
        "def load_master_data():\n",
        "    \"\"\"Load master transaction data\"\"\"\n",
        "    print(\" Loading Master Data...\")\n",
        "    master_data = pl.read_parquet(\"data/master_transactions.parquet\")\n",
        "    sales_data = master_data.filter(pl.col(\"Transaction Type\") == \"Sale\")\n",
        "    print(f\"✓ Loaded {sales_data.shape[0]:,} sales transactions\")\n",
        "    return sales_data\n",
        "\n",
        "def prepare_regression_data(sales_data):\n",
        "    \"\"\"Prepare data for multiple regression - aggregated by Store ID\"\"\"\n",
        "    print(\" Preparing Regression Dataset by Store ID...\")\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(\"ols_regression_results\", exist_ok=True)\n",
        "\n",
        "    # Aggregate by Store ID\n",
        "    regression_data = sales_data.group_by(\"Store ID\").agg([\n",
        "        pl.col(\"Line_Total_USD\").sum().alias(\"Total_Sales\"),\n",
        "        pl.col(\"Employee ID\").n_unique().alias(\"Staff_Count\"),\n",
        "        pl.col(\"Quantity\").sum().alias(\"Total_Quantity\")\n",
        "    ]).filter(\n",
        "        (pl.col(\"Total_Sales\") > 0) &\n",
        "        (pl.col(\"Staff_Count\") > 0) &\n",
        "        (pl.col(\"Total_Quantity\") > 0)\n",
        "    )\n",
        "\n",
        "    # Save dataset\n",
        "    regression_data.write_csv(\"ols_regression_results/regression_data.csv\")\n",
        "\n",
        "    print(f\"✓ Dataset prepared: {regression_data.shape[0]} stores\")\n",
        "    print(\"✓ Variables: Total_Sales, Staff_Count, Total_Quantity\")\n",
        "    return regression_data\n",
        "\n",
        "def run_ols_regression():\n",
        "    \"\"\"Run OLS regression and display results table\"\"\"\n",
        "    print(\" Running OLS Regression...\")\n",
        "\n",
        "    # Load data\n",
        "    df = pl.read_csv(\"ols_regression_results/regression_data.csv\").to_pandas()\n",
        "    df = df.dropna()\n",
        "\n",
        "    print(f\"✓ Analysis dataset: {len(df)} stores\")\n",
        "    print(f\"✓ Variables: {list(df.columns)}\")\n",
        "\n",
        "    # Define variables\n",
        "    Y = df['Total_Sales']  # Dependent variable\n",
        "    X = df[['Staff_Count', 'Total_Quantity']]  # Independent variables\n",
        "\n",
        "    # Add constant (intercept)\n",
        "    X = sm.add_constant(X)\n",
        "\n",
        "    # Fit OLS model\n",
        "    model = sm.OLS(Y, X).fit()\n",
        "\n",
        "    # Print the regression results table\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"OLS REGRESSION RESULTS\")\n",
        "    print(\"Total_Sales = β₀ + β₁×Staff_Count + β₂×Total_Quantity + ε\")\n",
        "    print(\"=\"*80)\n",
        "    print(model.summary())\n",
        "\n",
        "    # Save results to file\n",
        "    with open(\"ols_regression_results/regression_summary.txt\", \"w\") as f:\n",
        "        f.write(\"OLS REGRESSION RESULTS\\n\")\n",
        "        f.write(\"Total_Sales = β₀ + β₁×Staff_Count + β₂×Total_Quantity + ε\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\")\n",
        "        f.write(str(model.summary()))\n",
        "\n",
        "    # Save coefficients table\n",
        "    results_df = pd.DataFrame({\n",
        "        'Variable': ['const', 'Staff_Count', 'Total_Quantity'],\n",
        "        'Coefficient': model.params.values,\n",
        "        'Std_Error': model.bse.values,\n",
        "        't_Statistic': model.tvalues.values,\n",
        "        'P_Value': model.pvalues.values,\n",
        "        'Conf_Int_Lower': model.conf_int()[0].values,\n",
        "        'Conf_Int_Upper': model.conf_int()[1].values\n",
        "    })\n",
        "\n",
        "    results_df.to_csv(\"ols_regression_results/coefficients_table.csv\", index=False)\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(f\"\\n DATASET SUMMARY:\")\n",
        "    print(f\"   Stores analyzed: {len(df)}\")\n",
        "    print(f\"   Total Sales range: ${df['Total_Sales'].min():,.0f} - ${df['Total_Sales'].max():,.0f}\")\n",
        "    print(f\"   Staff Count range: {df['Staff_Count'].min()} - {df['Staff_Count'].max()}\")\n",
        "    print(f\"   Total Quantity range: {df['Total_Quantity'].min():,.0f} - {df['Total_Quantity'].max():,.0f}\")\n",
        "\n",
        "    print(f\"\\n✓ Results saved to: ols_regression_results/\")\n",
        "    print(f\"✓ Summary: regression_summary.txt\")\n",
        "    print(f\"✓ Coefficients: coefficients_table.csv\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function - OLS regression by Store ID\"\"\"\n",
        "    print(\" OLS REGRESSION ANALYSIS BY STORE\")\n",
        "    print(\"Model: Total_Sales = β₀ + β₁×Staff_Count + β₂×Total_Quantity + ε\")\n",
        "    print(\"Aggregation Level: Store ID\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load data\n",
        "    sales_data = load_master_data()\n",
        "\n",
        "    # Prepare regression dataset by Store ID\n",
        "    regression_data = prepare_regression_data(sales_data)\n",
        "\n",
        "    # Run OLS regression and display table\n",
        "    model = run_ols_regression()\n",
        "\n",
        "    print(\"\\n OLS REGRESSION COMPLETED!\")\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2aVcbxrKW18",
        "outputId": "91a60ed5-5f9e-4463-b9d5-2fe73bd04a6d"
      },
      "id": "h2aVcbxrKW18",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " OLS REGRESSION ANALYSIS BY STORE\n",
            "Model: Total_Sales = β₀ + β₁×Staff_Count + β₂×Total_Quantity + ε\n",
            "Aggregation Level: Store ID\n",
            "======================================================================\n",
            " Loading Master Data...\n",
            "✓ Loaded 6,077,200 sales transactions\n",
            " Preparing Regression Dataset by Store ID...\n",
            "✓ Dataset prepared: 35 stores\n",
            "✓ Variables: Total_Sales, Staff_Count, Total_Quantity\n",
            " Running OLS Regression...\n",
            "✓ Analysis dataset: 35 stores\n",
            "✓ Variables: ['Store ID', 'Total_Sales', 'Staff_Count', 'Total_Quantity']\n",
            "\n",
            "================================================================================\n",
            "OLS REGRESSION RESULTS\n",
            "Total_Sales = β₀ + β₁×Staff_Count + β₂×Total_Quantity + ε\n",
            "================================================================================\n",
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:            Total_Sales   R-squared:                       0.990\n",
            "Model:                            OLS   Adj. R-squared:                  0.989\n",
            "Method:                 Least Squares   F-statistic:                     1560.\n",
            "Date:                Sun, 03 Aug 2025   Prob (F-statistic):           1.27e-32\n",
            "Time:                        18:20:53   Log-Likelihood:                -512.46\n",
            "No. Observations:                  35   AIC:                             1031.\n",
            "Df Residuals:                      32   BIC:                             1036.\n",
            "Df Model:                           2                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==================================================================================\n",
            "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
            "----------------------------------------------------------------------------------\n",
            "const          -1.416e+06   6.76e+05     -2.095      0.044   -2.79e+06   -3.95e+04\n",
            "Staff_Count     1.503e+05   9.28e+04      1.619      0.115   -3.88e+04    3.39e+05\n",
            "Total_Quantity    47.2269      0.896     52.734      0.000      45.403      49.051\n",
            "==============================================================================\n",
            "Omnibus:                        2.179   Durbin-Watson:                   1.936\n",
            "Prob(Omnibus):                  0.336   Jarque-Bera (JB):                1.299\n",
            "Skew:                          -0.456   Prob(JB):                        0.522\n",
            "Kurtosis:                       3.247   Cond. No.                     1.55e+06\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 1.55e+06. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n",
            "\n",
            " DATASET SUMMARY:\n",
            "   Stores analyzed: 35\n",
            "   Total Sales range: $3,212,839 - $22,479,566\n",
            "   Staff Count range: 6 - 9\n",
            "   Total Quantity range: 70,218 - 497,826\n",
            "\n",
            "✓ Results saved to: ols_regression_results/\n",
            "✓ Summary: regression_summary.txt\n",
            "✓ Coefficients: coefficients_table.csv\n",
            "\n",
            " OLS REGRESSION COMPLETED!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N7NAQZQAKiXQ"
      },
      "id": "N7NAQZQAKiXQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descriptive stats"
      ],
      "metadata": {
        "id": "_03zbI4ABwgN"
      },
      "id": "_03zbI4ABwgN"
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_descriptive_statistics(discounts_pl, products_pl, employees_pl, stores_pl, customers_pl, transactions_pl):\n",
        "    \"\"\"\n",
        "    Perform comprehensive descriptive statistics on all datasets\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PERFORMING DESCRIPTIVE STATISTICS ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    import os\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Create output directory\n",
        "    stats_dir = \"descriptive_statistics\"\n",
        "    os.makedirs(stats_dir, exist_ok=True)\n",
        "\n",
        "    stats_summary = {}\n",
        "\n",
        "    # 1. DISCOUNTS DATASET ANALYSIS\n",
        "    print(\"\\nDISCOUNTS Dataset Analysis:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    discounts_stats = {\n",
        "        'total_records': discounts_pl.shape[0],\n",
        "        'total_columns': discounts_pl.shape[1],\n",
        "        'columns': discounts_pl.columns,\n",
        "        'data_types': dict(zip(discounts_pl.columns, [str(dtype) for dtype in discounts_pl.dtypes])),\n",
        "        'null_counts': discounts_pl.null_count().to_pandas().iloc[0].to_dict(),\n",
        "        'memory_usage_mb': discounts_pl.estimated_size('mb')\n",
        "    }\n",
        "\n",
        "    # Numeric columns analysis for discounts\n",
        "    numeric_cols = discounts_pl.select(pl.col(pl.NUMERIC_DTYPES)).columns\n",
        "    if numeric_cols:\n",
        "        discounts_stats['numeric_summary'] = discounts_pl.select(numeric_cols).describe().to_pandas().to_dict()\n",
        "\n",
        "    stats_summary['discounts'] = discounts_stats\n",
        "\n",
        "    print(f\"   Records: {discounts_stats['total_records']:,}\")\n",
        "    print(f\"   Columns: {discounts_stats['total_columns']}\")\n",
        "    print(f\"   Memory: {discounts_stats['memory_usage_mb']:.2f} MB\")\n",
        "\n",
        "    # 2. PRODUCTS DATASET ANALYSIS\n",
        "    print(\"\\nPRODUCTS Dataset Analysis:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    products_stats = {\n",
        "        'total_records': products_pl.shape[0],\n",
        "        'total_columns': products_pl.shape[1],\n",
        "        'columns': products_pl.columns,\n",
        "        'data_types': dict(zip(products_pl.columns, [str(dtype) for dtype in products_pl.dtypes])),\n",
        "        'null_counts': products_pl.null_count().to_pandas().iloc[0].to_dict(),\n",
        "        'memory_usage_mb': products_pl.estimated_size('mb')\n",
        "    }\n",
        "\n",
        "    # Unique categories/brands analysis\n",
        "    if 'Category' in products_pl.columns:\n",
        "        products_stats['unique_categories'] = products_pl['Category'].n_unique()\n",
        "        products_stats['category_distribution'] = products_pl.group_by('Category').len().sort('len', descending=True).to_pandas().to_dict('records')\n",
        "\n",
        "    # Price analysis if available\n",
        "    price_cols = [col for col in products_pl.columns if 'price' in col.lower() or 'cost' in col.lower()]\n",
        "    if price_cols:\n",
        "        products_stats['price_summary'] = products_pl.select(price_cols).describe().to_pandas().to_dict()\n",
        "\n",
        "    stats_summary['products'] = products_stats\n",
        "\n",
        "    print(f\"   Records: {products_stats['total_records']:,}\")\n",
        "    print(f\"   Columns: {products_stats['total_columns']}\")\n",
        "    print(f\"   Memory: {products_stats['memory_usage_mb']:.2f} MB\")\n",
        "    if 'unique_categories' in products_stats:\n",
        "        print(f\"   Categories: {products_stats['unique_categories']}\")\n",
        "\n",
        "    # 3. EMPLOYEES DATASET ANALYSIS\n",
        "    print(\"\\nEMPLOYEES Dataset Analysis:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    employees_stats = {\n",
        "        'total_records': employees_pl.shape[0],\n",
        "        'total_columns': employees_pl.shape[1],\n",
        "        'columns': employees_pl.columns,\n",
        "        'data_types': dict(zip(employees_pl.columns, [str(dtype) for dtype in employees_pl.dtypes])),\n",
        "        'null_counts': employees_pl.null_count().to_pandas().iloc[0].to_dict(),\n",
        "        'memory_usage_mb': employees_pl.estimated_size('mb')\n",
        "    }\n",
        "\n",
        "    # Department/Role analysis if available\n",
        "    dept_cols = [col for col in employees_pl.columns if any(keyword in col.lower() for keyword in ['department', 'role', 'position', 'title'])]\n",
        "    for col in dept_cols:\n",
        "        employees_stats[f'{col}_distribution'] = employees_pl.group_by(col).len().sort('len', descending=True).to_pandas().to_dict('records')\n",
        "\n",
        "    stats_summary['employees'] = employees_stats\n",
        "\n",
        "    print(f\"   Records: {employees_stats['total_records']:,}\")\n",
        "    print(f\"   Columns: {employees_stats['total_columns']}\")\n",
        "    print(f\"   Memory: {employees_stats['memory_usage_mb']:.2f} MB\")\n",
        "\n",
        "    # 4. STORES DATASET ANALYSIS\n",
        "    print(\"\\nSTORES Dataset Analysis:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    stores_stats = {\n",
        "        'total_records': stores_pl.shape[0],\n",
        "        'total_columns': stores_pl.shape[1],\n",
        "        'columns': stores_pl.columns,\n",
        "        'data_types': dict(zip(stores_pl.columns, [str(dtype) for dtype in stores_pl.dtypes])),\n",
        "        'null_counts': stores_pl.null_count().to_pandas().iloc[0].to_dict(),\n",
        "        'memory_usage_mb': stores_pl.estimated_size('mb')\n",
        "    }\n",
        "\n",
        "    # Location analysis if available\n",
        "    location_cols = [col for col in stores_pl.columns if any(keyword in col.lower() for keyword in ['city', 'state', 'country', 'region'])]\n",
        "    for col in location_cols:\n",
        "        if col in stores_pl.columns:\n",
        "            stores_stats[f'{col}_distribution'] = stores_pl.group_by(col).len().sort('len', descending=True).to_pandas().to_dict('records')\n",
        "\n",
        "    stats_summary['stores'] = stores_stats\n",
        "\n",
        "    print(f\"   Records: {stores_stats['total_records']:,}\")\n",
        "    print(f\"   Columns: {stores_stats['total_columns']}\")\n",
        "    print(f\"   Memory: {stores_stats['memory_usage_mb']:.2f} MB\")\n",
        "\n",
        "    # 5. CUSTOMERS DATASET ANALYSIS\n",
        "    print(\"\\nCUSTOMERS Dataset Analysis:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    customers_stats = {\n",
        "        'total_records': customers_pl.shape[0],\n",
        "        'total_columns': customers_pl.shape[1],\n",
        "        'columns': customers_pl.columns,\n",
        "        'data_types': dict(zip(customers_pl.columns, [str(dtype) for dtype in customers_pl.dtypes])),\n",
        "        'null_counts': customers_pl.null_count().to_pandas().iloc[0].to_dict(),\n",
        "        'memory_usage_mb': customers_pl.estimated_size('mb')\n",
        "    }\n",
        "\n",
        "    # Demographics analysis\n",
        "    demo_cols = [col for col in customers_pl.columns if any(keyword in col.lower() for keyword in ['gender', 'age', 'city', 'state', 'country'])]\n",
        "    for col in demo_cols:\n",
        "        if col in customers_pl.columns:\n",
        "            customers_stats[f'{col}_distribution'] = customers_pl.group_by(col).len().sort('len', descending=True).head(10).to_pandas().to_dict('records')\n",
        "\n",
        "    stats_summary['customers'] = customers_stats\n",
        "\n",
        "    print(f\"   Records: {customers_stats['total_records']:,}\")\n",
        "    print(f\"   Columns: {customers_stats['total_columns']}\")\n",
        "    print(f\"   Memory: {customers_stats['memory_usage_mb']:.2f} MB\")\n",
        "\n",
        "    # 6. TRANSACTIONS DATASET ANALYSIS (Most Important)\n",
        "    print(\"\\nTRANSACTIONS Dataset Analysis:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    transactions_stats = {\n",
        "        'total_records': transactions_pl.shape[0],\n",
        "        'total_columns': transactions_pl.shape[1],\n",
        "        'columns': transactions_pl.columns,\n",
        "        'data_types': dict(zip(transactions_pl.columns, [str(dtype) for dtype in transactions_pl.dtypes])),\n",
        "        'null_counts': transactions_pl.null_count().to_pandas().iloc[0].to_dict(),\n",
        "        'memory_usage_mb': transactions_pl.estimated_size('mb')\n",
        "    }\n",
        "\n",
        "    # Financial metrics\n",
        "    financial_cols = [col for col in transactions_pl.columns if any(keyword in col.lower() for keyword in ['amount', 'total', 'price', 'cost', 'revenue', 'quantity'])]\n",
        "    if financial_cols:\n",
        "        transactions_stats['financial_summary'] = transactions_pl.select(financial_cols).describe().to_pandas().to_dict()\n",
        "\n",
        "    # Date range analysis\n",
        "    date_cols = [col for col in transactions_pl.columns if any(keyword in col.lower() for keyword in ['date', 'time'])]\n",
        "    for col in date_cols:\n",
        "        if col in transactions_pl.columns:\n",
        "            try:\n",
        "                date_stats = transactions_pl.select([\n",
        "                    pl.col(col).min().alias('min_date'),\n",
        "                    pl.col(col).max().alias('max_date'),\n",
        "                    pl.col(col).n_unique().alias('unique_dates')\n",
        "                ]).to_pandas().iloc[0].to_dict()\n",
        "                transactions_stats[f'{col}_range'] = date_stats\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    # Unique counts for key columns\n",
        "    key_cols = ['Customer ID', 'Product ID', 'Store ID', 'Employee ID']\n",
        "    for col in key_cols:\n",
        "        if col in transactions_pl.columns:\n",
        "            transactions_stats[f'unique_{col.lower().replace(\" \", \"_\")}'] = transactions_pl[col].n_unique()\n",
        "\n",
        "    stats_summary['transactions'] = transactions_stats\n",
        "\n",
        "    print(f\"   Records: {transactions_stats['total_records']:,}\")\n",
        "    print(f\"   Columns: {transactions_stats['total_columns']}\")\n",
        "    print(f\"   Memory: {transactions_stats['memory_usage_mb']:.2f} MB\")\n",
        "\n",
        "    # 7. OVERALL SUMMARY\n",
        "    print(\"\\nOVERALL DATA SUMMARY:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    total_records = sum([stats['total_records'] for stats in stats_summary.values()])\n",
        "    total_memory = sum([stats['memory_usage_mb'] for stats in stats_summary.values()])\n",
        "\n",
        "    overall_stats = {\n",
        "        'total_datasets': len(stats_summary),\n",
        "        'total_records_across_all': total_records,\n",
        "        'total_memory_usage_mb': total_memory,\n",
        "        'analysis_timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    stats_summary['overall'] = overall_stats\n",
        "\n",
        "    print(f\"   Total Datasets: {overall_stats['total_datasets']}\")\n",
        "    print(f\"   Total Records: {overall_stats['total_records_across_all']:,}\")\n",
        "    print(f\"   Total Memory: {overall_stats['total_memory_usage_mb']:.2f} MB\")\n",
        "\n",
        "    # 8. SAVE RESULTS\n",
        "    print(f\"\\nSaving descriptive statistics...\")\n",
        "\n",
        "    # Save as JSON\n",
        "    import json\n",
        "    with open(f\"{stats_dir}/descriptive_statistics_summary.json\", 'w') as f:\n",
        "        json.dump(stats_summary, f, indent=2, default=str)\n",
        "\n",
        "    # Save detailed CSV reports for each dataset\n",
        "    for dataset_name, stats in stats_summary.items():\n",
        "        if dataset_name != 'overall':\n",
        "            stats_df = pd.DataFrame([stats])\n",
        "            stats_df.to_csv(f\"{stats_dir}/{dataset_name}_statistics.csv\", index=False)\n",
        "\n",
        "    print(f\"Descriptive statistics completed!\")\n",
        "    print(f\"Results saved in: {stats_dir}/\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return stats_summary\n",
        "\n",
        "# Usage in your main function:\n",
        "def load_data_and_analyze():\n",
        "    \"\"\"Load data and perform descriptive statistics\"\"\"\n",
        "\n",
        "    # Load data (your existing function)\n",
        "    discounts_pl, products_pl, employees_pl, stores_pl, customers_pl, transactions_pl = load_data_efficiently()\n",
        "\n",
        "    # Perform descriptive statistics\n",
        "    stats_summary = perform_descriptive_statistics(\n",
        "        discounts_pl, products_pl, employees_pl, stores_pl, customers_pl, transactions_pl\n",
        "    )\n",
        "\n",
        "    return discounts_pl, products_pl, employees_pl, stores_pl, customers_pl, transactions_pl, stats_summary\n",
        "\n",
        "load_data_and_analyze()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHt4__siB1dB",
        "outputId": "62fa5729-35c9-4c5b-928e-0e12470f1f80"
      },
      "id": "eHt4__siB1dB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loading data efficiently...\n",
            "Loading discounts...\n",
            "Loading products...\n",
            "Loading employees...\n",
            "Loading stores...\n",
            "Loading customers...\n",
            "Loading transactions (this may take a moment)...\n",
            "Intial Transactions loaded: 6416827 rows\n",
            "Final Transactions loaded: 6416029 rows\n",
            "Converting to Polars for better performance...\n",
            " Data loaded successfully!\n",
            "Transactions shape: (6416029, 19)\n",
            "\n",
            "============================================================\n",
            "PERFORMING DESCRIPTIVE STATISTICS ANALYSIS\n",
            "============================================================\n",
            "\n",
            "DISCOUNTS Dataset Analysis:\n",
            "----------------------------------------\n",
            "   Records: 181\n",
            "   Columns: 6\n",
            "   Memory: 0.02 MB\n",
            "\n",
            "PRODUCTS Dataset Analysis:\n",
            "----------------------------------------\n",
            "   Records: 17,940\n",
            "   Columns: 12\n",
            "   Memory: 4.67 MB\n",
            "   Categories: 3\n",
            "\n",
            "EMPLOYEES Dataset Analysis:\n",
            "----------------------------------------\n",
            "   Records: 404\n",
            "   Columns: 4\n",
            "   Memory: 0.02 MB\n",
            "\n",
            "STORES Dataset Analysis:\n",
            "----------------------------------------\n",
            "   Records: 35\n",
            "   Columns: 8\n",
            "   Memory: 0.00 MB\n",
            "\n",
            "CUSTOMERS Dataset Analysis:\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3133635659.py:32: DeprecationWarning: `NUMERIC_DTYPES` is deprecated. Define your own data type groups or use the `polars.selectors` module for selecting columns of a certain data type.\n",
            "  numeric_cols = discounts_pl.select(pl.col(pl.NUMERIC_DTYPES)).columns\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Records: 1,643,306\n",
            "   Columns: 9\n",
            "   Memory: 169.71 MB\n",
            "\n",
            "TRANSACTIONS Dataset Analysis:\n",
            "----------------------------------------\n",
            "   Records: 6,416,029\n",
            "   Columns: 19\n",
            "   Memory: 936.52 MB\n",
            "\n",
            "OVERALL DATA SUMMARY:\n",
            "----------------------------------------\n",
            "   Total Datasets: 6\n",
            "   Total Records: 8,077,895\n",
            "   Total Memory: 1110.94 MB\n",
            "\n",
            "Saving descriptive statistics...\n",
            "Descriptive statistics completed!\n",
            "Results saved in: descriptive_statistics/\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(shape: (181, 6)\n",
              " ┌────────────┬────────────┬─────────┬─────────────────────┬───────────┬────────────────────────────┐\n",
              " │ Start      ┆ End        ┆ Discont ┆ Description         ┆ Category  ┆ Sub Category               │\n",
              " │ ---        ┆ ---        ┆ ---     ┆ ---                 ┆ ---       ┆ ---                        │\n",
              " │ str        ┆ str        ┆ f64     ┆ str                 ┆ str       ┆ str                        │\n",
              " ╞════════════╪════════════╪═════════╪═════════════════════╪═══════════╪════════════════════════════╡\n",
              " │ 2020-01-01 ┆ 2020-01-10 ┆ 0.4     ┆ 40% discount during ┆ Feminine  ┆ Coats and Blazers          │\n",
              " │            ┆            ┆         ┆ our New Ye…         ┆           ┆                            │\n",
              " │ 2020-01-01 ┆ 2020-01-10 ┆ 0.4     ┆ 40% discount during ┆ Feminine  ┆ Sweaters and Knitwear      │\n",
              " │            ┆            ┆         ┆ our New Ye…         ┆           ┆                            │\n",
              " │ 2020-01-01 ┆ 2020-01-10 ┆ 0.4     ┆ 40% discount during ┆ Masculine ┆ Coats and Blazers          │\n",
              " │            ┆            ┆         ┆ our New Ye…         ┆           ┆                            │\n",
              " │ 2020-01-01 ┆ 2020-01-10 ┆ 0.4     ┆ 40% discount during ┆ Masculine ┆ Sweaters and Sweatshirts   │\n",
              " │            ┆            ┆         ┆ our New Ye…         ┆           ┆                            │\n",
              " │ 2020-01-01 ┆ 2020-01-10 ┆ 0.4     ┆ 40% discount during ┆ Children  ┆ Coats                      │\n",
              " │            ┆            ┆         ┆ our New Ye…         ┆           ┆                            │\n",
              " │ …          ┆ …          ┆ …       ┆ …                   ┆ …         ┆ …                          │\n",
              " │ 2025-03-15 ┆ 2025-03-31 ┆ 0.35    ┆ 35% discount during ┆ Feminine  ┆ Dresses and Jumpsuits      │\n",
              " │            ┆            ┆         ┆ our Early …         ┆           ┆                            │\n",
              " │ 2025-03-15 ┆ 2025-03-31 ┆ 0.35    ┆ 35% discount during ┆ Feminine  ┆ Shirts and Blouses         │\n",
              " │            ┆            ┆         ┆ our Early …         ┆           ┆                            │\n",
              " │ 2025-03-15 ┆ 2025-03-31 ┆ 0.35    ┆ 35% discount during ┆ Masculine ┆ T-shirts and Polos         │\n",
              " │            ┆            ┆         ┆ our Early …         ┆           ┆                            │\n",
              " │ 2025-03-15 ┆ 2025-03-31 ┆ 0.35    ┆ 35% discount during ┆ Masculine ┆ Shirts                     │\n",
              " │            ┆            ┆         ┆ our Early …         ┆           ┆                            │\n",
              " │ 2025-03-15 ┆ 2025-03-31 ┆ 0.35    ┆ 35% discount during ┆ Children  ┆ Girl and Boy (1-5 years,   │\n",
              " │            ┆            ┆         ┆ our Early …         ┆           ┆ 6-14 …                     │\n",
              " └────────────┴────────────┴─────────┴─────────────────────┴───────────┴────────────────────────────┘,\n",
              " shape: (17_940, 12)\n",
              " ┌────────────┬──────────┬───────────┬───────────┬───┬───────────┬───────────┬──────────┬───────────┐\n",
              " │ Product ID ┆ Category ┆ Sub       ┆ Descripti ┆ … ┆ Descripti ┆ Color     ┆ Sizes    ┆ Productio │\n",
              " │ ---        ┆ ---      ┆ Category  ┆ on PT     ┆   ┆ on ZH     ┆ ---       ┆ ---      ┆ n Cost    │\n",
              " │ i64        ┆ str      ┆ ---       ┆ ---       ┆   ┆ ---       ┆ str       ┆ str      ┆ ---       │\n",
              " │            ┆          ┆ str       ┆ str       ┆   ┆ str       ┆           ┆          ┆ f64       │\n",
              " ╞════════════╪══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪══════════╪═══════════╡\n",
              " │ 1          ┆ Feminine ┆ Coats and ┆ Esportivo ┆ … ┆ 运动天鹅  ┆ null      ┆ S|M|L|XL ┆ 10.73     │\n",
              " │            ┆          ┆ Blazers   ┆ Veludo    ┆   ┆ 绒运动与  ┆           ┆          ┆           │\n",
              " │            ┆          ┆           ┆ Verde Com ┆   ┆ 按钮      ┆           ┆          ┆           │\n",
              " │            ┆          ┆           ┆ Bot…      ┆   ┆           ┆           ┆          ┆           │\n",
              " │ 2          ┆ Feminine ┆ Sweaters  ┆ Luxuoso   ┆ … ┆ 豪华的粉  ┆ PINK      ┆ S|M|L|XL ┆ 19.55     │\n",
              " │            ┆          ┆ and       ┆ Denim     ┆   ┆ 红色牛仔  ┆           ┆          ┆           │\n",
              " │            ┆          ┆ Knitwear  ┆ Rosa Com  ┆   ┆ 布和纽扣  ┆           ┆          ┆           │\n",
              " │            ┆          ┆           ┆ Botões    ┆   ┆           ┆           ┆          ┆           │\n",
              " │ 3          ┆ Feminine ┆ Dresses   ┆ Retrô     ┆ … ┆ 黑色三角  ┆ BLACK     ┆ S|M|L|XL ┆ 25.59     │\n",
              " │            ┆          ┆ and       ┆ Tricot    ┆   ┆ 形印刷三  ┆           ┆          ┆           │\n",
              " │            ┆          ┆ Jumpsuits ┆ Preto     ┆   ┆ 角形      ┆           ┆          ┆           │\n",
              " │            ┆          ┆           ┆ Estampado ┆   ┆           ┆           ┆          ┆           │\n",
              " │ 4          ┆ Feminine ┆ Shirts    ┆ Blusa De  ┆ … ┆ 基本的棉  ┆ null      ┆ S|M|L|XL ┆ 27.62     │\n",
              " │            ┆          ┆ and       ┆ Algodão   ┆   ┆ 衬衫      ┆           ┆          ┆           │\n",
              " │            ┆          ┆ Blouses   ┆ Básica    ┆   ┆           ┆           ┆          ┆           │\n",
              " │ 5          ┆ Feminine ┆ T-shirts  ┆ T-Shirt   ┆ … ┆ 基本棉T恤 ┆ null      ┆ S|M|L    ┆ 11.69     │\n",
              " │            ┆          ┆ and Tops  ┆ Básica De ┆   ┆           ┆           ┆          ┆           │\n",
              " │            ┆          ┆           ┆ Algodão   ┆   ┆           ┆           ┆          ┆           │\n",
              " │ …          ┆ …        ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …        ┆ …         │\n",
              " │ 17936      ┆ Children ┆ Girl and  ┆ Executivo ┆ … ┆ 行政绿革  ┆ GREEN     ┆ P|M|G|GG ┆ 7.69      │\n",
              " │            ┆          ┆ Boy (1-5  ┆ Camurça   ┆   ┆ 与拉链    ┆           ┆          ┆           │\n",
              " │            ┆          ┆ years,    ┆ Verde Com ┆   ┆           ┆           ┆          ┆           │\n",
              " │            ┆          ┆ 6-14 …    ┆ Zí…       ┆   ┆           ┆           ┆          ┆           │\n",
              " │ 17937      ┆ Children ┆ Coats     ┆ Luxuoso   ┆ … ┆ 豪华的绿  ┆ TURQUOISE ┆ P|M|G    ┆ 11.65     │\n",
              " │            ┆          ┆           ┆ Lã        ┆   ┆ 松石羊毛  ┆           ┆          ┆           │\n",
              " │            ┆          ┆           ┆ Turquesa  ┆   ┆ 和引擎盖  ┆           ┆          ┆           │\n",
              " │            ┆          ┆           ┆ Com Capuz ┆   ┆           ┆           ┆          ┆           │\n",
              " │ 17938      ┆ Children ┆ Sweaters  ┆ Camisola  ┆ … ┆ 带几何印  ┆ null      ┆ P|M|G    ┆ 24.38     │\n",
              " │            ┆          ┆           ┆ Infantil  ┆   ┆ 刷的Kidsk ┆           ┆          ┆           │\n",
              " │            ┆          ┆           ┆ De Tricô  ┆   ┆ ot衬衫    ┆           ┆          ┆           │\n",
              " │            ┆          ┆           ┆ Com…      ┆   ┆           ┆           ┆          ┆           │\n",
              " │ 17939      ┆ Children ┆ Pajamas   ┆ Pijama    ┆ … ┆ 孩子们缎  ┆ null      ┆ P|M|G    ┆ 18.27     │\n",
              " │            ┆          ┆           ┆ Infantil  ┆   ┆ 面睡衣，  ┆           ┆          ┆           │\n",
              " │            ┆          ┆           ┆ De Cetim  ┆   ┆ 光滑的光  ┆           ┆          ┆           │\n",
              " │            ┆          ┆           ┆ Com B…    ┆   ┆ 泽和口袋  ┆           ┆          ┆           │\n",
              " │ 17940      ┆ Children ┆ Accessori ┆ Protetor  ┆ … ┆ 为了安全  ┆ null      ┆ null     ┆ 10.99     │\n",
              " │            ┆          ┆ es        ┆ De Braço  ┆   ┆ 起见儿童  ┆           ┆          ┆           │\n",
              " │            ┆          ┆           ┆ Infantil  ┆   ┆ 手臂保护  ┆           ┆          ┆           │\n",
              " │            ┆          ┆           ┆ Par…      ┆   ┆ 器        ┆           ┆          ┆           │\n",
              " └────────────┴──────────┴───────────┴───────────┴───┴───────────┴───────────┴──────────┴───────────┘,\n",
              " shape: (404, 4)\n",
              " ┌─────────────┬──────────┬────────────────────┬───────────────────┐\n",
              " │ Employee ID ┆ Store ID ┆ Name               ┆ Position          │\n",
              " │ ---         ┆ ---      ┆ ---                ┆ ---               │\n",
              " │ i64         ┆ i64      ┆ str                ┆ str               │\n",
              " ╞═════════════╪══════════╪════════════════════╪═══════════════════╡\n",
              " │ 1           ┆ 1        ┆ Stephen Johnson    ┆ Store Manager     │\n",
              " │ 2           ┆ 1        ┆ Rebecca Myers      ┆ Assistant Manager │\n",
              " │ 3           ┆ 1        ┆ Katherine Buchanan ┆ Cashier           │\n",
              " │ 4           ┆ 1        ┆ Jessica Hicks      ┆ Stock Clerk       │\n",
              " │ 5           ┆ 1        ┆ Ryan Gross         ┆ Sales Associate   │\n",
              " │ …           ┆ …        ┆ …                  ┆ …                 │\n",
              " │ 400         ┆ 35       ┆ Henrique Amaral    ┆ Sales Associate   │\n",
              " │ 401         ┆ 35       ┆ Brian Rocha        ┆ Sales Associate   │\n",
              " │ 402         ┆ 35       ┆ Matilde Campos     ┆ Sales Associate   │\n",
              " │ 403         ┆ 35       ┆ Emanuel Marques    ┆ Sales Associate   │\n",
              " │ 404         ┆ 35       ┆ Violeta Gonçalves  ┆ Sales Associate   │\n",
              " └─────────────┴──────────┴────────────────────┴───────────────────┘,\n",
              " shape: (35, 8)\n",
              " ┌──────────┬─────────────┬─────────────┬────────────┬────────────┬──────────┬──────────┬───────────┐\n",
              " │ Store ID ┆ Country     ┆ City        ┆ Store Name ┆ Number of  ┆ ZIP Code ┆ Latitude ┆ Longitude │\n",
              " │ ---      ┆ ---         ┆ ---         ┆ ---        ┆ Employees  ┆ ---      ┆ ---      ┆ ---       │\n",
              " │ i64      ┆ str         ┆ str         ┆ str        ┆ ---        ┆ str      ┆ f64      ┆ f64       │\n",
              " │          ┆             ┆             ┆            ┆ i64        ┆          ┆          ┆           │\n",
              " ╞══════════╪═════════════╪═════════════╪════════════╪════════════╪══════════╪══════════╪═══════════╡\n",
              " │ 1        ┆ United      ┆ New York    ┆ Store New  ┆ 10         ┆ 10001    ┆ 40.7128  ┆ -74.006   │\n",
              " │          ┆ States      ┆             ┆ York       ┆            ┆          ┆          ┆           │\n",
              " │ 2        ┆ United      ┆ Los Angeles ┆ Store Los  ┆ 8          ┆ 90001    ┆ 34.0522  ┆ -118.2437 │\n",
              " │          ┆ States      ┆             ┆ Angeles    ┆            ┆          ┆          ┆           │\n",
              " │ 3        ┆ United      ┆ Chicago     ┆ Store      ┆ 9          ┆ 60601    ┆ 41.8781  ┆ -87.6298  │\n",
              " │          ┆ States      ┆             ┆ Chicago    ┆            ┆          ┆          ┆           │\n",
              " │ 4        ┆ United      ┆ Houston     ┆ Store      ┆ 10         ┆ 77001    ┆ 29.7604  ┆ -95.3698  │\n",
              " │          ┆ States      ┆             ┆ Houston    ┆            ┆          ┆          ┆           │\n",
              " │ 5        ┆ United      ┆ Phoenix     ┆ Store      ┆ 9          ┆ 85001    ┆ 33.4484  ┆ -112.074  │\n",
              " │          ┆ States      ┆             ┆ Phoenix    ┆            ┆          ┆          ┆           │\n",
              " │ …        ┆ …           ┆ …           ┆ …          ┆ …          ┆ …        ┆ …        ┆ …         │\n",
              " │ 31       ┆ Portugal    ┆ Lisboa      ┆ Store      ┆ 10         ┆ 1000-001 ┆ 38.7167  ┆ -9.1333   │\n",
              " │          ┆             ┆             ┆ Lisboa     ┆            ┆          ┆          ┆           │\n",
              " │ 32       ┆ Portugal    ┆ Porto       ┆ Store      ┆ 7          ┆ 4000-001 ┆ 41.1496  ┆ -8.611    │\n",
              " │          ┆             ┆             ┆ Porto      ┆            ┆          ┆          ┆           │\n",
              " │ 33       ┆ Portugal    ┆ Braga       ┆ Store      ┆ 9          ┆ 4700-001 ┆ 41.5503  ┆ -8.4201   │\n",
              " │          ┆             ┆             ┆ Braga      ┆            ┆          ┆          ┆           │\n",
              " │ 34       ┆ Portugal    ┆ Guimarães   ┆ Store      ┆ 9          ┆ 4800-001 ┆ 41.4444  ┆ -8.2962   │\n",
              " │          ┆             ┆             ┆ Guimarães  ┆            ┆          ┆          ┆           │\n",
              " │ 35       ┆ Portugal    ┆ Coimbra     ┆ Store      ┆ 7          ┆ 3000-001 ┆ 40.2056  ┆ -8.4196   │\n",
              " │          ┆             ┆             ┆ Coimbra    ┆            ┆          ┆          ┆           │\n",
              " └──────────┴─────────────┴─────────────┴────────────┴────────────┴──────────┴──────────┴───────────┘,\n",
              " shape: (1_643_306, 9)\n",
              " ┌────────────┬────────────┬───────────┬───────────┬───┬───────────┬────────┬───────────┬───────────┐\n",
              " │ Customer   ┆ Name       ┆ Email     ┆ Telephone ┆ … ┆ Country   ┆ Gender ┆ Date Of   ┆ Job Title │\n",
              " │ ID         ┆ ---        ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---    ┆ Birth     ┆ ---       │\n",
              " │ ---        ┆ str        ┆ str       ┆ str       ┆   ┆ str       ┆ str    ┆ ---       ┆ str       │\n",
              " │ i64        ┆            ┆           ┆           ┆   ┆           ┆        ┆ str       ┆           │\n",
              " ╞════════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪════════╪═══════════╪═══════════╡\n",
              " │ 1          ┆ Tyler      ┆ tyler.gar ┆ 922.970.2 ┆ … ┆ United    ┆ M      ┆ 2003-07-1 ┆ null      │\n",
              " │            ┆ Garcia     ┆ cia@fake_ ┆ 265x47563 ┆   ┆ States    ┆        ┆ 5         ┆           │\n",
              " │            ┆            ┆ gmail.com ┆           ┆   ┆           ┆        ┆           ┆           │\n",
              " │ 2          ┆ Joshua     ┆ joshua.mi ┆ +1-958-72 ┆ … ┆ United    ┆ M      ┆ 2000-06-1 ┆ Records   │\n",
              " │            ┆ Miller     ┆ ller@fake ┆ 9-6169    ┆   ┆ States    ┆        ┆ 6         ┆ manager   │\n",
              " │            ┆            ┆ _gmail.co ┆           ┆   ┆           ┆        ┆           ┆           │\n",
              " │            ┆            ┆ m         ┆           ┆   ┆           ┆        ┆           ┆           │\n",
              " │ 3          ┆ Alison     ┆ alison.ma ┆ +1-645-56 ┆ … ┆ United    ┆ F      ┆ 2003-07-2 ┆ null      │\n",
              " │            ┆ Marshall   ┆ rshall.dd ┆ 7-0876x54 ┆   ┆ States    ┆        ┆ 2         ┆           │\n",
              " │            ┆ DDS        ┆ s@fake_ho ┆ 09        ┆   ┆           ┆        ┆           ┆           │\n",
              " │            ┆            ┆ tma…      ┆           ┆   ┆           ┆        ┆           ┆           │\n",
              " │ 4          ┆ Jeffery    ┆ jeffery.a ┆ 212.336.0 ┆ … ┆ United    ┆ M      ┆ 1996-11-1 ┆ Proofread │\n",
              " │            ┆ Acosta     ┆ costa@fak ┆ 912x84994 ┆   ┆ States    ┆        ┆ 2         ┆ er        │\n",
              " │            ┆            ┆ e_yahoo.c ┆           ┆   ┆           ┆        ┆           ┆           │\n",
              " │            ┆            ┆ om        ┆           ┆   ┆           ┆        ┆           ┆           │\n",
              " │ 5          ┆ Ashley     ┆ ashley.sa ┆ 781453578 ┆ … ┆ United    ┆ F      ┆ 1998-02-1 ┆ Exercise  │\n",
              " │            ┆ Sanders    ┆ nders@fak ┆ 1         ┆   ┆ States    ┆        ┆ 0         ┆ physiolog │\n",
              " │            ┆            ┆ e_hotmail ┆           ┆   ┆           ┆        ┆           ┆ ist       │\n",
              " │            ┆            ┆ .co…      ┆           ┆   ┆           ┆        ┆           ┆           │\n",
              " │ …          ┆ …          ┆ …         ┆ …         ┆ … ┆ …         ┆ …      ┆ …         ┆ …         │\n",
              " │ 1643302    ┆ Bernardo   ┆ bernardo. ┆ (351)     ┆ … ┆ Portugal  ┆ M      ┆ 1973-02-2 ┆ Structura │\n",
              " │            ┆ Vicente    ┆ vicente@f ┆ 962314916 ┆   ┆           ┆        ┆ 7         ┆ l         │\n",
              " │            ┆            ┆ ake_hotma ┆           ┆   ┆           ┆        ┆           ┆ engineer  │\n",
              " │            ┆            ┆ il.…      ┆           ┆   ┆           ┆        ┆           ┆           │\n",
              " │ 1643303    ┆ Luana      ┆ luana.lei ┆ (351) 288 ┆ … ┆ Portugal  ┆ F      ┆ 1997-02-0 ┆ Futures   │\n",
              " │            ┆ Leite      ┆ te@fake_c ┆ 728 807   ┆   ┆           ┆        ┆ 5         ┆ trader    │\n",
              " │            ┆            ┆ lix.pt    ┆           ┆   ┆           ┆        ┆           ┆           │\n",
              " │ 1643304    ┆ Ema        ┆ ema.freit ┆ +35191099 ┆ … ┆ Portugal  ┆ F      ┆ 2005-03-1 ┆ null      │\n",
              " │            ┆ Freitas    ┆ as@fake_c ┆ 0620      ┆   ┆           ┆        ┆ 4         ┆           │\n",
              " │            ┆            ┆ lix.pt    ┆           ┆   ┆           ┆        ┆           ┆           │\n",
              " │ 1643305    ┆ Rafaela    ┆ rafaela.c ┆ +35193818 ┆ … ┆ Portugal  ┆ F      ┆ 1989-07-1 ┆ Futures   │\n",
              " │            ┆ Carneiro   ┆ arneiro@f ┆ 2129      ┆   ┆           ┆        ┆ 4         ┆ trader    │\n",
              " │            ┆            ┆ ake_clix. ┆           ┆   ┆           ┆        ┆           ┆           │\n",
              " │            ┆            ┆ pt        ┆           ┆   ┆           ┆        ┆           ┆           │\n",
              " │ 1643306    ┆ Pilar da   ┆ pilar.da. ┆ (351) 938 ┆ … ┆ Portugal  ┆ F      ┆ 1978-12-2 ┆ Garment/t │\n",
              " │            ┆ Coelho     ┆ coelho@fa ┆ 886 805   ┆   ┆           ┆        ┆ 2         ┆ extile    │\n",
              " │            ┆            ┆ ke_hotmai ┆           ┆   ┆           ┆        ┆           ┆ technolog │\n",
              " │            ┆            ┆ l.c…      ┆           ┆   ┆           ┆        ┆           ┆ ist       │\n",
              " └────────────┴────────────┴───────────┴───────────┴───┴───────────┴────────┴───────────┴───────────┘,\n",
              " shape: (6_416_029, 19)\n",
              " ┌────────────┬──────┬────────────┬────────────┬───┬────────────┬───────────┬───────────┬───────────┐\n",
              " │ Invoice ID ┆ Line ┆ Customer   ┆ Product ID ┆ … ┆ SKU        ┆ Transacti ┆ Payment   ┆ Invoice   │\n",
              " │ ---        ┆ ---  ┆ ID         ┆ ---        ┆   ┆ ---        ┆ on Type   ┆ Method    ┆ Total     │\n",
              " │ str        ┆ i64  ┆ ---        ┆ i64        ┆   ┆ str        ┆ ---       ┆ ---       ┆ ---       │\n",
              " │            ┆      ┆ i64        ┆            ┆   ┆            ┆ str       ┆ str       ┆ f64       │\n",
              " ╞════════════╪══════╪════════════╪════════════╪═══╪════════════╪═══════════╪═══════════╪═══════════╡\n",
              " │ INV-US-001 ┆ 1    ┆ 47162      ┆ 485        ┆ … ┆ MASU485-M- ┆ Sale      ┆ Cash      ┆ 126.7     │\n",
              " │ -03558761  ┆      ┆            ┆            ┆   ┆            ┆           ┆           ┆           │\n",
              " │ INV-US-001 ┆ 2    ┆ 47162      ┆ 2779       ┆ … ┆ CHCO2779-G ┆ Sale      ┆ Cash      ┆ 126.7     │\n",
              " │ -03558761  ┆      ┆            ┆            ┆   ┆ -          ┆           ┆           ┆           │\n",
              " │ INV-US-001 ┆ 3    ┆ 47162      ┆ 64         ┆ … ┆ MACO64-M-N ┆ Sale      ┆ Cash      ┆ 126.7     │\n",
              " │ -03558761  ┆      ┆            ┆            ┆   ┆ EUTRAL     ┆           ┆           ┆           │\n",
              " │ INV-US-001 ┆ 1    ┆ 10142      ┆ 131        ┆ … ┆ FECO131-M- ┆ Sale      ┆ Cash      ┆ 77.0      │\n",
              " │ -03558762  ┆      ┆            ┆            ┆   ┆ BLUE       ┆           ┆           ┆           │\n",
              " │ INV-US-001 ┆ 2    ┆ 10142      ┆ 716        ┆ … ┆ MAT-716-L- ┆ Sale      ┆ Cash      ┆ 77.0      │\n",
              " │ -03558762  ┆      ┆            ┆            ┆   ┆ WHITE      ┆           ┆           ┆           │\n",
              " │ …          ┆ …    ┆ …          ┆ …          ┆ … ┆ …          ┆ …         ┆ …         ┆ …         │\n",
              " │ INV-PT-035 ┆ 2    ┆ 1640168    ┆ 15414      ┆ … ┆ CHGI15414- ┆ Sale      ┆ Credit    ┆ 69.55     │\n",
              " │ -01497756  ┆      ┆            ┆            ┆   ┆ P-         ┆           ┆ Card      ┆           │\n",
              " │ INV-PT-035 ┆ 3    ┆ 1640168    ┆ 15232      ┆ … ┆ CHGI15232- ┆ Sale      ┆ Credit    ┆ 69.55     │\n",
              " │ -01497756  ┆      ┆            ┆            ┆   ┆ G-RED      ┆           ┆ Card      ┆           │\n",
              " │ INV-PT-035 ┆ 1    ┆ 1636770    ┆ 15401      ┆ … ┆ FESP15401- ┆ Sale      ┆ Credit    ┆ 40.0      │\n",
              " │ -01497757  ┆      ┆            ┆            ┆   ┆ XL-        ┆           ┆ Card      ┆           │\n",
              " │ INV-PT-035 ┆ 1    ┆ 1642472    ┆ 15671      ┆ … ┆ MAUN15671- ┆ Sale      ┆ Credit    ┆ 36.5      │\n",
              " │ -01497758  ┆      ┆            ┆            ┆   ┆ M-         ┆           ┆ Card      ┆           │\n",
              " │ INV-PT-035 ┆ 1    ┆ 1639026    ┆ 17424      ┆ … ┆ FESH17424- ┆ Sale      ┆ Credit    ┆ 16.57     │\n",
              " │ -01497759  ┆      ┆            ┆            ┆   ┆ M-         ┆           ┆ Card      ┆           │\n",
              " └────────────┴──────┴────────────┴────────────┴───┴────────────┴───────────┴───────────┴───────────┘,\n",
              " {'discounts': {'total_records': 181,\n",
              "   'total_columns': 6,\n",
              "   'columns': ['Start',\n",
              "    'End',\n",
              "    'Discont',\n",
              "    'Description',\n",
              "    'Category',\n",
              "    'Sub Category'],\n",
              "   'data_types': {'Start': 'String',\n",
              "    'End': 'String',\n",
              "    'Discont': 'Float64',\n",
              "    'Description': 'String',\n",
              "    'Category': 'String',\n",
              "    'Sub Category': 'String'},\n",
              "   'null_counts': {'Start': 0,\n",
              "    'End': 0,\n",
              "    'Discont': 0,\n",
              "    'Description': 0,\n",
              "    'Category': 10,\n",
              "    'Sub Category': 10},\n",
              "   'memory_usage_mb': 0.017075538635253906,\n",
              "   'numeric_summary': {'statistic': {0: 'count',\n",
              "     1: 'null_count',\n",
              "     2: 'mean',\n",
              "     3: 'std',\n",
              "     4: 'min',\n",
              "     5: '25%',\n",
              "     6: '50%',\n",
              "     7: '75%',\n",
              "     8: 'max'},\n",
              "    'Discont': {0: 181.0,\n",
              "     1: 0.0,\n",
              "     2: 0.3433701657458564,\n",
              "     3: 0.1036029868428037,\n",
              "     4: 0.2,\n",
              "     5: 0.25,\n",
              "     6: 0.35,\n",
              "     7: 0.4,\n",
              "     8: 0.6}}},\n",
              "  'products': {'total_records': 17940,\n",
              "   'total_columns': 12,\n",
              "   'columns': ['Product ID',\n",
              "    'Category',\n",
              "    'Sub Category',\n",
              "    'Description PT',\n",
              "    'Description DE',\n",
              "    'Description FR',\n",
              "    'Description ES',\n",
              "    'Description EN',\n",
              "    'Description ZH',\n",
              "    'Color',\n",
              "    'Sizes',\n",
              "    'Production Cost'],\n",
              "   'data_types': {'Product ID': 'Int64',\n",
              "    'Category': 'String',\n",
              "    'Sub Category': 'String',\n",
              "    'Description PT': 'String',\n",
              "    'Description DE': 'String',\n",
              "    'Description FR': 'String',\n",
              "    'Description ES': 'String',\n",
              "    'Description EN': 'String',\n",
              "    'Description ZH': 'String',\n",
              "    'Color': 'String',\n",
              "    'Sizes': 'String',\n",
              "    'Production Cost': 'Float64'},\n",
              "   'null_counts': {'Product ID': 0,\n",
              "    'Category': 0,\n",
              "    'Sub Category': 0,\n",
              "    'Description PT': 0,\n",
              "    'Description DE': 0,\n",
              "    'Description FR': 0,\n",
              "    'Description ES': 0,\n",
              "    'Description EN': 0,\n",
              "    'Description ZH': 0,\n",
              "    'Color': 12445,\n",
              "    'Sizes': 2070,\n",
              "    'Production Cost': 0},\n",
              "   'memory_usage_mb': 4.672783851623535,\n",
              "   'unique_categories': 3,\n",
              "   'category_distribution': [{'Category': 'Feminine', 'len': 7590},\n",
              "    {'Category': 'Masculine', 'len': 6210},\n",
              "    {'Category': 'Children', 'len': 4140}],\n",
              "   'price_summary': {'statistic': {0: 'count',\n",
              "     1: 'null_count',\n",
              "     2: 'mean',\n",
              "     3: 'std',\n",
              "     4: 'min',\n",
              "     5: '25%',\n",
              "     6: '50%',\n",
              "     7: '75%',\n",
              "     8: 'max'},\n",
              "    'Production Cost': {0: 17940.0,\n",
              "     1: 0.0,\n",
              "     2: 16.096188963210704,\n",
              "     3: 11.628072215331974,\n",
              "     4: 0.56,\n",
              "     5: 7.8,\n",
              "     6: 13.14,\n",
              "     7: 20.97,\n",
              "     8: 77.19}}},\n",
              "  'employees': {'total_records': 404,\n",
              "   'total_columns': 4,\n",
              "   'columns': ['Employee ID', 'Store ID', 'Name', 'Position'],\n",
              "   'data_types': {'Employee ID': 'Int64',\n",
              "    'Store ID': 'Int64',\n",
              "    'Name': 'String',\n",
              "    'Position': 'String'},\n",
              "   'null_counts': {'Employee ID': 0, 'Store ID': 0, 'Name': 0, 'Position': 0},\n",
              "   'memory_usage_mb': 0.017333984375,\n",
              "   'Position_distribution': [{'Position': 'Sales Associate', 'len': 264},\n",
              "    {'Position': 'Assistant Manager', 'len': 35},\n",
              "    {'Position': 'Store Manager', 'len': 35},\n",
              "    {'Position': 'Cashier', 'len': 35},\n",
              "    {'Position': 'Stock Clerk', 'len': 35}]},\n",
              "  'stores': {'total_records': 35,\n",
              "   'total_columns': 8,\n",
              "   'columns': ['Store ID',\n",
              "    'Country',\n",
              "    'City',\n",
              "    'Store Name',\n",
              "    'Number of Employees',\n",
              "    'ZIP Code',\n",
              "    'Latitude',\n",
              "    'Longitude'],\n",
              "   'data_types': {'Store ID': 'Int64',\n",
              "    'Country': 'String',\n",
              "    'City': 'String',\n",
              "    'Store Name': 'String',\n",
              "    'Number of Employees': 'Int64',\n",
              "    'ZIP Code': 'String',\n",
              "    'Latitude': 'Float64',\n",
              "    'Longitude': 'Float64'},\n",
              "   'null_counts': {'Store ID': 0,\n",
              "    'Country': 0,\n",
              "    'City': 0,\n",
              "    'Store Name': 0,\n",
              "    'Number of Employees': 0,\n",
              "    'ZIP Code': 0,\n",
              "    'Latitude': 0,\n",
              "    'Longitude': 0},\n",
              "   'memory_usage_mb': 0.0022535324096679688,\n",
              "   'Country_distribution': [{'Country': '中国', 'len': 5},\n",
              "    {'Country': 'Portugal', 'len': 5},\n",
              "    {'Country': 'España', 'len': 5},\n",
              "    {'Country': 'United Kingdom', 'len': 5},\n",
              "    {'Country': 'United States', 'len': 5},\n",
              "    {'Country': 'Deutschland', 'len': 5},\n",
              "    {'Country': 'France', 'len': 5}],\n",
              "   'City_distribution': [{'City': 'Liverpool', 'len': 1},\n",
              "    {'City': 'Braga', 'len': 1},\n",
              "    {'City': 'Barcelona', 'len': 1},\n",
              "    {'City': 'New York', 'len': 1},\n",
              "    {'City': 'Phoenix', 'len': 1},\n",
              "    {'City': 'Hamburg', 'len': 1},\n",
              "    {'City': 'Coimbra', 'len': 1},\n",
              "    {'City': 'Paris', 'len': 1},\n",
              "    {'City': '广州', 'len': 1},\n",
              "    {'City': 'Toulouse', 'len': 1},\n",
              "    {'City': 'Glasgow', 'len': 1},\n",
              "    {'City': 'Los Angeles', 'len': 1},\n",
              "    {'City': 'Houston', 'len': 1},\n",
              "    {'City': 'Birmingham', 'len': 1},\n",
              "    {'City': 'Lyon', 'len': 1},\n",
              "    {'City': 'London', 'len': 1},\n",
              "    {'City': 'Valencia', 'len': 1},\n",
              "    {'City': 'Lisboa', 'len': 1},\n",
              "    {'City': '北京', 'len': 1},\n",
              "    {'City': 'Chicago', 'len': 1},\n",
              "    {'City': 'Guimarães', 'len': 1},\n",
              "    {'City': 'Nice', 'len': 1},\n",
              "    {'City': 'München', 'len': 1},\n",
              "    {'City': 'Zaragoza', 'len': 1},\n",
              "    {'City': 'Sevilla', 'len': 1},\n",
              "    {'City': 'Madrid', 'len': 1},\n",
              "    {'City': 'Köln', 'len': 1},\n",
              "    {'City': 'Frankfurt am Main', 'len': 1},\n",
              "    {'City': '重庆', 'len': 1},\n",
              "    {'City': '深圳', 'len': 1},\n",
              "    {'City': 'Bristol', 'len': 1},\n",
              "    {'City': 'Berlin', 'len': 1},\n",
              "    {'City': 'Marseille', 'len': 1},\n",
              "    {'City': 'Porto', 'len': 1},\n",
              "    {'City': '上海', 'len': 1}]},\n",
              "  'customers': {'total_records': 1643306,\n",
              "   'total_columns': 9,\n",
              "   'columns': ['Customer ID',\n",
              "    'Name',\n",
              "    'Email',\n",
              "    'Telephone',\n",
              "    'City',\n",
              "    'Country',\n",
              "    'Gender',\n",
              "    'Date Of Birth',\n",
              "    'Job Title'],\n",
              "   'data_types': {'Customer ID': 'Int64',\n",
              "    'Name': 'String',\n",
              "    'Email': 'String',\n",
              "    'Telephone': 'String',\n",
              "    'City': 'String',\n",
              "    'Country': 'String',\n",
              "    'Gender': 'String',\n",
              "    'Date Of Birth': 'String',\n",
              "    'Job Title': 'String'},\n",
              "   'null_counts': {'Customer ID': 0,\n",
              "    'Name': 0,\n",
              "    'Email': 0,\n",
              "    'Telephone': 0,\n",
              "    'City': 0,\n",
              "    'Country': 0,\n",
              "    'Gender': 0,\n",
              "    'Date Of Birth': 0,\n",
              "    'Job Title': 584185},\n",
              "   'memory_usage_mb': 169.70775318145752,\n",
              "   'City_distribution': [{'City': '深圳', 'len': 60709},\n",
              "    {'City': '北京', 'len': 51163},\n",
              "    {'City': 'New York', 'len': 50000},\n",
              "    {'City': 'Los Angeles', 'len': 45000},\n",
              "    {'City': '上海', 'len': 42381},\n",
              "    {'City': 'Chicago', 'len': 40000},\n",
              "    {'City': 'Houston', 'len': 35000},\n",
              "    {'City': '广州', 'len': 33600},\n",
              "    {'City': 'Phoenix', 'len': 30000},\n",
              "    {'City': 'Berlin', 'len': 29000}],\n",
              "   'Country_distribution': [{'Country': 'United States', 'len': 354450},\n",
              "    {'Country': '中国', 'len': 340082},\n",
              "    {'Country': 'España', 'len': 237575},\n",
              "    {'Country': 'Deutschland', 'len': 205560},\n",
              "    {'Country': 'France', 'len': 196696},\n",
              "    {'Country': 'United Kingdom', 'len': 190574},\n",
              "    {'Country': 'Portugal', 'len': 118369}],\n",
              "   'Gender_distribution': [{'Gender': 'M', 'len': 964562},\n",
              "    {'Gender': 'F', 'len': 677041},\n",
              "    {'Gender': 'D', 'len': 1703}]},\n",
              "  'transactions': {'total_records': 6416029,\n",
              "   'total_columns': 19,\n",
              "   'columns': ['Invoice ID',\n",
              "    'Line',\n",
              "    'Customer ID',\n",
              "    'Product ID',\n",
              "    'Size',\n",
              "    'Color',\n",
              "    'Unit Price',\n",
              "    'Quantity',\n",
              "    'Date',\n",
              "    'Discount',\n",
              "    'Line Total',\n",
              "    'Store ID',\n",
              "    'Employee ID',\n",
              "    'Currency',\n",
              "    'Currency Symbol',\n",
              "    'SKU',\n",
              "    'Transaction Type',\n",
              "    'Payment Method',\n",
              "    'Invoice Total'],\n",
              "   'data_types': {'Invoice ID': 'String',\n",
              "    'Line': 'Int64',\n",
              "    'Customer ID': 'Int64',\n",
              "    'Product ID': 'Int64',\n",
              "    'Size': 'String',\n",
              "    'Color': 'String',\n",
              "    'Unit Price': 'Float64',\n",
              "    'Quantity': 'Int64',\n",
              "    'Date': 'String',\n",
              "    'Discount': 'Float64',\n",
              "    'Line Total': 'Float64',\n",
              "    'Store ID': 'Int64',\n",
              "    'Employee ID': 'Int64',\n",
              "    'Currency': 'String',\n",
              "    'Currency Symbol': 'String',\n",
              "    'SKU': 'String',\n",
              "    'Transaction Type': 'String',\n",
              "    'Payment Method': 'String',\n",
              "    'Invoice Total': 'Float64'},\n",
              "   'null_counts': {'Invoice ID': 0,\n",
              "    'Line': 0,\n",
              "    'Customer ID': 0,\n",
              "    'Product ID': 0,\n",
              "    'Size': 413049,\n",
              "    'Color': 4350231,\n",
              "    'Unit Price': 0,\n",
              "    'Quantity': 0,\n",
              "    'Date': 0,\n",
              "    'Discount': 0,\n",
              "    'Line Total': 0,\n",
              "    'Store ID': 0,\n",
              "    'Employee ID': 0,\n",
              "    'Currency': 0,\n",
              "    'Currency Symbol': 0,\n",
              "    'SKU': 0,\n",
              "    'Transaction Type': 0,\n",
              "    'Payment Method': 0,\n",
              "    'Invoice Total': 0},\n",
              "   'memory_usage_mb': 936.5224657058716,\n",
              "   'financial_summary': {'statistic': {0: 'count',\n",
              "     1: 'null_count',\n",
              "     2: 'mean',\n",
              "     3: 'std',\n",
              "     4: 'min',\n",
              "     5: '25%',\n",
              "     6: '50%',\n",
              "     7: '75%',\n",
              "     8: 'max'},\n",
              "    'Unit Price': {0: 6416029.0,\n",
              "     1: 0.0,\n",
              "     2: 132.46136145893357,\n",
              "     3: 185.09469201896496,\n",
              "     4: 2.0,\n",
              "     5: 32.5,\n",
              "     6: 51.0,\n",
              "     7: 116.5,\n",
              "     8: 1153.5},\n",
              "    'Quantity': {0: 6416029.0,\n",
              "     1: 0.0,\n",
              "     2: 1.1002423773333943,\n",
              "     3: 0.3963777531988637,\n",
              "     4: 1.0,\n",
              "     5: 1.0,\n",
              "     6: 1.0,\n",
              "     7: 1.0,\n",
              "     8: 3.0},\n",
              "    'Line Total': {0: 6416029.0,\n",
              "     1: 0.0,\n",
              "     2: 114.22149234207016,\n",
              "     3: 211.57245540207052,\n",
              "     4: -3348.0,\n",
              "     5: 24.75,\n",
              "     6: 43.5,\n",
              "     7: 109.0,\n",
              "     8: 3460.5},\n",
              "    'Invoice Total': {0: 6416029.0,\n",
              "     1: 0.0,\n",
              "     2: 243.57930688436727,\n",
              "     3: 536.7375859717677,\n",
              "     4: -6750.5,\n",
              "     5: 34.1,\n",
              "     6: 83.5,\n",
              "     7: 241.1,\n",
              "     8: 8977.0}},\n",
              "   'Date_range': {'min_date': '2023-01-01 00:00:00',\n",
              "    'max_date': '2025-03-18 20:59:00',\n",
              "    'unique_dates': 608986},\n",
              "   'unique_customer_id': 1283707,\n",
              "   'unique_product_id': 17940,\n",
              "   'unique_store_id': 35,\n",
              "   'unique_employee_id': 264},\n",
              "  'overall': {'total_datasets': 6,\n",
              "   'total_records_across_all': 8077895,\n",
              "   'total_memory_usage_mb': 1110.9396657943726,\n",
              "   'analysis_timestamp': '2025-08-05T16:22:49.161654'}})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4uuuoBuoB2Sw"
      },
      "id": "4uuuoBuoB2Sw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}